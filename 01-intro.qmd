# Introduction

Over the last two decades, causal inference has played a central role in education research by providing tools and analytic strategies to generate evidence on the effectiveness of educational interventions and policies [@murnane2010methods; @Rosenbaum.2010]. The push for causal work has been driven by policymakers and funding agencies emphasizing rigorous evaluations, which can answer questions regarding cause and effect. Simultaneously, there has been a rapid emergence of increasingly sophisticated quantitative methods and access to an ever-increasing amount of high-dimensional administrative data [@daniel2019big; @williamson2017big; @Einav.2014]. However, there has been a lack of analytic tools that can deal with high-dimensional data in causal inference, especially in applied educational research.

High-dimensional data complicates the issue of causal inference because, generally, as you increase the number of covariates, traditional propensity score based methods degrade in performance [@d2020underspecification; @Collier.2022; @Song.2020]. Additionally, 'big' data creates computational issues using traditional statistical tools, where due to the sheer volume of data models may fail to run[@daniel2019big; @Hill.2011soh; @Reardon.2019]. However, computer science has generated a wealth of research on a potential solution: Deep Neural Networks (DNN) [@LeCun.2015; @Pang.2019; @Farrell.2021]. DNN are algorithms based on our brain's neural architecture. They are commonly used in industry to model complex prediction and classification tasks and have already proven to excel in modeling high-dimensional data [@LeCun.2015]. However, little work to date has applied DNN to causal work, particularly in educational research.

Questions of causality have traditionally been addressed using Randomized Control Trials (RCT), whereby students are randomly assigned to different treatment groups (e.g., Treatment\|Control) [@Rosenbaum.2010]. The act of randomization assures, on average, that both groups will be balanced on all observable and unobservable characteristics, ruling out that an intervention impact is due to the differences in group compositions [@Rosenbaum.2010]. However, in education and broader social science research, ethical, financial, or practical obstacles can prevent a researcher from conducting a randomized experiment. Instead, researchers must rely on non-experimental or observational studies in which students self-select or are placed in an intervention without randomization.

In education, propensity score analysis is one of the most widely used quasi-experimental methods [@Fan.2011; @Powell.2020; @Rosenbaum.2010; @Thoemmes.2011pt9; @Stuart.2010]. Propensity score analysis is used in non-experimental studies to balance the observed characteristics between treated and untreated students, just as randomization to treatment and control conditions creates balance on observable variables. The propensity score represents the probability that a student would have been exposed to treatment, conditional on observable covariates [@Rosenbaum.1981; @Rosenbaum.1984].

However, to estimate unbiased treatment effects correctly using propensity scores, certain strict assumptions must be met such that the propensity score model:

(1) captures all of the characteristics related to selection into treatment [@Rosenbaum.2010].

(2) correctly models the association between the student's characteristics and treatment selection, meaning that all proper interactions and non-linear terms should be specified [@Rosenbaum.2010].

The literature suggests a "kitchen sink" approach for variable selection to guard against the first condition since the penalties are high if a potential confounder is not included in the propensity score model [@Pirracchio.2015; @Shortreed.2017lu8; @Stuart.2010]. For the second condition, the literature suggests iterating through various model specifications, with each iteration adding interactions or nonlinear terms until a balance is achieved among the observed characteristics between treated and untreated students [@Stuart.2010]. The consequences are steep if these assumptions are not met, as it will lead to biased treatment effect estimates (CITE).

These conditions may be challenging to satisfy in the social science literature, especially education since behavioral data has many more intricacies than data used in the propensity score simulation literature [@Thoemmes.2011pt9]. However, the literature provides few methods for the critical step of estimating the propensity score in a high-dimensional setting [@Stuart.2010; @ho2007matching]. The available research base suggests that the most widely used models for estimating the propensity score in education are logistic regression models, which are inadequate and lead to biased treatment effects when incorrectly specified [@Lee.2010; @Pan.2018qcl; @Setoguchi.2008; @Westreich.2010]. For example, logistic models tend to degrade when modeling high-dimensional data [@Hill.2011soh]. Therefore, a key question is whether DNN represent a viable and worthy alternative to estimating the propensity score.

DNN have already been shown to outperform logistic models and machine learning algorithms in simulations outside of propensity score analyses [@LeCun.2015; @Pang.2019]. DNN represent an essential tool for researchers in estimating the correct propensity score model, in general, and mainly when dealing with large datasets that include hundreds of variables on students and when those variables have complex associations. Therefore, the fundamental question that motivates my dissertation is:

> How can we improve propensity score estimation in large observational datasets with modern DNN techniques?

My dissertation includes two studies: a methodological and substantive application. In my first study, I develop and assess a DNN-based approach for propensity score estimation against the traditional logistic model and other machine learning methods with high-dimensional data, using propensity score weighting. In my second study, I use propensity score matching to evaluate a large-scale text message campaign focused on helping students navigate college-going tasks during the COVID-19 pandemic. In this section, I provide a brief overview of my dissertation papers.

#### Paper 1: Evaluating modern propensity score estimation methods with high-dimensional data

Estimating the propensity score is a crucial step in propensity score analysis. Incorrect specifications of the propensity score model will result in biased treatment effects [@d2020underspecification; @murnane2010methods; @Rosenbaum.2010]. Therefore, in this simulation study, I introduce DNN into the field of education research and show that their unique properties, such as flexibility and lax model assumptions, are suitable and appropriate for propensity score estimation.

In my first study, I aim to assess the performance of the traditional logistic model and other machine learning methods with high-dimensional data, using propensity score weighting. In addition, I overcome the limitations of existing methods by proposing a novel DNN-based approach to the estimation of propensity scores and illustrate a novel new framework for generating multi-type, correlated high-dimensional data replicating the high-dimensional administrative data that is becoming available to education researchers.

#### Paper 2: When in-person support is not possible, can virtual outreach help? Evaluating the impact of an artificially intelligent conversational chatbot to promote college enrollment during the COVID-19 pandemic

The pandemic increased uncertainty for the class of 2020 in what ordinarily would be their timely transition to college. In response to this challenge, the Common Application (Common App), in partnership with Mainstay and the College Advising Corps (CAC), acted quickly to provide students with proactive outreach and guidance about college-going tasks via an innovative large-scale texting campaign. This outreach specifically targeted US high school students who were the first in their families to go to college and had low family income. Over 38 weeks, a Mainstay AI chatbot named Oli sent scripted messages to students on various topics related to the college search, application, and matriculation processes. To better target the information, Oli solicited information directly from students about the types of resources they needed and pressing questions they had. Student questions that Oli could not answer were forwarded to CAC advisers, who would follow up directly with individual students.

Using a propensity score matching approach, I investigate whether the college applications and enrollment rates were higher among students targeted for this chatbot outreach compared to their observationally similar peers who were not. In addition, I examine treatment heterogeneity based on students' application behavior, racial/ethnic identity, and the level of chatbot engagement.
