# Introduction

Over the last two decades, causal inference has played a central role in education research by providing tools and analytic strategies for generating evidence on the effectiveness of educational interventions and policies. The demand for rigorous evaluations by policymakers and funding agencies is driving this shift [@murnane2010methods; @Rosenbaum.2010]. This time has also seen an increase in sophisticated machine learning approaches, and access to high-dimensional administrative data [@daniel2019big; @williamson2017big; @Einav.2014]. These advances have created a need to develop analytic techniques that can effectively handle the complexities of high-dimensional data. This is especially important in the context of educational settings and research questions for which researchers have access to observational data containing hundreds of covariates of student characteristics and are using causal inference methods to evaluate the effectiveness of educational interventions.

However, as more data on students becomes available, existing causal inference methods, many of which rely on parametric models, face challenges when applied to high-dimensional data, such as overfitting and variable selection [@d2020underspecification; @Collier.2022; @Song.2020]. Overfitting occurs when a model is fit to noise rather than the actual relationship between covariates, resulting in inaccurate coefficient estimates [@Kuhn.2013]. Variable selection is also challenging, given that it is hard to determine the most important variables in a large dataset, resulting in biased estimates [@Buhlmann.2011]. Additionally, processing high-dimensional data requires significant computing power, making it challenging to fit models and perform analyses efficiently [@Efron.2004] [@daniel2019big; @Hill.2011soh; @Reardon.2019].

Deep neural networks (DNNs) are a potential solution to these challenges, thanks to advances in computing and software. DNNs are artificial intelligence algorithms inspired by the human brain's neural architecture [@LeCun.2015; @Pang.2019; @Farrell.2021]. DNNs are widely used in various industries for complex prediction and classification tasks and have proven to be effective in modeling complex high-dimensional data. However, there has been little application of DNNs to causal inference, particularly in social science research.

Randomized Control Trials (RCTs) have been the traditional approach to address questions of causality. In RCTs, students are randomly assigned to either a treatment or control group [@Rosenbaum.2010]. The randomization process ensures that, on average, both groups are balanced in terms of both *observable* and *unobservable* characteristics, ruling out the possibility that any intervention impact is due to differences in group composition [@Rosenbaum.2010]. However, conducting RCTs may not always be feasible for ethical, financial, or practical reasons. In such cases, researchers must rely on observational or non-experimental studies in which students either self-select or are placed in an intervention. To estimate unbiased treatment effects using observational data, researchers use quasi-experimental methods that attempt to balance *observable* characteristics between treatment groups.  

Propensity score analysis is the most widely used quasi-experimental method in education research that balances *observed* characteristics between treated and untreated students by conditioning on what Rosenbaum and Rubin call a propensity score (CITE). The propensity score represents the probability that a student would have been exposed to treatment, conditional on observable student characteristics [@Rosenbaum.1981; @Rosenbaum.1984]. 

However, certain strict assumptions must be met to estimate unbiased treatment effects correctly using propensity score analysis. For example, the propensity score model must:

(1) capture all the covariates related to the selection into treatment (Paul R. Rosenbaum 2010).

(2) correctly model the association between the student's characteristics and treatment selection, meaning that all proper interactions and non-linear terms should be specified (Paul R. Rosenbaum 2010).

To have the best chance of meeting the first condition, the literature suggests a "kitchen sink" approach for variable selection, because the penalties are high if a potential confounder is not included in the propensity score model [@Pirracchio.2015; @Shortreed.2017lu8; @Stuart.2010]. For the second condition, the literature suggests iterating through various model specifications, with each iteration adding interactions or non-linear terms until a balance is achieved among the observed characteristics between treated and untreated students [@Stuart.2010]. The consequences are steep if these assumptions are not met, as it will lead to biased treatment effect estimates (CITE).

The literature provides few methods for the critical step of estimating the propensity score in a high-dimensional setting [@Stuart.2010; @ho2007matching]. The available research base suggests that the most widely used models for estimating the propensity score in education are logistic regression models, which are inadequate and lead to biased treatment effects when incorrectly specified [@Lee.2010; @Pan.2018qcl; @Setoguchi.2008; @Westreich.2010]. When modeling high-dimensional data, logistic models tend to overfit the data or cause issues with perfect separation, such that the logistic regression returns fitted probabilities that are either 0 or 1 [@Hill.2011soh]. Therefore, an open question is whether DNNs represent a viable and worthy alternative to estimating the propensity score.

DNNs have already been shown to outperform logistic models and machine learning algorithms in simulation studies outside of propensity score analyses [@LeCun.2015; @Pang.2019]. DNNs represent an essential tool for researchers in estimating the correct propensity score model, in general, and mainly when dealing with large datasets that include hundreds of variables on students and when those variables have complex associations. Therefore, the fundamental question that motivates my dissertation is:

> How can we improve propensity score estimation in large observational datasets with modern DNN techniques?

My dissertation includes two studies: a methodological and substantive application. In my first study, I developed and assessed a DNN-based approach for propensity score estimation against the traditional logistic model and other machine learning methods with high-dimensional data, using propensity score weighting. In my second study, I use propensity score matching to evaluate a large-scale text message campaign focused on helping students navigate college-going tasks during the COVID-19 pandemic. Note that I did not use my DNN-based approach to evaluate this intervention for two reasons: 1) this evaluation is a rare case where the treatment assignment mechanism is known. Specifically, students in the intervention had to be first-generation and qualified for a fee waiver, and 2) although we had access to a rich set of covariates after data cleaning, only 22 covariates were deemed appropriate for estimating the propensity score, negating the use of the DNN-based approach. What follows is a brief overview of my dissertation papers. This raises the important point that DNN-based approaches are not necessarily needed in every application. Therefore, in my first paper, I aim to inform questions regarding the conditions under which DNN methods outperform more traditional approaches. 

#### Paper 1: Evaluating modern propensity score estimation methods with high-dimensional data

Estimating the propensity score is a crucial step in propensity score analysis. Incorrect specifications of the propensity score model can result in biased treatment effect estimates [@d2020underspecification; @murnane2010methods; @Rosenbaum.2010]. Therefore, in this simulation study, I introduce DNNs into the field of education research and show that their unique properties, such as flexibility and lax model assumptions, are suitable and appropriate for propensity score estimation, particularly in the context of high-dimensional data.

In my first study, I assess the performance of the traditional logistic model and other machine learning methods with high-dimensional data using propensity score weighting. In addition, I overcome the limitations of existing methods by proposing a novel DNN-based approach to the estimation of propensity scores and illustrating a novel framework for generating multi-type, correlated high-dimensional data replicating the high-dimensional administrative data that is becoming available to education researchers.

#### Paper 2: When in-person support is not possible, can virtual outreach help? Evaluating the impact of an artificially intelligent conversational chatbot to promote college enrollment during the COVID-19 pandemic

The pandemic increased uncertainty for the class of 2020 in what ordinarily would be their timely transition to college. In response to this challenge, the Common Application (Common App), in partnership with Mainstay and the College Advising Corps (CAC), acted quickly to provide students with proactive outreach and guidance about college-going tasks via an innovative large-scale texting campaign. This outreach specifically targeted US high school students who were the first in their families to go to college and had low family income. For up to 38 weeks, a Mainstay AI chatbot named Oli sent scripted messages to students on various topics related to the college search, application, and matriculation processes. To better target the information, Oli solicited information directly from students about the types of resources they needed and pressing questions they had. Student questions that Oli could not answer were forwarded to CAC advisers, who would follow up directly with individual students.

Using a propensity score matching approach, I investigate whether college application rates and enrollment rates were higher among students targeted for this chatbot outreach compared to their observationally similar peers who were not. In addition, I examine treatment heterogeneity based on students' application behavior, racial/ethnic identity, and the level of chatbot engagement.
