# Introduction

Over the last two decades, causal inference has played a central role in education research by providing tools, and analytic strategies to generate evidence on the effectiveness of educational interventions and policies, fueled by policymakers and funding agencies demand for rigorous evaluations [@murnane2010methods; @Rosenbaum.2010]. This period has also seen a growth in sophisticated quantitative methods, such as machine learning, and access to high-dimensional administrative data [@daniel2019big; @williamson2017big; @Einav.2014]. These advancements have created the need for developing analytic techniques that can effectively handle the complexities of high-dimensional data. This is particularly relevant when using causal inference methods to evaluate the effectiveness of educational interventions when researchers have access to observational data with hundreds of covariates of student characteristics.

The current challenge in using causal inference with big data is due to the difficulties posed by high-dimensional data, such as overfitting and variable selection \[\@d2020underspecification; \@Collier.2022; \@Song.2020\]. Overfitting occurs when models are fit to noise instead of the actual relationship among covariates, leading to incorrect coefficient estimates \[\@Kuhn.2013\]. Variable selection is also challenging because it is hard to determine the most important variables in a large dataset, resulting in biased estimates \[\@Buhlmann.2011\]. Additionally, processing high-dimensional data requires significant computing power, making it challenging to fit models and perform analyses efficiently \[\@Efron.2004\] \[\@daniel2019big; \@Hill.2011soh; \@Reardon.2019\].

Recent progress in computing and computer science has presented a potential solution for these challenges in the form of deep neural networks (DNNs). DNNs are a type of AI algorithm inspired by the interconnected neural architecture of the human brain \[\@LeCun.2015; \@Pang.2019; \@Farrell.2021\]. They are widely used in various industries for complex prediction and classification tasks and have proven to be effective in modeling complex high-dimensional data. However, to date, there has been limited application of DNNs to causal inference, especially in the field of social science research.

Randomized Control Trials (RCTs) have been the traditional approach to address questions of causality. In RCTs, students are randomly assigned to either a treatment or control group \[\@Rosenbaum.2010\]. The randomization process ensures that, on average, both groups are balanced in terms of both observable and unobservable characteristics, ruling out the possibility that any intervention impact is due to differences in group composition \[\@Rosenbaum.2010\]. However, conducting RCTs may not always be feasible due to ethical, financial, or practical reasons. In such cases, researchers have to rely on observational or non-experimental studies, where students either self-select or are placed into an intervention without randomization. In contrast to RCT data, observational data must be analyzed using quasi-experimental methods.

In education, propensity score analysis is one of the most widely used quasi-experimental methods [@Fan.2011; @Powell.2020; @Rosenbaum.2010; @Thoemmes.2011pt9; @Stuart.2010]. Propensity score analysis is used in non-experimental studies to balance the observed characteristics between treated and untreated students, just as randomization to treatment and control conditions creates balance on observable variables. The propensity score represents the probability that a student would have been exposed to treatment, conditional on observable covariates [@Rosenbaum.1981; @Rosenbaum.1984].

One widely used quasi-experimental method in education is propensity score analysis, which balances observed characteristics between treated and untreated students. However, correctly estimating treatment effects using propensity scores requires strict assumptions to be met, such as capturing all variables related to selection into treatment and correctly modeling the association between student characteristics and treatment selection. Logistic regression models are commonly used to estimate the propensity score, but they have been shown to be inadequate and lead to biased treatment effects when data is high-dimensional.

However, to estimate unbiased treatment effects correctly using propensity scores, certain strict assumptions must be met. Most importantly we have to assumem that the treatment asisngment mechanims is ignorable, meaning that treatment assingmen is independed of the potential outcomes given the observed covariates [@Rosenbaum.2010]. This is curical for causal inference as it allows us to make inferences about the treatment effect from observational data. In order for this key assumption to hold we must ensure that the propenisty score model:

(1) captures all of the characteristics related to selection into treatment [@Rosenbaum.2010].

(2) correctly models the association between the student's characteristics and treatment selection, meaning that all proper interactions and non-linear terms should be specified [@Rosenbaum.2010].

Adheres to the positivity assumption, meaning that the propensity score model should assign positive probabilities to each student receiving treatment or not receiving treatment \[\@Rosenbaum.2010\]. Establishing these assumptions is crucial in order to accurately estimate treatment effects using propensity score analysis.

The literature suggests a "kitchen sink" approach for variable selection to guard against the first condition since the penalties are high if a potential confounder is not included in the propensity score model [@Pirracchio.2015; @Shortreed.2017lu8; @Stuart.2010]. For the second condition, the literature suggests iterating through various model specifications, with each iteration adding interactions or nonlinear terms until a balance is achieved among the observed characteristics between treated and untreated students [@Stuart.2010]. The consequences are steep if these assumptions are not met, as it will lead to biased treatment effect estimates (CITE).

In additon, the literature provides few methods for the critical step of estimating the propensity score in a high-dimensional setting [@Stuart.2010; @ho2007matching]. The available research base suggests that the most widely used models for estimating the propensity score in education are logistic regression models, which are inadequate and lead to biased treatment effects when incorrectly specified [@Lee.2010; @Pan.2018qcl; @Setoguchi.2008; @Westreich.2010]. For example, logistic models tend to degrade when modeling high-dimensional data [@Hill.2011soh]. Therefore, a key question is whether DNN represent a viable and worthy alternative to estimating the propensity score.

DNN have already been shown to outperform logistic models and machine learning algorithms in simulations outside of propensity score analyses [@LeCun.2015; @Pang.2019]. DNN represent an essential tool for researchers in estimating the correct propensity score model, in general, and mainly when dealing with large datasets that include hundreds of variables on students and when those variables have complex associations. Therefore, the fundamental question that motivates my dissertation is:

> How can we improve propensity score estimation in large observational datasets with modern DNN techniques?

My dissertation includes two studies: a methodological and substantive application. In my first study, I develop and assess a DNN-based approach for propensity score estimation against the traditional logistic model and other machine learning methods with high-dimensional data, using propensity score weighting. In my second study, I use propensity score matching to evaluate a large-scale text message campaign focused on helping students navigate college-going tasks during the COVID-19 pandemic. In this section, I provide a brief overview of my dissertation papers.

In this dissertation, we explore the potential of DNN for causal inference in high-dimensional education data. We compare the performance of DNN and traditional methods for estimating the propensity score, and assess their ability to accurately estimate treatment effects. Our findings will contribute to the development of new tools for causal inference in high-dimensional education data, and support the need for further research in this area.

#### Paper 1: Evaluating modern propensity score estimation methods with high-dimensional data

Estimating the propensity score is a crucial step in propensity score analysis. Incorrect specifications of the propensity score model will result in biased treatment effects [@d2020underspecification; @murnane2010methods; @Rosenbaum.2010]. Therefore, in this simulation study, I introduce DNN into the field of education research and show that their unique properties, such as flexibility and lax model assumptions, are suitable and appropriate for propensity score estimation.

In my first study, I aim to assess the performance of the traditional logistic model and other machine learning methods with high-dimensional data, using propensity score weighting. In addition, I overcome the limitations of existing methods by proposing a novel DNN-based approach to the estimation of propensity scores and illustrate a novel new framework for generating multi-type, correlated high-dimensional data replicating the high-dimensional administrative data that is becoming available to education researchers.

#### Paper 2: When in-person support is not possible, can virtual outreach help? Evaluating the impact of an artificially intelligent conversational chatbot to promote college enrollment during the COVID-19 pandemic

The pandemic increased uncertainty for the class of 2020 in what ordinarily would be their timely transition to college. In response to this challenge, the Common Application (Common App), in partnership with Mainstay and the College Advising Corps (CAC), acted quickly to provide students with proactive outreach and guidance about college-going tasks via an innovative large-scale texting campaign. This outreach specifically targeted US high school students who were the first in their families to go to college and had low family income. Over 38 weeks, a Mainstay AI chatbot named Oli sent scripted messages to students on various topics related to the college search, application, and matriculation processes. To better target the information, Oli solicited information directly from students about the types of resources they needed and pressing questions they had. Student questions that Oli could not answer were forwarded to CAC advisers, who would follow up directly with individual students.

Using a propensity score matching approach, I investigate whether the college applications and enrollment rates were higher among students targeted for this chatbot outreach compared to their observationally similar peers who were not. In addition, I examine treatment heterogeneity based on students' application behavior, racial/ethnic identity, and the level of chatbot engagement.
