# Introduction

Over the last two decades, causal inference has played a central role in education research by providing tools and analytic strategies to generate evidence on the effectiveness of educational interventions and policies (Cook et al., 2002; Murnane & Willett, 2010; Rosenbaum, 2010, 2020). The push for causal work has been driven by policymakers and funding agencies emphasizing rigorous evaluations, which can answer questions regarding cause and effect. Simultaneously, there has been a rapid emergence of increasingly sophisticated quantitative methods and access to an ever-increasing amount of high-dimensional administrative data (Daniel, 2019; Gibson & Ifenthaler, 2016; Williamson, 2017). However, there has been a lack of analytical tools that can deal with high-dimensional data in causal inference, especially in applied educational research. @10.1007/s11162-021-09633-z

High-dimensional data complicates the issue of causal inference because, generally, as you increase the number of variables, traditional propensity score based causal methods degrade in performance (D'Amour et al., 2020). Additionally, 'big' data creates computational issues using traditional statistical tools, where due to the sheer volume of data models may fail to run (Daniel, 2019; Prinsloo & Slade, 2016). However, computer science has generated a wealth of research on a potential solution: Deep Neural Networks (DNN) (Deng & Yu, 2014; Hernández-Blanco et al., 2019; LeCun et al., 2015; Perrotta & Selwyn, 2019). DNN's are algorithms based on our brain's neural architecture. An example of a simple DNN is found in Figure 1. They are commonly used in industry to model complex prediction and classification tasks and have already proven to excel in modeling high-dimensional data (LeCun et al., 2015). However, no work to date has applied DNN's to causal inference questions, particularly in educational research.

Questions of causality have traditionally been addressed using Randomized Control Trials (RCT), whereby students are randomly assigned to different treatment groups (e.g., Treatment\|Control) (Murnane & Willett, 2010). The act of randomization assures, on average, that both groups will be balanced on all observable and unobservable characteristics, ruling out that an intervention impact is due to the differences in the group compositions (Rosenbaum, 2010, 2020). However, ethical and cost concerns can sometimes stand in the way of experimental studies in education (Murnane & Willett, 2010). Instead, we often depend on quasi-experimental methods and observational data where students either self-select or are placed into an intervention without randomization.

In education, propensity score analysis is one of the most widely used quasi-experimental methods (Fan & Nowell, 2011; Powell et al., 2019; Rosenbaum, 2020; Stuart, 2007; Thoemmes & Kim, 2011). Propensity score analysis is used in non-experimental studies to balance the observed characteristics between treated and un-treated students, just as randomization to treatment and control conditions creates balance on observable variables. The propensity score represents the probability that a student would have been exposed to treatment, conditional on observable variables (Rosenbaum & Rubin, 1983, 1984).

However, to estimate unbiased treatment effects correctly using propensity scores, certain strict assumptions must be met; ignorability assumption (Rosenbaum & Rubin, 1981, 1984). The propensity score model must:

(1) capture all of the characteristics related to the selection into treatment, leaving out no potential confounders (Rosenbaum, 2010, 2020).

(2) correctly model the association between the student's characteristics and treatment selection, meaning that all proper interactions and non-linear terms should be specified (Rosenbaum, 2010, 2020).

The literature suggests a "kitchen sink" approach for variable selection to guard against the first condition since the penalties are high if a potential confounder is not included in the propensity score model (Pirracchio et al., 2015; Shortreed & Ertefaie, 2017). For the second condition, the literature suggests iterating through various model specifications, with each iteration adding interactions or nonlinear terms until a balance is achieved amongst the observed characteristics between treated and untreated students (Murnane & Willett, 2010). The consequences are steep if these assumptions are not met, as it will lead to biased treatment effect estimates.

These conditions may be difficult to satisfy in the social science literature, especially education, since behavioral data has many more intricacies than data used in the propensity score simulation literature (Thoemmes & Kim, 2011). However, the literature provides few methods for the critical step of estimating the propensity score (Stuart, 2007; Thoemmes & Kim, 2011). The available research base suggests that the most widely used models for estimating the propensity score in education are logistic regression models, which are inadequate and lead to biased treatment effects when incorrectly specified (Lee et al., 2010; Pan & Bai, 2015; Setoguchi et al., 2008; Westreich et al., 2010). For example, logistic models tend to degrade when modeling high-dimensional data. Therefore, a key question is whether DNN's represent a viable and worthy alternative to estimating the propensity score.

DNN's have multiple properties that make them worthy of consideration for propensity score estimation:

(1) DNN's are highly flexible and capable of capturing complex interactions and nonlinearities, and

(2) DNN's allow for automatic variable selection (Hernández-Blanco et al., 2019; LeCun et al., 2015; Zou et al., 2019), which aides in having to iterate through various models adding interactions or non-linear terms.

DNN's have already been shown to outperform logistic models and machine learning algorithms in simulations outside of propensity score analyses (Dahl et al., 2013; Hu et al., 2015; Mousavi et al., 2016). These methods could represent an essential tool for researchers in estimating the correct propensity score model, in general, and mainly when dealing with large datasets that include hundreds of variables on students, and when those variables have complex associations. Therefore, the fundamental question that motivates my dissertation is:

> How can we improve propensity score estimations in large observational datasets with modern DNN techniques?

My dissertation includes two studies: a methodological and a substantive application. In the first study, I develop a new method for estimating the propensity score based on recent advancements in information and computer science on DNN's. In the second study, I evaluate a large-scale text message campaign focused on helping students navigate college-going task during the COVID-19 pandemic. In the remainder of this chapter, I provide a brief overview of each of my dissertation papers.

## Paper 1: Evaluating modern propensity score estimation methods with high-dimensional data

In my first study, I aim to overcome the limitation of existing methods by proposing a novel DNN-based approach to propensity score estimations. DNN's are based on how our brains process information through the flow of data through interconnected neurons. DNN's can uncover complex relationships in high-dimensional data with better precision than traditional statistical methods and machine learning methods, which were novel just a decade ago (Hernández-Blanco et al., 2019; LeCun et al., 2015; Schmidhuber, 2014; Zou et al., 2019). A DNNs ability to identify the intricate relationships withing large-volume, high-dimensional data makes these algorithms applicable to causal methods, like propensity score analysis.

Propensity score estimation is a crucial step in a propensity score analysis. Given that if the propensity score model is misspecified, there may be inadequate overlap between the treatment and control groups' estimated propensity scores. The insufficient overlap makes it challenging to balance the groups based on their pre-treatment characteristics, leading to biased treatment effects (D'Amouret al., 2020; Murnane & Willett, 2010; Rosenbaum, 2010, 2020). Therefore, in this simulation study, I aim to introduce DNNs into the field of education research and show that their unique properties, such as flexibility and lax model assumptions, are suitable and appropriate for propensity score estimation.

## Paper 2: When in-person support is not possible, can virtual outreach help? Evaluating the impact of an artificially intelligent conversational chatbot to promote college enrollment during the COVID-19 pandemic

In my final study, I evaluate a large-scale nonrandomized educational intervention using an AI chatbot during the COVID-19 pandemic. The pandemic increased uncertainty for the class of 2020 in what ordinarily would be their timely transition to college. In response to this challenge, the Common Application (Common App), in partnership with Mainstay and the College Advising Corps (CAC), acted quickly to provide students with proactive outreach and guidance about college-going tasks via an innovative large-scale texting campaign. This outreach specifically targeted US high school students who were the first in their families to go to college and had low family income. These students were selected given they would likely benefit from additional support through their college transition process, particularly in the backdrop of a global pandemic.

Over 38 weeks, a Mainstay AI chatbot named Oli sent scripted messages to students on various topics related to the college search, application, and matriculation processes. To better target the information, Oli solicited information directly from students about the types of resources they needed and pressing questions they had. Student questions that Oli could not answer were forwarded to CAC advisers, who would follow up directly with individual students.

As part of a research collaboration I was tasked with evaluating the effectiveness of this outreach. Specifically, whether the outreach improved students' college-going outcomes (i.e., college application submission and enrollment). The intervention was quickly deployed to help thousands of students and, as a result, was not implemented in the context of an experimental study. Nevertheless, understanding the program's impact will benefit students and policymakers interested in lowering college access barriers while providing evidence of the pandemic's effect on college access.
