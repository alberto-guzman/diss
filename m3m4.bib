
@article{Kale.2015, 
year = {2015}, 
title = {{Causal Phenotype Discovery via Deep Networks.}}, 
author = {Kale, David C and Che, Zhengping and Bahadori, Mohammad Taha and Li, Wenzhe and Liu, Yan and Wetzel, Randall}, 
journal = {AMIA ... Annual Symposium proceedings. AMIA Symposium}, 
pmid = {26958203}, 
pmcid = {PMC4765623}, 
abstract = {{The rapid growth of digital health databases has attracted many researchers interested in using modern computational methods to discover and model patterns of health and illness in a research program known as computational phenotyping. Much of the work in this area has focused on traditional statistical learning paradigms, such as classification, prediction, clustering, pattern mining. In this paper, we propose a related but different paradigm called causal phenotype discovery, which aims to discover latent representations of illness that are causally predictive. We illustrate this idea with a two-stage framework that combines the latent representation learning power of deep neural networks with state-of-the-art tools from causal inference. We apply this framework to two large ICU time series data sets and show that it can learn features that are predictively useful, that capture complex physiologic patterns associated with critical illnesses, and that are potentially more clinically meaningful than manually designed features.}}, 
pages = {677--86}, 
volume = {2015}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/AMIA%20...%20Annual%20Symposium%20proceedings.%20AMIA%20Symposium/Causal%20Phenotype%20Discovery%20via%20Deep%20Networks-_2015_AMIA%20...%20Annual%20Symposium%20proceedings.%20AMIA%20Symposium_1.pdf}
}
@article{Luo.2020, 
year = {2020}, 
title = {{When causal inference meets deep learning}}, 
author = {Luo, Yunan and Peng, Jian and Ma, Jianzhu}, 
journal = {Nature Machine Intelligence}, 
doi = {10.1038/s42256-020-0218-x}, 
abstract = {{Bayesian networks can capture causal relations, but learning such a network from data is NP-hard. Recent work has made it possible to approximate this problem as a continuous optimization task that can be solved efficiently with well-established numerical techniques.}}, 
pages = {426--427}, 
number = {8}, 
volume = {2}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Nature%20Machine%20Intelligence/When%20causal%20inference%20meets%20deep%20learning_2020_Nature%20Machine%20Intelligence_1.pdf}
}
@article{Iglesias.2021, 
year = {2021}, 
title = {{A primer on deep learning and convolutional neural networks for clinicians}}, 
author = {Iglesias, Lara Lloret and Bellón, Pablo Sanz and Barrio, Amaia Pérez del and Fernández-Miranda, Pablo Menéndez and González, David Rodríguez and Vega, José A. and Mandly, Andrés A. González and Blanco, José A. Parra}, 
journal = {Insights into Imaging}, 
issn = {1869-4101}, 
doi = {10.1186/s13244-021-01052-z}, 
pmid = {34383173}, 
pmcid = {PMC8360246}, 
abstract = {{Deep learning is nowadays at the forefront of artificial intelligence. More precisely, the use of convolutional neural networks has drastically improved the learning capabilities of computer vision applications, being able to directly consider raw data without any prior feature extraction. Advanced methods in the machine learning field, such as adaptive momentum algorithms or dropout regularization, have dramatically improved the convolutional neural networks predicting ability, outperforming that of conventional fully connected neural networks. This work summarizes, in an intended didactic way, the main aspects of these cutting-edge techniques from a medical imaging perspective.}}, 
pages = {117}, 
number = {1}, 
volume = {12}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Insights%20into%20Imaging/A%20primer%20on%20deep%20learning%20and%20convolutional%20neural%20networks%20for%20clinicians_2021_Insights%20into%20Imaging_1.pdf}
}
@article{Olmos.2017, 
year = {2017}, 
title = {{A Practical Guide for Using Propensity Score Weighting in R}}, 
author = {Olmos, Antonio and Govindasamy, Priyalatha}, 
journal = {Practical Assessment, Research \& Evaluation}, 
issn = {1531-7714}, 
pages = {19--46}, 
volume = {20}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Practical%20Assessment,%20Research%20&%20Evaluation/A%20Practical%20Guide%20for%20Using%20Propensity%20Score%20Weighting%20in%20R_2017_Practical%20Assessment,%20Research%20&%20Evaluation.pdf}
}
@article{Rosenbaum.1981, 
year = {1981}, 
title = {{The Central Role of the Propensity Score in Observational Studies for Causal Effects.}}, 
author = {Rosenbaum, Paul R and Rubin, Donald B}, 
journal = {Biometrika}, 
doi = {10.21236/ada114514}, 
abstract = {{The results of observational studies are often disputed because of nonrandom treatment assignment. For example, patients at greater risk may be overrepresented in some treatment group. This paper discusses the central role of propensity scores and balancing scores in the analysis of observational studies. The propensity score is the (estimated) conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: matched sampling on the univariate propensity score which is equal percent bias reducing under more general conditions than required for discriminant matching, multivariate adjustment by subclassification on balancing scores where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and visual representation of multivariate adjustment by a two-dimensional plot. (Author)}}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Biometrika/The%20Central%20Role%20of%20the%20Propensity%20Score%20in%20Observational%20Studies%20for%20Causal%20Effects-_1981_Biometrika_1.pdf}
}
@article{Cousineau.2022, 
year = {2022}, 
title = {{Estimating Causal Effects with Optimization-Based Methods: A Review and Empirical Comparison}}, 
author = {Cousineau, Martin and Verter, Vedat and Murphy, Susan A. and Pineau, Joelle}, 
journal = {European Journal of Operational Research}, 
issn = {0377-2217}, 
doi = {10.1016/j.ejor.2022.01.046}, 
eprint = {2203.00097}, 
abstract = {{In the absence of randomized controlled and natural experiments, it is necessary to balance the distributions of (observable) covariates of the treated and control groups in order to obtain an unbiased estimate of a causal effect of interest; otherwise, a different effect size may be estimated, and incorrect recommendations may be given. To achieve this balance, there exist a wide variety of methods. In particular, several methods based on optimization models have been recently proposed in the causal inference literature. While these optimization-based methods empirically showed an improvement over a limited number of other causal inference methods in their relative ability to balance the distributions of covariates and to estimate causal effects, they have not been thoroughly compared to each other and to other noteworthy causal inference methods. In addition, we believe that there exist several unaddressed opportunities that operational researchers could contribute with their advanced knowledge of optimization, for the benefits of the applied researchers that use causal inference tools. In this review paper, we present an overview of the causal inference literature and describe in more detail the optimization-based causal inference methods, provide a comparative analysis of the prevailing optimization-based methods, and discuss opportunities for new methods.}}, 
pages = {367--380}, 
number = {2}, 
volume = {304}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/European%20Journal%20of%20Operational%20Research/Estimating%20Causal%20Effects%20with%20Optimization-Based%20Methods-%20A%20Review%20and%20Empirical%20Comparison_2022_European%20Journal%20of%20Operational%20Research_1.pdf}
}
@article{Austin.2011, 
year = {2011}, 
title = {{An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies}}, 
author = {Austin, Peter C.}, 
journal = {Multivariate Behavioral Research}, 
issn = {0027-3171}, 
doi = {10.1080/00273171.2011.568786}, 
pmid = {21818162}, 
pmcid = {PMC3144483}, 
abstract = {{The propensity score is the probability of treatment assignment conditional on observed baseline characteristics. The propensity score allows one to design and analyze an observational (nonrandomized) study so that it mimics some of the particular characteristics of a randomized controlled trial. In particular, the propensity score is a balancing score: conditional on the propensity score, the distribution of observed baseline covariates will be similar between treated and untreated subjects. I describe 4 different propensity score methods: matching on the propensity score, stratification on the propensity score, inverse probability of treatment weighting using the propensity score, and covariate adjustment using the propensity score. I describe balance diagnostics for examining whether the propensity score model has been adequately specified. Furthermore, I discuss differences between regression-based methods and propensity score-based methods for the analysis of observational data. I describe different causal average treatment effects and their relationship with propensity score analyses.}}, 
pages = {399--424}, 
number = {3}, 
volume = {46}
}
@article{Guo.2020, 
year = {2020}, 
title = {{Propensity Score Analysis: Recent Debate and Discussion}}, 
author = {Guo, Shenyang and Fraser, Mark and Chen, Qi}, 
journal = {Journal of the Society for Social Work and Research}, 
issn = {2334-2315}, 
doi = {10.1086/711393}, 
pages = {463--482}, 
number = {3}, 
volume = {11}
}
@article{Schafer.2008, 
year = {2008}, 
title = {{Average Causal Effects From Nonrandomized Studies: A Practical Guide and Simulated Example}}, 
author = {Schafer, Joseph L. and Kang, Joseph}, 
journal = {Psychological Methods}, 
issn = {1082-989X}, 
doi = {10.1037/a0014268}, 
pmid = {19071996}, 
abstract = {{In a well-designed experiment, random assignment of participants to treatments makes causal inference straightforward. However, if participants are not randomized (as in observational study, quasi-experiment, or nonequivalent control-group designs), group comparisons may be biased by confounders that influence both the outcome and the alleged cause. Traditional analysis of covariance, which includes confounders as predictors in a regression model, often fails to eliminate this bias. In this article, the authors review Rubin's definition of an average causal effect (ACE) as the average difference between potential outcomes under different treatments. The authors distinguish an ACE and a regression coefficient. The authors review 9 strategies for estimating ACEs on the basis of regression, propensity scores, and doubly robust methods, providing formulas for standard errors not given elsewhere. To illustrate the methods, the authors simulate an observational study to assess the effects of dieting on emotional distress. Drawing repeated samples from a simulated population of adolescent girls, the authors assess each method in terms of bias, efficiency, and interval coverage. Throughout the article, the authors offer insights and practical guidance for researchers who attempt causal inference with observational data.}}, 
pages = {279--313}, 
number = {4}, 
volume = {13}
}
@article{Bird.2022, 
year = {2022}, 
title = {{Is Big Data Better? LMS Data and Predictive  Analytic Performance in Postsecondary  Education}}, 
author = {Bird, Kelli A. and Castleman, Benjamin L. and Song, Yifeng and Yu, Renzhe}, 
journal = {EdWorking Paper}, 
url = {https://doi.org/10.26300/8xys-ym74}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/EdWorking%20Paper/Is%20Big%20Data%20Better-%20LMS%20Data%20and%20Predictive%20%20Analytic%20Performance%20in%20Postsecondary%20%20Education_2022_EdWorking%20Paper.pdf}
}
@article{Rosenbaum.2022, 
year = {2022}, 
title = {{Propensity Scores in the Design of Observational Studies for Causal Effects}}, 
author = {Rosenbaum, P.R. and Rubin, D. B.}, 
journal = {Biometrika}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Biometrika/Propensity%20Scores%20in%20the%20Design%20of%20Observational%20Studies%20for%20Causal%20Effects_2022_Biometrika.pdf}
}
@article{Carvalho.2019, 
year = {2019}, 
title = {{Assessing Treatment Effect Variation in Observational Studies: Results from a Data Challenge}}, 
author = {Carvalho, Carlos and Feller, Avi and Murray, Jared and Woody, Spencer and Yeager, David}, 
journal = {arXiv}, 
doi = {10.48550/arxiv.1907.07592}, 
eprint = {1907.07592}, 
abstract = {{A growing number of methods aim to assess the challenging question of treatment effect variation in observational studies. This special section of "Observational Studies" reports the results of a workshop conducted at the 2018 Atlantic Causal Inference Conference designed to understand the similarities and differences across these methods. We invited eight groups of researchers to analyze a synthetic observational data set that was generated using a recent large-scale randomized trial in education. Overall, participants employed a diverse set of methods, ranging from matching and flexible outcome modeling to semiparametric estimation and ensemble approaches. While there was broad consensus on the topline estimate, there were also large differences in estimated treatment effect moderation. This highlights the fact that estimating varying treatment effects in observational studies is often more challenging than estimating the average treatment effect alone. We suggest several directions for future work arising from this workshop.}}
}
@article{Suk.2021, 
year = {2021}, 
title = {{Random Forests Approach for Causal Inference with Clustered Observational Data}}, 
author = {Suk, Youmi and Kang, Hyunseung and Kim, Jee-Seon}, 
journal = {Multivariate Behavioral Research}, 
issn = {0027-3171}, 
doi = {10.1080/00273171.2020.1808437}, 
pmid = {32856937}, 
abstract = {{There is a growing interest in using machine learning (ML) methods for causal inference due to their (nearly) automatic and flexible ability to model key quantities such as the propensity score or the outcome model. Unfortunately, most ML methods for causal inference have been studied under single-level settings where all individuals are independent of each other and there is little work in using these methods with clustered or nested data, a common setting in education studies. This paper investigates using one particular ML method based on random forests known as Causal Forests to estimate treatment effects in multilevel observational data. We conduct simulation studies under different types of multilevel data, including two-level, three-level, and cross-classified data. Our simulation study shows that when the ML method is supplemented with estimated propensity scores from multilevel models that account for clustered/hierarchical structure, the modified ML method outperforms preexisting methods in a wide variety of settings. We conclude by estimating the effect of private math lessons in the Trends in International Mathematics and Science Study data, a large-scale educational assessment where students are nested within schools.}}, 
pages = {829--852}, 
number = {6}, 
volume = {56}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Multivariate%20Behavioral%20Research/Random%20Forests%20Approach%20for%20Causal%20Inference%20with%20Clustered%20Observational%20Data_2021_Multivariate%20Behavioral%20Research.pdf}
}
@article{Singer.2019w0m, 
year = {2019}, 
title = {{Reshaping the Arc of Quantitative Educational Research: It’s Time to Broaden Our Paradigm}}, 
author = {Singer, Judith D.}, 
journal = {Journal of Research on Educational Effectiveness}, 
issn = {1934-5747}, 
doi = {10.1080/19345747.2019.1658835}, 
abstract = {{The arc of quantitative educational research should not be etched in stone but should adapt and change over time. In this article, I argue that it is time for a reshaping by offering my personal view of the past, present and future of our field. Educational research—and research in the social and life sciences—is at a crossroads. There are many reasons for this, but chief among them is the rapid rise of data science, which has implications for educational research in general and SREE in particular. I ask us to question whether our laser focus on causal inference—which will remain crucially important—has crowded out other methods for studying equally important—yet not necessarily causal—questions. After introducing the wisdom of four muses—two philosophers of science and two statisticians—I sketch my personal research trajectory and its intersection with the field’s. The remainder of the article describes three types of studies that I would like to see more of: longitudinal studies using truly longitudinal analyses; assessment and measurement studies; and studies using data science methods.}}, 
pages = {570--593}, 
number = {4}, 
volume = {12}
}
@article{Reardon.2019, 
year = {2019}, 
title = {{Education Research in a New Data Environment: Special Issue Introduction}}, 
author = {Reardon, Sean F. and Stuart, Elizabeth A.}, 
journal = {Journal of Research on Educational Effectiveness}, 
issn = {1934-5747}, 
doi = {10.1080/19345747.2019.1685339}, 
pages = {567--569}, 
number = {4}, 
volume = {12}
}
@article{Whata.2022, 
year = {2022}, 
title = {{Evaluating Uses of Deep Learning Methods for Causal Inference}}, 
author = {Whata, Albert and Chimedza, Charles}, 
journal = {IEEE Access}, 
issn = {2169-3536}, 
doi = {10.1109/access.2021.3140189}, 
abstract = {{Logistic regression (LR) is a popular method that is used for estimating causal effects in observational studies using propensity scores. We examine the use of deep learning models such as the deep neural network (DNN), PropensityNet (PN), convolutional neural network (CNN), and convolutional neural network-long short-term memory network (CNN-LSTM) to estimate propensity scores and evaluate causal inference. We conducted studies using simulated data with different sample sizes (N = 500, N = 1000, N = 2000), 15 covariates, a continuous outcome and a binary exposure. These data were used in seven scenarios that were different in the degree of nonlinearity and nonadditivity associations between the exposure and covariates. Estimation of propensity scores was considered a classification task and performance metrics that included classification accuracy, receiver operating characteristic curve area under the curve (AUCROC), covariate balance, standard error, absolute bias, and the 95\% confidence interval coverage were evaluated for each model. Our simulation results show that deep learning models (CNN, DNN, and CNN-LSTM) outperformed LR in the estimation of the propensity score. CNN and CNN-LSTM achieved good results for covariate balance, classification accuracy, AUCROC, and Cohen’s Kappa. Although LR provided substantially better bias reduction, it produced subpar performance based on classification accuracy, AUCROC, Cohen’s Kappa, and 95\% confidence interval coverage compared to the deep learning models. The results suggest that deep learning methods, especially CNN, may be useful for estimating propensity scores that are used to estimate causal effects.}}, 
pages = {2813--2827}, 
volume = {10}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/IEEE%20Access/Evaluating%20Uses%20of%20Deep%20Learning%20Methods%20for%20Causal%20Inference_2022_IEEE%20Access_1.pdf}
}
@article{Wendling.2018, 
year = {2018}, 
title = {{Comparing methods for estimation of heterogeneous treatment effects using observational data from health care databases}}, 
author = {Wendling, T. and Jung, K. and Callahan, A. and Schuler, A. and Shah, N. H. and Gallego, B.}, 
journal = {Statistics in Medicine}, 
issn = {0277-6715}, 
doi = {10.1002/sim.7820}, 
pmid = {29862536}, 
abstract = {{There is growing interest in using routinely collected data from health care databases to study the safety and effectiveness of therapies in “real‐world” conditions, as it can provide complementary evidence to that of randomized controlled trials. Causal inference from health care databases is challenging because the data are typically noisy, high dimensional, and most importantly, observational. It requires methods that can estimate heterogeneous treatment effects while controlling for confounding in high dimensions. Bayesian additive regression trees, causal forests, causal boosting, and causal multivariate adaptive regression splines are off‐the‐shelf methods that have shown good performance for estimation of heterogeneous treatment effects in observational studies of continuous outcomes. However, it is not clear how these methods would perform in health care database studies where outcomes are often binary and rare and data structures are complex. In this study, we evaluate these methods in simulation studies that recapitulate key characteristics of comparative effectiveness studies. We focus on the conditional average effect of a binary treatment on a binary outcome using the conditional risk difference as an estimand. To emulate health care database studies, we propose a simulation design where real covariate and treatment assignment data are used and only outcomes are simulated based on nonparametric models of the real outcomes. We apply this design to 4 published observational studies that used records from 2 major health care databases in the United States. Our results suggest that Bayesian additive regression trees and causal boosting consistently provide low bias in conditional risk difference estimates in the context of health care database studies.}}, 
pages = {3309--3324}, 
number = {23}, 
volume = {37}
}
@article{Einav.2014, 
year = {2014}, 
title = {{Economics in the age of big data}}, 
author = {Einav, Liran and Levin, Jonathan}, 
journal = {Science}, 
issn = {0036-8075}, 
doi = {10.1126/science.1243089}, 
pmid = {25378629}, 
abstract = {{The quality and quantity of data on economic activity are expanding rapidly. Empirical research increasingly relies on newly available large-scale administrative data or private sector data that often is obtained through collaboration with private firms. Here we highlight some challenges in accessing and using these new data. We also discuss how new data sets may change the statistical methods used by economists and the types of questions posed in empirical research.}}, 
pages = {1243089}, 
number = {6210}, 
volume = {346}
}
@article{Song.2020, 
year = {2020}, 
title = {{Using Administrative Big Data to Solve Problems in Social Science.pdf}}, 
author = {Song, Xi and Coleman, Thomas S.}, 
journal = {University of Pennsylvania Population Center Working Paper (PSC/PARC)}, 
url = {https://repository.upenn.edu/psc\_publications/58/}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/University%20of%20Pennsylvania%20Population%20Center%20Working%20Paper%20(PSC-PARC)/Using%20Administrative%20Big%20Data%20to%20Solve%20Problems%20in%20Social%20Science.pdf_2020_University%20of%20Pennsylvania%20Population%20Center%20Working%20Paper%20(PSC-PARC)_1.pdf}
}
@article{Figlio.2017, 
year = {2017}, 
title = {{The Promise of Administrative Data in Education Research}}, 
author = {Figlio, David and Karbownik, Krzysztof and Salvanes, Kjell}, 
journal = {Education Finance and Policy}, 
issn = {1557-3060}, 
doi = {10.1162/edfp\_a\_00229}, 
abstract = {{Thanks to extraordinary and exponential improvements in data storage and computing capacities, it is now possible to collect, manage, and analyze data in magnitudes and in manners that would have been inconceivable just a short time ago. As the world has developed this remarkable capacity to store and analyze data, so have the world's governments developed large-scale, comprehensive datafiles on tax programs, workforce information, benefit programs, health, and education. Although these data are collected for purely administrative purposes, they represent remarkable new opportunities for expanding our knowledge. We describe some of the benefits and challenges associated with the use of administrative data in education research.}}, 
pages = {129--136}, 
number = {2}, 
volume = {12}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Education%20Finance%20and%20Policy/The%20Promise%20of%20Administrative%20Data%20in%20Education%20Research_2017_Education%20Finance%20and%20Policy_1.pdf}
}
@article{Tan.2019rgh, 
year = {2019}, 
title = {{Regularized calibrated estimation of propensity scores with model misspecification and high-dimensional data}}, 
author = {Tan, Z}, 
journal = {Biometrika}, 
issn = {0006-3444}, 
doi = {10.1093/biomet/asz059}, 
abstract = {{Summary Propensity scores are widely used with inverse probability weighting to estimate treatment effects in observational studies. We study calibrated estimation as an alternative to maximum likelihood estimation for fitting logistic propensity score models. We show that, with possible model misspecification, minimizing the expected calibration loss underlying the calibrated estimators involves reducing both the expected likelihood loss and a measure of relative errors between the limiting and true propensity scores, which governs the mean squared errors of inverse probability weighted estimators. Furthermore, we derive a regularized calibrated estimator by minimizing the calibration loss with a lasso penalty. We develop a Fisher scoring descent algorithm for computing the proposed estimator and provide a high-dimensional analysis of the resulting inverse probability weighted estimators, leveraging the control of relative errors of propensity scores for calibrated estimation. We present a simulation study and an empirical application to demonstrate the advantages of the proposed methods over maximum likelihood and its regularization. The methods are implemented in the R package RCAL.}}, 
pages = {137--158}, 
number = {1}, 
volume = {107}
}
@article{Colter.2022, 
year = {2022}, 
title = {{Explained: Artificial Intelligence for Propensity Score Estimation in  Explained: Artificial Intelligence for Propensity Score Estimation in  Multilevel Educational Settings  Multilevel Educational Settings}}, 
author = {Colter, Zachary K. and Zhang, Haobai and Liu, Liu}, 
journal = {Practical Assessment, Research, and Evaluation}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Practical%20Assessment,%20Research,%20and%20Evaluation/Explained-%20Artificial%20Intelligence%20for%20Propensity%20Score%20Estimation%20in%20%20Explained-%20Artificial%20Intelligence%20for%20Propensity%20Score%20Estimation%20in%20%20Multilevel%20Educational%20Settings%20%20Multilevel%20Educational%20Settings_2022_Practical%20Assessment,%20Research,%20an.pdf}
}
@article{Weberpals.2021, 
year = {2021}, 
title = {{Deep Learning-based Propensity Scores for Confounding Control in Comparative Effectiveness Research: A Large-scale, Real-world Data Study}}, 
author = {Weberpals, Janick and Becker, Tim and Davies, Jessica and Schmich, Fabian and Rüttinger, Dominik and Theis, Fabian J and Bauer-Mehren, Anna}, 
journal = {Epidemiology}, 
issn = {1044-3983}, 
doi = {10.1097/ede.0000000000001338}, 
pmid = {33591049}, 
abstract = {{Due to the non-randomized nature of real-world data, prognostic factors need to be balanced, which is often done by propensity scores (PSs). This study aimed to investigate whether autoencoders, which are unsupervised deep learning architectures, might be leveraged to compute PS.}}, 
pages = {378--388}, 
number = {3}, 
volume = {32}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Epidemiology/Deep%20Learning-based%20Propensity%20Scores%20for%20Confounding%20Control%20in%20Comparative%20Effectiveness%20Research-%20A%20Large-scale,%20Real-world%20Data%20Study_2021_Epidemiology.pdf}
}
@article{Lee.2017, 
year = {2017}, 
title = {{A practical guide to propensity score analysis for applied clinical research}}, 
author = {Lee, Jaehoon and Little, Todd D.}, 
journal = {Behaviour Research and Therapy}, 
issn = {0005-7967}, 
doi = {10.1016/j.brat.2017.01.005}, 
pmid = {28153337}, 
abstract = {{ Observational studies are often the only viable options in many clinical settings, especially when it is unethical or infeasible to randomly assign participants to different treatment régimes. In such case propensity score (PS) analysis can be applied to accounting for possible selection bias and thereby addressing questions of causal inference. Many PS methods exist, yet few guidelines are available to aid applied researchers in their conduct and evaluation of a PS analysis. In this article we give an overview of available techniques for PS estimation and application, balance diagnostic, treatment effect estimation, and sensitivity assessment, as well as recent advances. We also offer a tutorial that can be used to emulate the steps of PS analysis. Our goal is to provide information that will bring PS analysis within the reach of applied clinical researchers and practitioners.}}, 
pages = {76--90}, 
volume = {98}
}
@article{Zhao.2021, 
year = {2021}, 
title = {{Propensity score matching with R: conventional methods and new features}}, 
author = {Zhao, Qin-Yu and Luo, Jing-Chao and Su, Ying and Zhang, Yi-Jie and Tu, Guo-Wei and Luo, Zhe}, 
journal = {Annals of Translational Medicine}, 
issn = {2305-5847}, 
doi = {10.21037/atm-20-3998}, 
pmid = {34268425}, 
pmcid = {PMC8246231}, 
abstract = {{Propensity score matching with R: conventional methods and new features}}, 
pages = {812--812}, 
number = {9}, 
volume = {9}
}
@article{Guo.2009, 
year = {2009}, 
title = {{Propensity Score Analysis: Statistical Methods and Applications}}, 
author = {Guo, Shenyang and Guo, Shenyang and Fraser, Mark W.}, 
journal = {null}, 
doi = {null}, 
abstract = {{List of Tables List of Figures Preface About the Authors Chapter 1: Introduction Observational Studies History and Development Randomized Experiments Why and When a Propensity Score Analysis Is Needed Computing Software Packages Plan of the Book Chapter 2: Counterfactual Framework and Assumptions Causality, Internal Validity, and Threats Counterfactuals and the Neyman-Rubin Counterfactual Framework The Ignorable Treatment Assignment Assumption The Stable Unit Treatment Value Assumption Methods for Estimating Treatment Effects The Underlying Logic of Statistical Inference Types of Treatment Effects Treatment Effect Heterogeneity Heckman's Econometric Model of Causality Conclusion Chapter 3: Conventional Methods for Data Balancing Why Is Data Balancing Necessary? A Heuristic Example Three Methods for Data Balancing Design of the Data Simulation Results of the Data Simulation Implications of the Data Simulation Key Issues Regarding the Application of OLS Regression Conclusion Chapter 4: Sample Selection and Related Models The Sample Selection Model Treatment Effect Model Overview of the Stata Programs and Main Features of treatreg Examples Conclusion Chapter 5: Propensity Score Matching and Related Models Overview The Problem of Dimensionality and the Properties of Propensity Scores Estimating Propensity Scores Matching Postmatching Analysis Propensity Score Matching With Multilevel Data Overview of the Stata and R Programs Examples Conclusion Chapter 6: Propensity Score Subclassification Overview The Overlap Assumption and Methods to Address Its Violation Structural Equation Modeling With Propensity Score Subclassification The Stratification-Multilevel Method Examples Conclusion Chapter 7: Propensity Score Weighting Overview Weighting Estimators Examples Conclusion Chapter 8: Matching Estimators Overview Methods of Matching Estimators Overview of the Stata Program nnmatch Examples Conclusion Chapter 9: Propensity Score Analysis With Nonparametric Regression Overview Methods of Propensity Score Analysis With Nonparametric Regression Overview of the Stata Programs psmatch2 and bootstrap Examples Conclusion Chapter 10: Propensity Score Analysis of Categorical or Continuous Treatments Overview Modeling Doses With a Single Scalar Balancing Score Estimated by an Ordered Logistic Regression Modeling Doses With Multiple Balancing Scores Estimated by a Multinomial Logit Model The Generalized Propensity Score Estimator Overview of the Stata gpscore Program Examples Conclusion Chapter 11: Selection Bias and Sensitivity Analysis Selection Bias: An Overview A Monte Carlo Study Comparing Corrective Models Rosenbaum's Sensitivity Analysis Overview of the Stata Program rbounds Examples Conclusion Chapter 12: Concluding Remarks Common Pitfalls in Observational Studies: A Checklist for Critical Review Approximating Experiments With Propensity Score Approaches Other Advances in Modeling Causality Directions for Future Development References Index}}
}
@article{Rosenbaum.1984, 
year = {1984}, 
title = {{Reducing Bias in Observational Studies Using Subclassification on the Propensity Score}}, 
author = {Rosenbaum, Paul R and Rubin, Donald B}, 
journal = {Journal of the American Statistical Association}, 
issn = {0162-1459}, 
doi = {10.2307/2288398}, 
abstract = {{Abstract The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Previous theoretical arguments have shown that subclassification on the propensity score will balance all observed covariates. Subclassification on an estimated propensity score is illustrated, using observational data on treatments for coronary artery disease. Five subclasses defined by the estimated propensity score are constructed that balance 74 covariates, and thereby provide estimates of treatment effects using direct adjustment. These subclasses are applied within sub-populations, and model-based adjustments are then used to provide estimates of treatment effects within these sub-populations. Two appendixes address theoretical issues related to the application: the effectiveness of subclassification on the propensity score in removing bias, and balancing properties of propensity scores with incomplete data.}}, 
pages = {516}, 
number = {387}, 
volume = {79}
}
@article{Rosenbaum.1987, 
year = {1987}, 
title = {{Model-Based Direct Adjustment}}, 
author = {Rosenbaum, Paul R.}, 
journal = {Journal of the American Statistical Association}, 
issn = {0162-1459}, 
doi = {10.1080/01621459.1987.10478441}, 
abstract = {{Direct adjustment or standardization applies population weights to subclass means in an effort to estimate population quantities from a sample that is not representative of the population. Direct adjustment has several attractive features, but when there are many subclasses it can attach large weights to small quantities of data, often in a fairly erratic manner. In the extreme, direct adjustment can attach infinite weight to nonexistent data, a noticeable inconvenience in practice. This article proposes a method of model-based direct adjustment that preserves the attractive features of conventional direct adjustment while stabilizing the weights attached to small subclasses. The sample mean and conventional direct adjustment are both special cases of model-based direct adjustment under two different extreme models for the subclass-specific selection probabilities. The discussion of this method provides some insights into the behavior of true and estimated propensity scores: the estimated scores are better than the true ones for almost the same reason that direct adjustment can outperform the sample mean in a simple random sample. The method is applied to a nonrandom sample in an effort to estimate a discrete distribution of essay scores in the College Board's Advanced Placement Examination in Biology.}}, 
pages = {387--394}, 
number = {398}, 
volume = {82}
}
@article{Collier.2021lp, 
year = {2021}, 
title = {{Estimating propensity scores using neural networks and traditional methods: a comparative simulation study}}, 
author = {Collier, Zachary K and Leite, Walter L and Zhang, Haobai}, 
journal = {Communications in Statistics - Simulation and Computation}, 
issn = {0361-0918}, 
doi = {10.1080/03610918.2021.1963455}, 
abstract = {{Neural networks are a contending data mining procedure to estimate propensity scores due to its robustness to non-normal residual distributions, ability to detect complex nonlinear relationships be...}}, 
pages = {1--16}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Communications%20in%20Statistics%20-%20Simulation%20and%20Computation/Estimating%20propensity%20scores%20using%20neural%20networks%20and%20traditional%20methods-%20a%20comparative%20simulation%20study_2021_Communications%20in%20Statistics%20-%20Simulation%20and%20Computation_1.pdf}
}
@article{Guzman-Alvarez.2022, 
year = {2022}, 
title = {{Deep Neural Networks for Propensity Score Estimation}}, 
author = {Guzman-Alvarez, Alberto and Qin, Xu and Scott, Paul W.}, 
journal = {Multivariate Behavioral Research}, 
issn = {0027-3171}, 
doi = {10.1080/00273171.2021.2011695}, 
pmid = {35007456}, 
abstract = {{Propensity score methods are widely used for causal inference (Rosenbaum, 2020; Thoemmes \& Kim, 2011). However, traditional propensity score estimation methods (e.g., logistic regression) degrade in performance when applied to high-dimensional data common in psychological and educational studies (D’Amour et al., 2021). Nevertheless, computer science has generated a wealth of research on a potential solution: Deep Neural Networks (DNNs) (Deng \& Yu, 2014). DNNs are algorithms based on our brain’s neural architecture and are common in industry to model complex prediction/classification tasks. Despite the advantages in modeling high-dimensional data, no work has applied DNNs to propensity score analysis. The traditional logistic regression model for propensity score estimation tends to worsen in performance as the number of covariates increases in several ways (Hill et al., 2011); including failure to converge, overfitting the data, or increased difficultly to include all interactions and non-linearities. If the propensity score model is misspecified, there may be inadequate overlap between the estimated propensity scores of the treatment and control groups. The insufficient overlap makes it challenging to balance the groups based on their pretreatment characteristics, leading to biased treatment effects (Rosenbaum, 2020). We evaluated the performance of DNNs (via simulation) against the traditional logistic model and other more sophisticated data mining/machine learning techniques, including classification and regression trees, Bayesian additive regression trees, neural networks, and random forest. We generated datasets that varied in dimensionality and sample size and included covariates of various scales to reflect the diverse datasets encountered by psychological and educational researchers. To represent complicated relationships between variables, treatment assignment, and the outcome that exist in real applications, we extended the simulation literature by including non-linearities and higherorder terms in the data generating models of the treatment and outcome. We evaluated the performance of DNNs when implemented in both propensity score-based weighting and propensity score-based matching by using the Matching package in R. The evaluation criteria included the degree of covariate balance, bias, standard error, mean squared error, 95\% confidence interval coverage rate of the treatment effect estimate, and the robustness of the method to unmeasured confounding. Results showed the advantage of DNN over the traditional logistic model and a range of machine learning approaches currently available in high-dimensional data, especially when treatment selection is complex. Our method overcomes important limitations of existing propensity score estimation approaches under high-dimensional data and complex treatment selection.}}, 
pages = {164--165}, 
number = {1}, 
volume = {57}
}
@article{Collier.2022, 
year = {2021}, 
title = {{A Tutorial on Artificial Neural Networks in Propensity Score Analysis}}, 
author = {Collier, Zachary K. and Leite, Walter L.}, 
journal = {The Journal of Experimental Education}, 
issn = {0022-0973}, 
doi = {10.1080/00220973.2020.1854158}, 
abstract = {{Artificial neural networks (NN) can help researchers estimate propensity scores for quasi-experimental estimation of treatment effects because they can automatically detect complex interactions involving many covariates. However, NN is difficult to implement due to the complexity of choosing an algorithm for various treatment levels and monitoring model performance. This research aims to develop a tutorial to facilitate the use of NN to derive causal inferences. The tutorial provides social scientists with a gentle overview of machine learning terminology and best practices for training, validating, and testing NN to estimate propensity scores. The veracity of NN is demonstrated in this study using data on 5,770 teachers from the Beginner Teacher Longitudinal Study. Propensity score analysis was used to estimate the effects of assigning mentors to new teachers on the probability of continuing in the teaching profession. The results show that NN provided a better covariate balance between treatment versions than multinomial logistic regression and generalized boosted modeling. The study's findings align with previous research showing NN's advantages over conventional propensity score estimation methods.}}, 
pages = {1003--1020}, 
number = {4}, 
volume = {90}
}
@article{Morris.2018, 
year = {2018}, 
keywords = {experimental design,monte carlo,reporting,simulation studies}, 
title = {{Using simulation studies to evaluate statistical methods}}, 
author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.}, 
journal = {Statistics in Medicine}, 
issn = {0277-6715}, 
doi = {10.1002/sim.8086}, 
pmid = {30652356}, 
pmcid = {PMC6492164}, 
eprint = {1712.03198}, 
abstract = {{Simulation studies are computer experiments that involve creating data by pseudo‐random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods because some “truth” (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analyzed, and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting, and presentation. In particular, this tutorial provides a structured approach for planning and reporting simulation studies, which involves defining aims, data‐generating mechanisms, estimands, methods, and performance measures (“ADEMP”); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine, which included at least one simulation study and identify areas for improvement.}}, 
pages = {2074--2102}, 
number = {11}, 
volume = {38}, 
note = {arXiv: 1712.03198}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Statistics%20in%20Medicine/Using%20simulation%20studies%20to%20evaluate%20statistical%20methods_2018_Statistics%20in%20Medicine.pdf}
}
@article{Gopalan.2020, 
year = {2020}, 
title = {{Use of Quasi-Experimental Research Designs in Education Research: Growth, Promise, and Challenges}}, 
author = {Gopalan, Maithreyi and Rosinger, Kelly and Ahn, Jee Bin}, 
journal = {Review of Research in Education}, 
issn = {0091-732X}, 
doi = {10.3102/0091732x20903302}, 
url = {https://doi.org/10.3102/0091732X20903302}, 
abstract = {{In the past few decades, we have seen a rapid proliferation in the use of quasi-experimental research designs in education research. This trend, stemming in part from the “credibility revolution” in the social sciences, particularly economics, is notable along with the increasing use of randomized controlled trials in the strive toward rigorous causal inference. The overarching purpose of this chapter is to explore and document the growth, applicability, promise, and limitations of quasi-experimental research designs in education research. We first provide an overview of widely used quasi-experimental research methods in this growing literature, with particular emphasis on articles from the top ranked education research journals, including those published by the American Educational Research Association. Next, we demonstrate the applicability and promise of these methods in enhancing our understanding of the causal effects of education policies and interventions using key examples and case studies culled from the extant literature across the pre-K–16 education spectrum. Finally, we explore the limitations of these methods and conclude with thoughts on how education researchers can adapt these innovative, interdisciplinary techniques to further our understanding of some of the most enduring questions in educational policy and practice.}}, 
shorttitle = {Use of Quasi-Experimental Research Designs in Education Research}, 
pages = {218--243}, 
number = {1}, 
volume = {44}, 
note = {ZSCC: NoCitationData[s0]  Publisher: American Educational Research Association}, 
keywords = {}
}
@article{Bai.2011, 
year = {2011}, 
title = {{Using Propensity Score Analysis for Making Causal Claims in Research Articles}}, 
author = {Bai, Haiyan}, 
journal = {Educational Psychology Review}, 
issn = {1573-336X}, 
url = {https://link.springer.com/article/10.1007/s10648-011-9164-9}, 
abstract = {{The central role of the propensity score analysis (PSA) in observational studies is for causal inference; as such, PSA is often used for making causal claims in research articles. However, there are still some issues for researchers to consider when making claims of causality using PSA results. This summary first briefly reviews PSA, followed by discussions of its effectiveness and limitations. Finally, a guideline of how to address these concerns is also provided for researchers to make appropriate causal claims using PSA results in their research articles.}}, 
pages = {273---278}, 
number = {2}, 
volume = {23}, 
language = {en}, 
copyright = {2011 Springer Science+Business Media, LLC}, 
note = {ZSCC: 0000037  Company: Springer
Distributor: Springer
Institution: Springer
Label: Springer
Number: 2
Publisher: Springer US}, 
keywords = {}
}
@article{Mullainathan.2017, 
year = {2017}, 
title = {{Machine Learning: An Applied Econometric Approach}}, 
author = {Mullainathan, Sendhil and Spiess, Jann}, 
journal = {Journal of Economic Perspectives}, 
issn = {0895-3309}, 
doi = {10.1257/jep.31.2.87}, 
url = {http://pubs.aeaweb.org/doi/10.1257/jep.31.2.87}, 
abstract = {{Machines are increasingly doing “intelligent” things. Face recognition algorithms use a large dataset of photos labeled as having a face or not to estimate a function that predicts the presence y of a face from pixels x. This similarity to econometrics raises questions: How do these new empirical tools fit with what we know? As empirical economists, how can we use them? We present a way of thinking about machine learning that gives it its own place in the econometric toolbox. Machine learning not only provides new tools, it solves a different problem. Specifically, machine learning revolves around the problem of prediction, while many economic applications revolve around parameter estimation. So applying machine learning to economics requires finding relevant tasks. Machine learning algorithms are now technically easy to use: you can download convenient packages in R or Python. This also raises the risk that the algorithms are applied naively or their output is misinterpreted. We hope to make them conceptually easier to use by providing a crisper understanding of how these algorithms work, where they excel, and where they can stumble—and thus where they can be most usefully applied.}}, 
pages = {87--106}, 
number = {2}, 
volume = {31}, 
keywords = {}
}
@article{Farrell.2021, 
year = {2021}, 
title = {{Deep Neural Networks for Estimation and Inference}}, 
author = {Farrell, Max H. and Liang, Tengyuan and Misra, Sanjog}, 
journal = {Econometrica}, 
issn = {0012-9682}, 
doi = {10.3982/ecta16901}, 
eprint = {1809.09953}, 
abstract = {{We study deep neural networks and their use in semiparametric inference. We establish novel nonasymptotic high probability bounds for deep feedforward neural nets. These deliver rates of convergence that are sufficiently fast (in some cases minimax optimal) to allow us to establish valid second‐step inference after first‐step estimation with deep learning, a result also new to the literature. Our nonasymptotic high probability bounds, and the subsequent semiparametric inference, treat the current standard architecture: fully connected feedforward neural networks (multilayer perceptrons), with the now‐common rectified linear unit activation function, unbounded weights, and a depth explicitly diverging with the sample size. We discuss other architectures as well, including fixed‐width, very deep networks. We establish the nonasymptotic bounds for these deep nets for a general class of nonparametric regression‐type loss functions, which includes as special cases least squares, logistic regression, and other generalized linear models. We then apply our theory to develop semiparametric inference, focusing on causal parameters for concreteness, and demonstrate the effectiveness of deep learning with an empirical application to direct mail marketing.}}, 
pages = {181--213}, 
number = {1}, 
volume = {89}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Econometrica/Deep%20Neural%20Networks%20for%20Estimation%20and%20Inference_2021_Econometrica.pdf}
}
@article{Keller.2019, 
year = {2019}, 
title = {{Variable Selection for Causal Effect Estimation: Nonparametric Conditional Independence Testing With Random Forests}}, 
author = {Keller, Bryan}, 
journal = {Journal of Educational and Behavioral Statistics}, 
issn = {1076-9986}, 
doi = {10.3102/1076998619872001}, 
abstract = {{Widespread availability of rich educational databases facilitates the use of conditioning strategies to estimate causal effects with nonexperimental data. With dozens, hundreds, or more potential predictors, variable selection can be useful for practical reasons related to communicating results and for statistical reasons related to improving the efficiency of estimators. Background knowledge should take precedence in deciding which variables to retain. However, with many potential predictors, theory may be weak, such that functional form relationships are likely to be unknown. In this article, I propose a nonparametric method for data-driven variable selection based on permutation testing with conditional random forest variable importance. The algorithm automatically handles nonlinear relationships and interactions in its naive implementation. Through a series of Monte Carlo simulation studies and a case study with Early Childhood Longitudinal Study–K data, I find that the method performs well across a variety of scenarios where other methods fail.}}, 
pages = {119--142}, 
number = {2}, 
volume = {45}
}
@article{Pang.2019, 
year = {2019}, 
keywords = {ML}, 
title = {{Deep Learning With TensorFlow: A Review}}, 
author = {Pang, Bo and Nijkamp, Erik and Wu, Ying Nian}, 
journal = {Journal of Educational and Behavioral Statistics}, 
issn = {1076-9986}, 
doi = {10.3102/1076998619872761}, 
abstract = {{This review covers the core concepts and design decisions of TensorFlow. TensorFlow, originally created by researchers at Google, is the most popular one among the plethora of deep learning libraries. In the field of deep learning, neural networks have achieved tremendous success and gained wide popularity in various areas. This family of models also has tremendous potential to promote data analysis and modeling for various problems in educational and behavioral sciences given its flexibility and scalability. We give the reader an overview of the basics of neural network models such as the multilayer perceptron, the convolutional neural network, and stochastic gradient descent, the most commonly used optimization method for neural network models. However, the implementation of these models and optimization algorithms is time-consuming and error-prone. Fortunately, TensorFlow greatly eases and accelerates the research and application of neural network models. We review several core concepts of TensorFlow such as graph construction functions, graph execution tools, and TensorFlow’s visualization tool, TensorBoard. Then, we apply these concepts to build and train a convolutional neural network model to classify handwritten digits. This review is concluded by a comparison of low- and high-level application programming interfaces and a discussion of graphical processing unit support, distributed training, and probabilistic modeling with TensorFlow Probability library.}}, 
pages = {227--248}, 
number = {2}, 
volume = {45}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Journal%20of%20Educational%20and%20Behavioral%20Statistics/Deep%20Learning%20With%20TensorFlow-%20A%20Review_2019_Journal%20of%20Educational%20and%20Behavioral%20Statistics_1.pdf}
}
@article{Schmidhuber.2015, 
year = {2015}, 
keywords = {include}, 
title = {{Deep learning in neural networks: An overview}}, 
author = {Schmidhuber, Jürgen}, 
journal = {Neural Networks}, 
issn = {0893-6080}, 
doi = {10.1016/j.neunet.2014.09.003}, 
pmid = {25462637}, 
eprint = {1404.7828}, 
abstract = {{In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.}}, 
pages = {85--117}, 
volume = {61}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Neural%20Networks/Deep%20learning%20in%20neural%20networks-%20An%20overview_2015_Neural%20Networks.pdf}
}
@article{Olmuş.2022, 
year = {2022}, 
title = {{Performance evaluation of some propensity score matching methods by using binary logistic regression model}}, 
author = {Olmuş, Hülya and Beşpınar, Esra and Nazman, Ezgi}, 
journal = {Communications in Statistics - Simulation and Computation}, 
issn = {0361-0918}, 
doi = {10.1080/03610918.2019.1679181}, 
abstract = {{The unit selection bias in treatment and control group affects negatively in evaluation of treatment effects. In some studies, the random units selected for the treatment and control group are out of control of the researcher and there may be differences between the units under consideration. This will cause the estimates obtained to be biased. Propensity score matching (PSM) has been used to reduce bias in estimation of treatment effect in observational data. Therefore, nearest neighbor (1:1), caliper, stratification, mahalanobis metric, full and combining propensity score and mahalanobis metric matching, which have been widely used as PSM methods, were compared in terms of the correct classification rates conducting a detailed Monte Carlo simulation study. In addition, standardized and percent reduction bias of covariates were evaluated for each of the PSM methods. It is suggested that stratification and full matching methods should be considered to study with high correct classification rate whereas caliper matching method should be prefered due to the low bias to make statistical inferences.}}, 
pages = {1647--1660}, 
number = {4}, 
volume = {51}
}
@article{Linden.2017, 
year = {2017}, 
title = {{Using classification tree analysis to generate propensity score weights}}, 
author = {Linden, Ariel and Yarnold, Paul R.}, 
journal = {Journal of Evaluation in Clinical Practice}, 
issn = {1356-1294}, 
doi = {10.1111/jep.12744}, 
pmid = {28371206}, 
abstract = {{In evaluating non‐randomized interventions, propensity scores (PS) estimate the probability of assignment to the treatment group given observed characteristics. Machine learning algorithms have been proposed as an alternative to conventional logistic regression for modelling PS in order to avoid limitations of linear methods. We introduce classification tree analysis (CTA) to generate PS which is a “decision‐tree”‐like classification model that provides accurate, parsimonious decision rules that are easy to display and interpret, reports P values derived via permutation tests, and evaluates cross‐generalizability. Using empirical data, we identify all statistically valid CTA PS models and then use them to compute strata‐specific, observation‐level PS weights that are subsequently applied in outcomes analyses. We compare findings obtained using this framework to logistic regression and boosted regression, by evaluating covariate balance using standardized differences, model predictive accuracy, and treatment effect estimates obtained using median regression and a weighted CTA outcomes model. While all models had some imbalanced covariates, main‐effects logistic regression yielded the lowest average standardized difference, whereas CTA yielded the greatest predictive accuracy. Nevertheless, treatment effect estimates were generally consistent across all models. Assessing standardized differences in means as a test of covariate balance is inappropriate for machine learning algorithms that segment the sample into two or more strata. Because the CTA algorithm identifies all statistically valid PS models for a sample, it is most likely to identify a correctly specified PS model, and should be considered as an alternative approach to modeling the PS.}}, 
pages = {703--712}, 
number = {4}, 
volume = {23}
}
@article{Wyss.2014, 
year = {2014}, 
keywords = {include,read}, 
title = {{The Role of Prediction Modeling in Propensity Score Estimation: An Evaluation of Logistic Regression, bCART, and the Covariate-Balancing Propensity Score}}, 
author = {Wyss, Richard and Ellis, Alan R. and Brookhart, M. Alan and Girman, Cynthia J. and Funk, Michele Jonsson and LoCasale, Robert and Stürmer, Til}, 
journal = {American Journal of Epidemiology}, 
issn = {0002-9262}, 
doi = {10.1093/aje/kwu181}, 
pmid = {25143475}, 
pmcid = {PMC4157700}, 
abstract = {{The covariate-balancing propensity score (CBPS) extends logistic regression to simultaneously optimize covariate balance and treatment prediction. Although the CBPS has been shown to perform well in certain settings, its performance has not been evaluated in settings specific to pharmacoepidemiology and large database research. In this study, we use both simulations and empirical data to compare the performance of the CBPS with logistic regression and boosted classification and regression trees. We simulated various degrees of model misspecification to evaluate the robustness of each propensity score (PS) estimation method. We then applied these methods to compare the effect of initiating glucagonlike peptide-1 agonists versus sulfonylureas on cardiovascular events and all-cause mortality in the US Medicare population in 2007–2009. In simulations, the CBPS was generally more robust in terms of balancing covariates and reducing bias compared with misspecified logistic PS models and boosted classification and regression trees. All PS estimation methods performed similarly in the empirical example. For settings common to pharmacoepidemiology, logistic regression with balance checks to assess model specification is a valid method for PS estimation, but it can require refitting multiple models until covariate balance is achieved. The CBPS is a promising method to improve the robustness of PS models.}}, 
shorttitle = {The role of prediction modeling in propensity score estimation}, 
pages = {645--655}, 
number = {6}, 
volume = {180}, 
note = {Notes from Reading: 
This article looks at covariate-balancing PS with extends logiction regression via a method of moments estimation. Should in practice optimize covariatle blaance and teratment prediction.




Interesting point brough up by Westreich et al. that the purpose of PS is not to predict treatment asingment but to balance covriates in order to control for counfounding.




PS model estimate using MLE in acutally find parameterr esitmates that maximize an assumed likelihood fucntion rathen than mazimize covariate balance




CBS proposed my Imai




Compared CBPS LR and bCART (boosted)




Once again use Setoguchi simulation method.

Good paper to show all the conditions of Setoguchi.




Also used a probit mode, why?

*In education we rarely observe binary outcomes therefore we simulate a continous outcome to simplify the analysis.




CBPS outperformed all other methods, useful to use this as a test for DNN.

Cite a study that looks at how imporoving the predictive performance of PS through variable selection did not necessarily correpond with improved counfonunding control.






Extracted Annotations (7/26/2020, 10:16:44 PM)

"The covariate-balancing propensity score (CBPS) extends logistic regression to simultaneously optimize covariate balance and treatment prediction. Although the CBPS has been shown to perform well in certain settings, its performance has not been evaluated in settings specific to pharmacoepidemiology and large database research. In this study, we use both simulations and empirical data to compare the performance of the CBPS with logistic regression and boosted classification and regression trees. We simulated various degrees of model misspecification to evaluate the robustness of each propensity score (PS) estimation method. We then applied these methods to compare the effect of initiating glucagonlike peptide-1 agonists versus sulfonylureas on cardiovascular events and all-cause mortality in the US Medicare population in 2007-2009. In simulations, the CBPS was generally more robust in terms of balancing covariates and reducing bias compared with misspecified logistic PS models and boosted classification and regression trees. All PS estimation methods performed similarly in the empirical example. For settings common to pharmacoepidemiology, logistic regression with balance checks to assess model specification is a valid method for PS estimation, but it can require refitting multiple models until covariate balance is achieved. The CBPS is a promising method to improve the robustness of PS models. cardiovascular disease; covariate balance; diabetes; epidemiologic" (Wyss et al 2014:645)

"The propensity score (PS), defined as the conditional probability of treatment given a set of observed covariates, has been shown to effectively balance measured covariates across treatment groups in comparative observational studies (1)." (Wyss et al 2014:645)

"In practice, the true PS function is unknown and must be estimated from the available data. PS estimation has often been viewed as prediction modeling because misspecified or biased predictions of the true PS, in general, result in estima" (Wyss et al 2014:645)

"The topic of prediction modeling in PS estimation has been discussed in terms of variable selection for PS models. Westreich et al. (6) emphasized that the purpose of PSs is not D h g 645 Am J Epidemiol. 2014;180(6):645-655" (Wyss et al 2014:645)

"646 Wyss et al. to predict treatment assignment, but to balance covariates in order to control for confounding, and that variable selection for PS models should not be approached with the objective of predicting treatment." (Wyss et al 2014:646)

"ve modeling approach, it is unclear what role prediction modeling should play" (Wyss et al 2014:646)

"When misspecification does occur, parameter estimates that maximize thefit of the data, or minimize prediction error, may not correspond with parameter estimates that maximize covariate balance." (Wyss et al 2014:646)

"Recently, various authors have proposed PS estimation methods that focus on minimizing the imbalance of covariates when estimating parameters of parametric PS models (15-17)." (Wyss et al 2014:646)

"We then evaluate the performance of the CBPS compared with logistic regression and boosted classification and regression trees (bCART) using simulations and empirical data." (Wyss et al 2014:646)

"We simulated treatment assignment using a similar structure to that described by Setoguchi et al. (18) and Lee et al. (19)." (Wyss et al 2014:647)

"The reason for adding this second set of simulations is that, in most practical settings, parametric models are only approximations of the true functional form underlying the data. Therefore, we followed the example of Brookhart et al. (7) by using a probit model to simulate treatment assignment to reflect settings where there is some misspecification in the functional form of the logistic PS model." (Wyss et al 2014:648)

"Although binary outcomes are more common in pharmacoepidemiology, we chose to simulate a continuous outcome to simplify the analysis and to avoid issues with the noncollapsibility of the odds ratio (22)." (Wyss et al 2014:648)

"For bCART, we used a maximum of 50,000 iterations with an iteration stopping point that minimized the mean of the Kolmogorov-Smirnov test statistic. We used 2 different parameter settings for the interaction depth. Because the scenarios in this simulation involve, at most, 2-way interactions, we used boosted classification and regression trees with an interaction depth of 2 (bCART2). In practice, the optimal interaction depth is unknown. Therefore, we also followed the recommendation of McCaffrey et al. (10), using boosted classification and regression trees with an interaction depth of 4 (bCART4). The bCART models were implemented using the twang package within the R statistical programming environment (24)." (Wyss et al 2014:649)

"We calculated the bias, defined as the expected value of the difference between the effect estimate and the true effect, by taking the mean of this difference over all simulation runs. The mean squared error (MSE)was calculated by taking the mean of the squared bias over all simulation runs. To evaluate precision, we estimated the standard error using the empirical standard deviationofthedistributionofthetreatmenteffectestimatesacross all simulation runs. We evaluated covariate balance by calculating the average standardized absolute mean difference (ASAMD) of the covariates across treatment groups. Because the data are simulated and the true PSs are known, we directly evaluated the mean prediction error for each PS model by calculating the absolute difference between the predicted PS and the true PS for each individual and then taking the mean of these differences across the entire population." (Wyss et al 2014:649)

"For the scenarios assessed in the simulation, the CBPS method outperformed the other methods in terms of covariate balance." (Wyss et al 2014:653)

"As with any simulation, however, the observed results are specific to the scenarios considered, and one should avoid generalizing results to settings that have not been evaluated." (Wyss et al 2014:653)

"Previous studies have shown that improving the predictive performance of a PS model through variable selection (e.g., including instrumental variables) does not necessarily correspond with improved confounding control (6-8)." (Wyss et al 2014:654)

"Researchers could improve covariate balance by refitting the logistic model using different functional forms of the covariates." (Wyss et al 2014:654)

"In pharmacoepidemiology and large database research, however, refitting the logistic PS model until an acceptable degree of balance is achieved can be difficult." (Wyss et al 2014:654)

"It is unlikely that any single PS estimation method is optimal in every setting. More work is needed to better understand the performance of bCART and the CBPS over a wide variety of parameter constellations common to pharmacoepidemiology. We conclude that logistic regression with balance checks to assess model specification is a viable PS estimation method, but the CBPS seems to be a promising alternative." (Wyss et al 2014:654)}
}
@article{Fan.2011, 
year = {2011}, 
keywords = {include,read}, 
title = {{Using propensity score matching in educational research}}, 
author = {Fan, Xitao and Nowell, Dana L}, 
journal = {Gifted Child Quarterly}, 
pages = {74---79}, 
number = {1}, 
volume = {55}, 
note = {Notes on Reading 
Promising title but focuses on gifted education

Would like to find a more current review of PS in Ed but nothing seems to come up

Adoption in Ed Research has been slow

As ed research moves as a field to placing more emphasis on the validity of causal inference, this will change

Review done with a data example but only use 2 covariates

As Ed Research get access to more fine grain data the advantages of DNN in high dimention data will be a positive.

Use ANOVA to access covariate balance

Connections:

Also use LR in line with review that mentioned most work uses LR, could be the same in ed research.}
}
@article{Rubin.2004, 
year = {2004}, 
rating = {5}, 
keywords = {include}, 
title = {{Teaching Statistical Inference for Causal Effects in Experiments and Observational Studies}}, 
author = {Rubin, Donald B.}, 
journal = {Journal of Educational and Behavioral Statistics}, 
issn = {1076-9986}, 
url = {https://doi.org/10.3102/10769986029003343}, 
abstract = {{Inference for causal effects is a critical activity in many branches of science and public policy. The field of statistics is the one field most suited to address such problems, whether from designed experiments or observational studies. Consequently, it is arguably essential that departments of statistics teach courses in causal inference to both graduate and undergraduate students. This article discusses an outline of such courses based on repeated experience over more than a decade.}}, 
pages = {343---367}, 
number = {3}, 
volume = {29}, 
language = {en}, 
note = {Rubin's thoughts on teaching causal inference to undergrad/grad students.  Future work: for a subgroup of folks LowIncome FAFSA completion is a intermediate varible on the way to the outcome college matriculation? }
}
@article{Keller.2015q8, 
year = {2015}, 
rating = {5}, 
keywords = {include,read}, 
title = {{Quantitative Psychology Research, The 79th Annual Meeting of the Psychometric Society, Madison, Wisconsin, 2014}}, 
author = {Keller, Bryan and Kim, Jee-Seon and Steiner, Peter M.}, 
journal = {Springer Proceedings in Mathematics \& Statistics}, 
issn = {2194-1009}, 
doi = {10.1007/978-3-319-19977-1\_20}, 
url = {http://link.springer.com/10.1007/978-3-319-19977-1\_20}, 
abstract = {{Neural networks have been noted as promising for propensity score estimation because they algorithmically handle nonlinear relationships and interactions. We examine the performance neural networks as compared with main-effects logistic regression for propensity score estimation via simulation study. When the main-effects logistic propensity score model is correctly specified, the two approaches yield almost identical mean square error. When the logistic propensity score model is misspecified due to the addition of quadratic terms and interactions to the data-generating propensity score model, neural networks perform better in terms of bias and mean square error. We link the performance results to balance on observed covariates and demonstrate that our results underscore the importance of checking balance on higher-order covariate terms.}}, 
shorttitle = {Neural Networks for Propensity Score Estimation}, 
editor = {Ark, L. Andries van der and Bolt, Daniel M. and Wang, Wen-Chung and Douglas, Jeffrey A. and Chow, Sy-Miin}, 
journaltitle = {Quantitative Psychology Research}, 
pages = {279--291}, 
volume = {140}, 
language = {en}, 
note = {Notes on Reading 



This will be one of the most important papers of the lit review as our work is an extension of this.

NN are promising for PS estimation becuase they algorithmically handle nonlinear realtionships and interactions.

When the logistic model is correctly specified you get identical results but when its mispecified due to higher order polynomical and interactions NN perfrom better by reducing bias.




Data mining techniques can adapt to comple response surfaces




Simulation:

Use weight decay to eliminate overfitting.

And they generate pari of models. 1) propensity score data generatign model and the outcome generation model.

Include non linearity in both models.

This paper and Setoguchi et al. 2008 are only papers that used NN to estimate PS scores. Promising restuls from both

Hypothesize that when both models are mispecified that is where NN wil shine

The focus of the PS literature is on proposal and evalution of methods..

Use main effect LR first order polynomiasls

Incrase flexibility higher change you will overfit your data. and why they use weight decay. Need to see what the new updated correction should be.




****Key they use Cholesky deomposition to induce a rellation structure to a set of 12 randomly generate covariation from a standard normal distribution. Should also include difference draws...

Induce different levels.

Then have a mian effect PS model




Need to also induce regression coefecients between -1 and 1?

The more complicated the selection and outcome model the better NN performed.




Need to check literature to see if in educaiton and social science the main effect logitic model is what is used.




May overfit your data.

Extension could be DNN with weak predictors.




Connections:




Argues that NN are difficult because you have to choose the optimal tuning parameter, although tihs can be automized now a days.}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Springer%20Proceedings%20in%20Mathematics%20&%20Statistics/Quantitative%20Psychology%20Research,%20The%2079th%20Annual%20Meeting%20of%20the%20Psychometric%20Society,%20Madison,%20Wisconsin,%202014_2015_Springer%20Proceedings%20in%20Mathematics%20&%20Statistics_1.pdf}
}
@article{Powell.2020, 
year = {2020}, 
keywords = {matching,Propensity scores,observational study,include,intro}, 
title = {{Propensity Score Matching for Education Data: Worked Examples}}, 
author = {Powell, Marvin G. and Hull, Darrell M. and Beaujean, A. Alexander}, 
journal = {The Journal of Experimental Education}, 
issn = {0022-0973}, 
doi = {10.1080/00220973.2018.1541850}, 
url = {https://doi.org/10.1080/00220973.2018.1541850}, 
abstract = {{Randomized controlled trials are not always feasible in educational research, so researchers must use alternative methods to study treatment effects. Propensity score matching is one such method for observational studies that has shown considerable growth in popularity since it was first introduced in the early 1980s. This paper outlines the concept of propensity scores by explaining their theoretical principles and providing two examples of their usefulness within the realm of educational research. Through worked examples, we highlight the effectiveness of propensity scores as a method for reducing bias and increasing the balance between treatment and comparison groups. To aid in the understanding and future use of propensity scores, we provide R syntax for all our analyses.}}, 
shorttitle = {Propensity Score Matching for Education Data}, 
pages = {145--164}, 
number = {1}, 
volume = {88}, 
note = {ZSCC: 0000003  Publisher: Routledge
\textbackslash\_eprint: https://doi.org/10.1080/00220973.2018.1541850}
}
@article{Westreich.2010, 
year = {2010}, 
keywords = {Logistic regression,Neural networks,Propensity scores,Classification and regression trees (CART),Recursive partitioning algorithms,Review,include,read}, 
title = {{Propensity score estimation: neural networks, support vector machines, decision trees (CART), and meta-classifiers as alternatives to logistic regression}}, 
author = {Westreich, Daniel and Lessler, Justin and Funk, Michele Jonsson}, 
journal = {Journal of Clinical Epidemiology}, 
issn = {0895-4356}, 
url = {http://www.sciencedirect.com/science/article/pii/S0895435610001022}, 
abstract = {{Objective Propensity scores for the analysis of observational data are typically estimated using logistic regression. Our objective in this review was to assess machine learning alternatives to logistic regression, which may accomplish the same goals but with fewer assumptions or greater accuracy.
Study Design and Setting
We identified alternative methods for propensity score estimation and/or classification from the public health, biostatistics, discrete mathematics, and computer science literature, and evaluated these algorithms for applicability to the problem of propensity score estimation, potential advantages over logistic regression, and ease of use.
Results
We identified four techniques as alternatives to logistic regression: neural networks, support vector machines, decision trees (classification and regression trees [CART]), and meta-classifiers (in particular, boosting).
Conclusion
Although the assumptions of logistic regression are well understood, those assumptions are frequently ignored. All four alternatives have advantages and disadvantages compared with logistic regression. Boosting (meta-classifiers) and, to a lesser extent, decision trees (particularly CART), appear to be most promising for use in the context of propensity score analysis, but extensive simulation studies are needed to establish their utility in practice.}}, 
shorttitle = {Propensity score estimation}, 
pages = {826---833}, 
number = {8}, 
volume = {63}, 
language = {en}, 
note = {Notes on Paper 
Review of possible techniques for PS matching

Look at propensity score as a search term within education

PS using LR is pointed out. I want to include the reality that most current research applies robust techniques using packages.

LR PS don't seem to use interactions in modeling

ML can perform better than regression specially with high-dimentional data

NN perform better than LR in "high-dimentional" setting statistically speaking

Good citations on neural network literature

(+)

A dense enough NN (DNN) can estimate any smooth polynomial funciton, regarless of the order of interaction terms.

NN can deal with high-dimentional data

(-)

More of an art than a science, this has really changed since 2010 now we have ensemble methods that help with optimization

CART prominent way to estimate PS although now 2020 BART is common bayesian extension of CART

Use of DNN in the estimation of PS has not been done

Boositing-meta clasifiers

Connections:

Reference Setoguchi et al. and Lee et al. as setting the intial work in this domain but more simulation is needed.

Final Thoughts:

NN may not be friendly in the sense that alot of tuning has to happen, this to a great deal has been resolved in recent years.}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Journal%20of%20Clinical%20Epidemiology/Propensity%20score%20estimation-%20neural%20networks,%20support%20vector%20machines,%20decision%20trees%20(CART),%20and%20meta-classifiers%20as%20alternatives%20to%20logistic%20regression_2010_Journal%20of%20Clinical%20Epidemiology.pdf}
}
@article{McCaffrey.2004, 
year = {2004}, 
keywords = {include,read}, 
title = {{Propensity Score Estimation With Boosted Regression for Evaluating Causal Effects in Observational Studies}}, 
author = {McCaffrey, Daniel F. and Ridgeway, Greg and Morral, Andrew R.}, 
journal = {Psychological Methods}, 
issn = {1082-989X}, 
doi = {10.1037/1082-989x.9.4.403}, 
pmid = {15598095}, 
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1082-989X.9.4.403}, 
abstract = {{Causal effect modeling with naturalistic rather than experimental data is challenging. In observational studies participants in different treatment conditions may also differ on pretreatment characteristics that influence outcomes. Propensity score methods can theoretically eliminate these confounds for all observed covariates, but accurate estimation of propensity scores is impeded by large numbers of covariates, uncertain functional forms for their associations with treatment selection, and other problems. This article demonstrates that boosting, a modern statistical technique, can overcome many of these obstacles. The authors illustrate this approach with a study of adolescent probationers in substance abuse treatment programs. Propensity score weights estimated using boosting eliminate most pretreatment group differences and substantially alter the apparent relative effects of adolescent substance abuse treatment.}}, 
pages = {403--425}, 
number = {4}, 
volume = {9}, 
language = {en}, 
note = {Notes from Reading 



"Accurate estimation of the propensity score is impeded by large number of covariates, uncertain functional forms for their associations wiht treatment seltion and others"

They want to show that boosting can overcome these obstacles

PS aatemp to model the selection process

Currently propensity score estimation is at a standstill widely by applied researchers, most educaitonal studies realy on a logistic regression for the estimation step. In this paper we aim to show a new approch that relaxes the many parametric assumptions that logitstic regression makes...

Perhaps look at dichotomous outcomes?




Variable seelction risk biasing estmates of treatment effect because we could potentially omit covaritate that are important for selection, or mispecify the fucntional form of the relations betwene covariate and treatment selection.




Propesity score are unknwo and must be estimated from available data




To be acurate we have all covaraites related to both treation and outcome and specify the correct fucntions form, in practice this is difficult...

Good point, that with a large number of covariates we can quickly exhaust the avaialble degress of freedom in traditional paramteti approaches.




Page 8 has a great way of introducing boosting that I would like to use as a template.


}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Psychological%20Methods/Propensity%20Score%20Estimation%20With%20Boosted%20Regression%20for%20Evaluating%20Causal%20Effects%20in%20Observational%20Studies_2004_Psychological%20Methods.pdf}
}
@article{Pan.2018qcl, 
year = {2018}, 
keywords = {include,Social sciences -- Statistical methods.,read}, 
title = {{Propensity score methods for causal inference: an overview}}, 
author = {Pan, Wei and Bai, Haiyan}, 
journal = {Behaviormetrika}, 
issn = {0385-7417}, 
doi = {10.1007/s41237-018-0058-8}, 
abstract = {{Propensity score methods are popular and effective statistical techniques for reducing selection bias in observational data to increase the validity of causal inference based on observational studies in behavioral and social science research. Some methodologists and statisticians have raised concerns about the rationale and applicability of propensity score methods. In this review, we addressed these concerns by reviewing the development history and the assumptions of propensity score methods, followed by the fundamental techniques of and available software packages for propensity score methods. We especially discussed the issues in and debates about the use of propensity score methods. This review provides beneficial information about propensity score methods from the historical point of view and helps researchers to select appropriate propensity score methods for their observational studies.}}, 
pages = {317--334}, 
number = {2}, 
volume = {45}, 
note = {ZSCC: 0000005}
}
@article{Shortreed.2017lu8, 
year = {2017}, 
keywords = {include,read}, 
title = {{Outcome‐adaptive lasso: Variable selection for causal inference}}, 
author = {Shortreed, Susan M. and Ertefaie, Ashkan}, 
journal = {Biometrics}, 
issn = {0006-341X}, 
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.12679}, 
abstract = {{Methodological advancements, including propensity score methods, have resulted in improved unbiased estimation of treatment effects from observational data. Traditionally, a “throw in the kitchen sink” approach has been used to select covariates for inclusion into the propensity score, but recent work shows including unnecessary covariates can impact both the bias and statistical efficiency of propensity score estimators. In particular, the inclusion of covariates that impact exposure but not the outcome, can inflate standard errors without improving bias, while the inclusion of covariates associated with the outcome but unrelated to exposure can improve precision. We propose the outcome-adaptive lasso for selecting appropriate covariates for inclusion in propensity score models to account for confounding bias and maintaining statistical efficiency. This proposed approach can perform variable selection in the presence of a large number of spurious covariates, i.e. covariates unrelated to outcome or exposure. We present theoretical and simulation results indicating that the outcome-adaptive lasso selects the propensity score model that includes all true confounders and predictors of outcome, while excluding other covariates. We illustrate covariate selection using the outcome-adaptive lasso, including comparison to alternative approaches, using simulated data and in a survey of patients using opioid therapy to manage chronic pain.}}, 
shorttitle = {Outcome‐adaptive lasso}, 
pages = {1111---1122}, 
number = {4}, 
volume = {73}, 
language = {en}, 
note = {Notes from Reading: 
This paper brings up the fact that traditionally "the kitchen sink" approach has dominated the PS literature. But recent literature has questioned this claim. Including uncessary covariate can impact both bian and esffeics of the PS estimato. Parituclarly inclusion of instrumental variables (covar that impact the tx but not outcome) can inflate standar erros wihtout improving bias, while covar asosciated with outcome but not related to exposure can improve precision. They use LASSO in a simulation framwork to make their case. Claiming that LASSO can also can select variables in the presense of spurious covariates.




They claim that PS is a prediction problem I claim its a classification problem.




Claim that variable seleciton is important in causal inference because of the positivity assumption. That no combination of covariates has everyone with those covariate values either treatrer or untreated.




**Tested a range of correlation structures, from low to high.








}
}
@article{Stuart.2010, 
year = {2010}, 
keywords = {include,read}, 
title = {{Matching Methods for Causal Inference: A Review and a Look Forward}}, 
author = {Stuart, Elizabeth A.}, 
journal = {Statistical Science}, 
issn = {0883-4237}, 
doi = {10.1214/09-sts313}, 
pmid = {20871802}, 
pmcid = {PMC2943670}, 
eprint = {1010.5586}, 
url = {10/dszwb7}, 
abstract = {{When estimating causal effects using observational data, it is desirable to replicate a randomized experiment as closely as possible by obtaining treated and control groups with similar covariate distributions. This goal can often be achieved by choosing well-matched samples of the original treated and control groups, thereby reducing bias due to the covariates. Since the 1970s, work on matching methods has examined how to best choose treated and control subjects for comparison. Matching methods are gaining popularity in fields such as economics, epidemiology, medicine and political science. However, until now the literature and related advice has been scattered across disciplines. Researchers who are interested in using matching methods—or developing methods related to matching—do not have a single place to turn to learn about past and current research. This paper provides a structure for thinking about matching methods and guidance on their use, coalescing the existing research (both old and new) and providing a summary of where the literature on matching methods is now and where it should be headed.}}, 
pages = {1--21}, 
number = {1}, 
volume = {25}, 
note = {Notes on Paper 



Review of matching methods in Causal Inference




POsitive of matching in comparition to regression, IV and SEM is that matching allows the ability to acess covariate distirbution so we do no exprapolate treatment effects from a narrow overlap.




Make researches aware of the quality of their resulting inference




Great primer on notation and causal inference!




Some may be concerned with including variables that are not related to selection this will lead to a slight increase in variance, but the penalty is larger for exluding potentially important confounders as they will increa bias. Thus reasercher should be libearal in their inclusion o covariates. This may not hold in small samples.




I wonder what would happen in a model fully saturated with only collinear terms..?

Proponents of coarsened exact matching




Concerns about collinearity among covaraites dont apply with PS estimation because we don't care about parameter estimate but the prediction and resulting balance.




Final Thoughts

Claim most commen way to estimate a PS score is through logistic although make reference to non-parametric approaches  such as Setogui et al, and Lee at al.

Make a call for researcher to look at the claim that misestimation of the PS is not a large problem and that the outcome model mispecification is more important. Evaluations are limited and large scale simulation work has not been done.

Once issue they point out is missing data in matching




Major critique specially of PS matching is that we could vioalte igonorable tratme assingment. We may have unobserved variable taht we did not capture. Could test with some work...? May need to include sensitivity analysis.}
}
@article{Dreiseitl.2002, 
year = {2002}, 
keywords = {Classification,Logistic regression,include,read,Artificial neural networks,Medical data analysis,Model comparison,Model evaluation}, 
title = {{Logistic regression and artificial neural network classification models: a methodology review}}, 
author = {Dreiseitl, Stephan and Ohno-Machado, Lucila}, 
journal = {Journal of Biomedical Informatics}, 
issn = {1532-0464}, 
url = {http://www.sciencedirect.com/science/article/pii/S1532046403000340}, 
abstract = {{Logistic regression and artificial neural networks are the models of choice in many medical data classification tasks. In this review, we summarize the differences and similarities of these models from a technical point of view, and compare them with other machine learning algorithms. We provide considerations useful for critically assessing the quality of the models and the results based on these models. Finally, we summarize our findings on how quality criteria for logistic regression and artificial neural network models are met in a sample of papers from the medical literature.}}, 
shorttitle = {Logistic regression and artificial neural network classification models}, 
pages = {352---359}, 
number = {5}, 
volume = {35}, 
language = {en}, 
note = {Notes on Reading: 
Compares logistic to neural networks for classiffication.

Covariates are inputs in ML

Both outpit a probability of membership

Can't see importance of varibles in NN, need to look at updates.

Need to pay close attention to the activation function used, sigmoidal is the logistic function.

NN are black box models

More flexicibility in NN decision boundary.

Pruning network to avoid overfitting


Extracted Annotations (7/22/2020, 4:13:53 PM)

"Abstract Logistic regression and artificial neural networks are the models of choice in many medical data classification tasks. In this review, we summarize the differences and similarities of these models from a technical point of view, and compare them with other machine learning algorithms. We provide considerations useful for critically assessing the quality of the models and the results based on these models. Finally, we summarize our findings on how quality criteria for logistic regression and artificial neural network models are met in a sample of papers from the medical literature." (Dreiseitl and Ohno-Machado 2002:352)

"Among the options in the latter category, the most popular models in medicine are logistic regression (LR) and artificial neural networks (ANN). These models have their origins in two different communities (statistics and computer science), but share many similarities." (Dreiseitl and Ohno-Machado 2002:352)

"In this article, we show that logistic regression and artificial neural networks share common roots in statistical pattern recognition, and how the latter model can be seen as a generalization of the former." (Dreiseitl and Ohno-Machado 2002:352)

"The task of classifying data is to decide class membershipy0 of an unknown data itemx0 based on a data setD¼ðx1;y1Þ;...;ðxn;ynÞ of data itemsxi with known class membershipsyi." (Dreiseitl and Ohno-Machado 2002:353)

"Thexi are usuallymdimensional vectors, the components of which are called covariates and independent variables (in statistics parlance) or input variables (by the machine learning community)." (Dreiseitl and Ohno-Machado 2002:353)

"The second attempts to model PðyjxÞ; this yields not only a class label for a data item, but also a probability of class membership." (Dreiseitl and Ohno-Machado 2002:353)

"Currently, logistic regression and artificial neural networks are the most widely used models in biomedicine, as measured by the number of publications indexed in MEDLINE: 28,500 for logistic regression, 8500 for neural networks, 1300 fork-nearest neighbors, 1100 for decision trees, and 100 for support vector machines." (Dreiseitl and Ohno-Machado 2002:353)

"The disadvantage of support vector machines is that the classification result is purely dichotomous, and no probability of class membership is given." (Dreiseitl and Ohno-Machado 2002:353)

"The advantage thatk-nearest neighbors have over other algorithms is the fact that the neighbors can provide an explanation for the classification result; this case-based explanation can provide an advantage in areas where black-box models are inadequate." (Dreiseitl and Ohno-Machado 2002:353)

"These models differ from the three algorithms above in the sense that they both provide a functional formf and parameter vector" (Dreiseitl and Ohno-Machado 2002:354)

"This distinction is important because the contribution of parameters in logistic regression (coefficients and intercept) can be interpreted, whereas this is not always the case with the parameters of a neural network (weights)." (Dreiseitl and Ohno-Machado 2002:354)

"A logistic regression model that includes only the original covariates is called amaineffectsmodel; including interaction terms such as products makes the model nonlinear in the covariates, and therefore more flexible." (Dreiseitl and Ohno-Machado 2002:354)

"Although the functional forms for logistic regression and artificial neural network models are quite different, a network without a hidden layer is actually identical to a logistic regression model if the logistic (sigmoidal) activation function is used [9,10]." (Dreiseitl and Ohno-Machado 2002:354)

"In a classification context, this means that the decision boundary can be nonlinear as well, making the model more flexible compared to logistic regression." (Dreiseitl and Ohno-Machado 2002:354)

"For both logistic regression and artificial neural networks, the model parameters are determined by maximum likelihood estimation," (Dreiseitl and Ohno-Machado 2002:354)

"Compared to logistic regression, neural network models are more flexible, and thus more susceptible to overfitting." (Dreiseitl and Ohno-Machado 2002:355)

"Alternatively, one can require the model output to be sufficiently smooth. This can be achieved by regularization; in a neural network context this is called weight decay." (Dreiseitl and Ohno-Machado 2002:355)

"The Bayesian framework provides an alternative to maximum-likelihood parameter estimation, and thus to the problem of overfitting." (Dreiseitl and Ohno-Machado 2002:355)

"Instead, one can use automatic relevance determination [9] or sensitivity analysis [17] to heuristically assess the importance of input variables for the classification result." (Dreiseitl and Ohno-Machado 2002:355)

"The latter is possible only for so-called white-box models that allow an interpretation of model parameters. Examples of such algorithms are decision trees (which may be expressed as a set of rules),k-nearest neighbors (which provides exemplars similar to cases to be classified), and logistic regression (where coefficientsÕ sizes determine their relative importance for the classification result)." (Dreiseitl and Ohno-Machado 2002:357)

"Black-boxmodels, such as support vector machines or artificial neural networks, do not allow such an interpretation, and can only be verified externally. Contrasting views on the role of artificial neural networks as predictive models are given in [26,27]. Nevertheless, their discriminating power is often significantly better than that of white-box models, which may explain their popularity in domains where classification performance is more important than model interpretation." (Dreiseitl and Ohno-Machado 2002:357)

"The number of neurons in the hidden layer needs to be set empirically, e.g. by crossvalidation or bootstrapping." (Dreiseitl and Ohno-Machado 2002:357)

"One should avoid the use of plain backpropagation or backpropagation with momentum, as these minimization algorithms are slower to convergence than second-order algorithms such as conjugate gradients or quasi-Newton methods." (Dreiseitl and Ohno-Machado 2002:357)}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Journal%20of%20Biomedical%20Informatics/Logistic%20regression%20and%20artificial%20neural%20network%20classification%20models-%20a%20methodology%20review_2002_Journal%20of%20Biomedical%20Informatics.pdf}
}
@article{Lee.2010, 
year = {2010}, 
keywords = {Propensity score,machine learning,propensity score,Boosting,CART,Data mining,Ensemble methods,Machine learning,Simulation,Weighting,boosting,data mining,ensemble methods,simulation,weighting,include,read}, 
title = {{Improving propensity score weighting using machine learning}}, 
author = {Lee, Brian K. and Lessler, Justin and Stuart, Elizabeth A.}, 
journal = {Statistics in Medicine}, 
issn = {0277-6715}, 
doi = {10.1002/sim.3782}, 
pmid = {19960510}, 
pmcid = {PMC2807890}, 
abstract = {{Machine learning techniques such as classification and regression trees (CART) have been suggested as promising alternatives to logistic regression for the estimation of propensity scores. The authors examined the performance of various CART‐based propensity score models using simulated data. Hypothetical studies of varying sample sizes (n=500, 1000, 2000) with a binary exposure, continuous outcome, and 10 covariates were simulated under seven scenarios differing by degree of non‐linear and non‐additive associations between covariates and the exposure. Propensity score weights were estimated using logistic regression (all main effects), CART, pruned CART, and the ensemble methods of bagged CART, random forests, and boosted CART. Performance metrics included covariate balance, standard error, per cent absolute bias, and 95 per cent confidence interval (CI) coverage. All methods displayed generally acceptable performance under conditions of either non‐linearity or non‐additivity alone. However, under conditions of both moderate non‐additivity and moderate non‐linearity, logistic regression had subpar performance, whereas ensemble methods provided substantially better bias reduction and more consistent 95 per cent CI coverage. The results suggest that ensemble methods, especially boosted CART, may be useful for propensity score weighting. Copyright © 2009 John Wiley \& Sons, Ltd.}}, 
pages = {337--346}, 
number = {3}, 
volume = {29}, 
note = {Notes on Paper 
Argue that using logistic fucntion runs into assumptions regarding variable selection, the fuction form and specifications




Focus on decision trees (CART)




CART may be sensitive to overfitting so are DNN but we correct with learning rates




Ensemble methods, boostrap, I feel most DNN algorithms already do an iterative fitting on repeated samples

Focus on PS weithig not matching

Simulation Plan:

Same as Setoguichi et al. with some modificaitons.

Evaluated same scenories as Setoguichi et al.

Differed sample size they used (n = 500, 1,000 2,000)

Focus not on IPW but ATET. This weight strategies persons in the control group who are more similat to thoese in treatmet group are given larger weight.




Evaluated odels based on ASAM, bias, and SE. 95\%, and weight distribution

Connections:




Build on Setoguchi et al. by considering ensemble methods. Setoguchi et al. did look at NN CART and pruned CART. Argue that ensemble methods are better at clasifications problems.

Lee et al. also builds on Setoguchi by assesing covariate balance




Used R very "out of the box" settings, should be helpful to mimic applied reserachers using these methods

Paper support findings of Setoguchi et al. that ML method for PS estimation perform better than LR

Final Thoughts:

Should consider a DAG diagram as they do to illustrate our simulation exercise

How many applied reserachers still use just a LR to model propensity score??

LR with only main effect perform relatively well but degrade as non-additivity and non-linearities are introduced

Regardless of model of sample size ensemble methods performed very well with covar balance and effect estimation

Some critique that ML is too black box Lee et al. argue that etiologic inference is not a necessary componet of PS estimation. Therefore ML is well suited for these problems.}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Statistics%20in%20Medicine/Improving%20propensity%20score%20weighting%20using%20machine%20learning_2010_Statistics%20in%20Medicine_1.pdf}
}
@article{Pirracchio.2015, 
year = {2015}, 
keywords = {include,read}, 
title = {{Improving Propensity Score Estimators' Robustness to Model Misspecification Using Super Learner}}, 
author = {Pirracchio, Romain and Petersen, Maya L. and Laan, Mark van der}, 
journal = {American Journal of Epidemiology}, 
issn = {0002-9262}, 
doi = {10.1093/aje/kwu253}, 
pmid = {25515168}, 
pmcid = {PMC4351345}, 
url = {https://academic-oup-com.pitt.idm.oclc.org/aje/article/181/2/108/2739158}, 
abstract = {{The consistency of propensity score (PS) estimators relies on correct specification of the PS model. The PS is frequently estimated using main-effects logistic regression. However, the underlying model assumptions may not hold. Machine learning methods provide an alternative nonparametric approach to PS estimation. In this simulation study, we evaluated the benefit of using Super Learner (SL) for PS estimation. We created 1,000 simulated data sets (n = 500) under 4 different scenarios characterized by various degrees of deviance from the usual main-term logistic regression model for the true PS. We estimated the average treatment effect using PS matching and inverse probability of treatment weighting. The estimators' performance was evaluated in terms of PS prediction accuracy, covariate balance achieved, bias, standard error, coverage, and mean squared error. All methods exhibited adequate overall balancing properties, but in the case of model misspecification, SL performed better for highly unbalanced variables. The SL-based estimators were associated with the smallest bias in cases of severe model misspecification. Our results suggest that use of SL to estimate the PS can improve covariate balance and reduce bias in a meaningful manner in cases of serious model misspecification for treatment assignment.}}, 
pages = {108--119}, 
number = {2}, 
volume = {181}, 
language = {en}, 
note = {Notes on Reading 



This article is a simulation study looking at the performance of a Super Learner (SL) in comparison to a main effects logistic regression. They create 1,00 simulated data sets under 4 conditions. They then estimate the treatment effect using matching and IPW. 




Results:

They find that all methods performed well with the corectly specified model but once the model is mispecified or unbalanced covsaries the SL model performed better.




Once agian they state LS is parametric and requires accepting strong assumptions that are not there with non-parametric methods. Mispecifictaion can affect both covariate balance and bias in the treatment effect estimate.




SL proposed by van der Lann et al. method that chooses optimat regresison algortihm among a set of candidates, both parametric and non-parametric. Seems a bit like tree based methods.




Simulation:




Monte Carlo simulation

Model Y continous outcome A binary exporse and W set of 10 covariates. Again I htink they are using the Setoguchi et al. 2008 simulation plan...







SL is essentially running the models (kitchen sink) estimating a loss function to compare among models and selecting the optimal ones.

Differ from Setoguchi and Lee in the estimation fo the ATT instead of ATE




Performance:




-predition perofmance of each algorithm ussing the loss fucntion

-covariate balance: SMD ASAM >10\% considered a concern (CITE)

-distributuion of weight forIPTW \# of subjects discarted

-Point estimate: Report aboslute bias and relative bias

-Variance Estimators**: Evaluted as the bias of the standard error estimatio




Provide some code for SL




Suggestions:

Provide a range of treatment efferct from a small to large***




Comment on the black box nature of SL}
}
@article{Tipton.2013, 
year = {2013}, 
keywords = {include}, 
title = {{Improving Generalizations From Experiments Using Propensity Score Subclassification: Assumptions, Properties, and Contexts}}, 
author = {Tipton, Elizabeth}, 
journal = {Journal of Educational and Behavioral Statistics}, 
issn = {1076-9986}, 
url = {https://doi.org/10.3102/1076998612441947}, 
abstract = {{As a result of the use of random assignment to treatment, randomized experiments typically have high internal validity. However, units are very rarely randomly selected from a well-defined population of interest into an experiment; this results in low external validity. Under nonrandom sampling, this means that the estimate of the sample average treatment effect calculated in the experiment can be a biased estimate of the population average treatment effect. This article explores the use of the propensity score subclassification estimator as a means for improving generalizations from experiments. It first lays out the assumptions necessary for generalizations, then investigates the amount of bias reduction and average variance inflation that is likely when compared to a conventional estimator. It concludes with a discussion of issues that arise when the population of interest is not well represented by the experiment, and an example.}}, 
shorttitle = {Improving Generalizations From Experiments Using Propensity Score Subclassification}, 
pages = {239---266}, 
number = {3}, 
volume = {38}, 
language = {en}, 
note = {Publisher: American Educational Research Association}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Journal%20of%20Educational%20and%20Behavioral%20Statistics/Improving%20Generalizations%20From%20Experiments%20Using%20Propensity%20Score%20Subclassification-%20Assumptions,%20Properties,%20and%20Contexts_2013_Journal%20of%20Educational%20and%20Behavioral%20Statistics.pdf}
}
@article{Setoguchi.2008, 
year = {2008}, 
keywords = {include,read}, 
title = {{Evaluating uses of data mining techniques in propensity score estimation: a simulation study}}, 
author = {Setoguchi, Soko and Schneeweiss, Sebastian and Brookhart, M. Alan and Glynn, Robert J. and Cook, E. Francis}, 
journal = {Pharmacoepidemiology and Drug Safety}, 
issn = {1053-8569}, 
doi = {10.1002/pds.1555}, 
pmid = {18311848}, 
pmcid = {PMC2905676}, 
url = {www.interscience.wiley.com}, 
abstract = {{In propensity score modeling, it is a standard practice to optimize the prediction of exposure status based on the covariate information. In a simulation study, we examined in what situations analyses based on various types of exposure propensity score (EPS) models using data mining techniques such as recursive partitioning (RP) and neural networks (NN) produce unbiased and/or efficient results. We simulated data for a hypothetical cohort study (n = 2000) with a binary exposure/outcome and 10 binary/continuous covariates with seven scenarios differing by non‐linear and/or non‐additive associations between exposure and covariates. EPS models used logistic regression (LR) (all possible main effects), RP1 (without pruning), RP2 (with pruning), and NN. We calculated c‐statistics (C), standard errors (SE), and bias of exposure‐effect estimates from outcome models for the PS‐matched dataset. Data mining techniques yielded higher C than LR (mean: NN, 0.86; RPI, 0.79; RP2, 0.72; and LR, 0.76). SE tended to be greater in models with higher C. Overall bias was small for each strategy, although NN estimates tended to be the least biased. C was not correlated with the magnitude of bias (correlation coefficient [COR] = −0.3, p = 0.1) but increased SE (COR = 0.7, p < 0.001). Effect estimates from EPS models by simple LR were generally robust. NN models generally provided the least numerically biased estimates. C was not associated with the magnitude of bias but was with the increased SE. Copyright © 2008 John Wiley \& Sons, Ltd.}}, 
pages = {546--555}, 
number = {6}, 
volume = {17}, 
note = {Notes from Reading: 



Authors mention overfitting is not a concern with PS model because we are focused on predictior, valid claim?

Be made as complex as posible with quadratic and interactions.

Method: They used a monte carlo simulation with two cohort sizes, 2,000 and 10,000. Some exposure Tx, outcome Y. 10 Covariates. 4 of which were indepent from Tx and Y and 4 confoundesrs (associated with Tx and Y), 3 associated with Y. 8 covariates  were linear combination of eachother.




Induced correlations to some variables (Appendix). Tx was modeled using LR as a function of Covs and varied based on scenario tested. Outcome Y was modeled using LR as a fucntion of Cov and Tx. The tx effect was set to be -.4 which is in line with the epi study they cite.




Scenarios (Table2/Appendix A):

Simplest: assumed lienar assocaitions between continous cov and the tx and additive effect for all cov

Mild/Moderate non-linearity/aditivity: Varied the number of interation terms and the number of quadratic terms.

Midly complex interations: 3 2-way interactions and/or one quadratic term involving the confounders.

Complex Association: Same as mild complex but added 10 2-way interactions and/or 3 quadratic terms.




Compared:

NN 1 layer and 10 hidden nodes. Relatively simplistic NN and not as adanced as our proposed DNN

Recursive Paritioing with pruning: seems to be a tree based method

Recursive Paritions without pruning

LR with all main effects




Matching:

Seems like a caliper based method with .1 ST aroudn the PS score

Calculated the treatment effect across all replications took the avaerage ETA, Report Bias and C statistic.

Need to look more in the C coefecient.




Results: NN tended to produce the least biased estimation in many scenariors with non addvitiy and non linearity, in both small and large sample sets. But no better perfomance when tested in simulation using dataset wiht only confounders. LR tended to produce more biased estimates than NN. They suggest that NN have the ability to correctly detec the assocation between the exposrue and the covariates. Although something might induce uncesarry complexity.




Thoughts:

Include colllinearity, and dimentionality are + for our study}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Pharmacoepidemiology%20and%20Drug%20Safety/Evaluating%20uses%20of%20data%20mining%20techniques%20in%20propensity%20score%20estimation-%20a%20simulation%20study_2008_Pharmacoepidemiology%20and%20Drug%20Safety.pdf}
}
@article{Brown.2018, 
year = {2018}, 
keywords = {include,read}, 
title = {{Estimating Average Treatment Effects With Propensity Scores Estimated With Four Machine Learning Procedures: Simulation Results in High Dimensional Settings and With Time to Event Outcomes}}, 
author = {Brown, Kip and Merrigan, Phil and Royer, Jimmy}, 
journal = {SSRN Electronic Journal}, 
issn = {1556-5068}, 
doi = {10.2139/ssrn.3272396}, 
url = {https://www.ssrn.com/abstract=3272396}, 
abstract = {{Background: The increased availability of claims data allows one to build high dimensional datasets, rich in covariates, for accurately estimating treatment effects in medical and epidemiological cohort studies. This paper shows the full potential of machine learning for the estimation of average treatment effects with propensity score methods in a context rich and high dimensional datasets. Methods: Four different methods are used to estimate average treatment effects in the context of time to event outcomes. The four methods explored in this study are LASSO, Random Forest, Gradient Descent Boosting and Artificial Neural networks. Simulations based on an actual medical claims data set are used to assess the efficiency of these methods. The simulations are performed with over 100, 000 observations and 1,100 explanatory variables. Each method is tested on 500 datasets that are created from the original dataset, allowing us to report the mean and standard deviation of estimated average treatment effects. Results: The results are very promising for all four methods; however, LASSO, Random Forest and Gradient Boosting seem to be performing better than Random Forest. Conclusion: Machine Learning methods can be helpful for observational studies that use the propensity score when a very large number of covariates are available, the total number of observations is large, and the dependent event rare. This is an important result given the availability of big data related to Health Economics and Outcomes Research (HEOR) around the world.}}, 
shorttitle = {Estimating Average Treatment Effects With Propensity Scores Estimated With Four Machine Learning Procedures}, 
language = {en}, 
note = {ZSCC: 0000002}
}
@book{Rosenbaum.2010, 
year = {2010}, 
keywords = {include,read}, 
title = {{Design of Observational Studies}}, 
author = {Rosenbaum, Paul R.}, 
isbn = {9781441912121}, 
url = {http://link.springer.com/10.1007/978-1-4419-1213-8}, 
urldate = {2020-07-10}, 
series = {Springer Series in Statistics}, 
publisher = {Springer New York}, 
address = {New York, NY}, 
language = {en}, 
note = {ZSCC: 0001004 }, 
doi = {10.1007/978-1-4419-1213-8}
}
@article{Testolin.2013, 
year = {2013}, 
keywords = {include,Cognitive Modeling,computer cluster,deep neural networks,GPUs,hierarchical generative models,MPI,parallel computing architectures,unsupervised learning}, 
title = {{Deep Unsupervised Learning on a Desktop PC: A Primer for Cognitive Scientists}}, 
author = {Testolin, Alberto and Stoianov, Ivilin and Grazia, Michele De Filippo De and Zorzi, Marco}, 
journal = {Frontiers in Psychology}, 
issn = {1664-1078}, 
doi = {10.3389/fpsyg.2013.00251}, 
pmid = {23653617}, 
pmcid = {PMC3644707}, 
url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00251/full}, 
abstract = {{Deep belief networks hold great promise for the simulation of human cognition because they show how structured and abstract representations may emerge from probabilistic unsupervised learning. These networks build a hierarchy of progressively more complex distributed representations of the sensory data by fitting a hierarchical generative model. However, learning in deep networks typically requires big datasets and it can involve millions of connection weights, which implies that simulations on standard computers are unfeasible. Developing realistic, medium-to-large-scale learning models of cognition would therefore seem to require expertise in programing parallel-computing hardware, and this might explain why the use of this promising approach is still largely confined to the machine learning community. Here we show how simulations of deep unsupervised learning can be easily performed on a desktop PC by exploiting the processors of low cost graphic cards (graphic processor units) without any specific programing effort, thanks to the use of high-level programming routines (available in MATLAB or Python). We also show that even an entry-level graphic card can outperform a small high-performance computing cluster in terms of learning time and with no loss of learning quality. We therefore conclude that graphic card implementations pave the way for a widespread use of deep learning among cognitive scientists for modeling cognition and behavior.}}, 
shorttitle = {Deep Unsupervised Learning on a Desktop PC}, 
pages = {251}, 
volume = {4}, 
language = {English}, 
note = {Publisher: Frontiers}
}
@article{LeCun.2015, 
year = {2015}, 
keywords = {include,read}, 
title = {{Deep learning}}, 
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey}, 
journal = {Nature}, 
issn = {1476-4687}, 
url = {10.1038/nature14539}, 
abstract = {{Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.}}, 
pages = {436---444}, 
number = {7553}, 
volume = {521}, 
language = {en}, 
note = {Notes on Reading 
Deep learning algorithsm amplificy inputs that are important and supress irrevant varitions

Good at learning intricrate sturctures in high dimentional data


Extracted Annotations (7/28/2020, 9:01:41 PM)

"Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction." (LeCun et al 2015:436)

"Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input." (LeCun et al 2015:436)

"Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification." (LeCun et al 2015:436)

"Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level." (LeCun et al 2015:436)

"For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations." (LeCun et al 2015:436)

"Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years." (LeCun et al 2015:436)

"e very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government." (LeCun et al 2015:436)

"The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average." (LeCun et al 2015:437)

"This simple procedure usually finds a good set of weights surprisingly quickly when compared with far more elaborate optimization techniques18." (LeCun et al 2015:437)

"A two-class linear classifier computes a weighted sum of the feature vector components. If the weighted sum is above a threshold, the input is classified as belonging to a particular category." (LeCun et al 2015:437)

"This is why shallow classifiers require a good feature extractor that solves the selectivity-invariance dilemma — one that produces representations that are selective to the aspects of the image that are important for discrimination, but that are invariant to irrelevant aspects such as the pose of the animal." (LeCun et al 2015:438)

"But this can all be avoided if good features can be learned automatically using a general-purpose learning procedure. This is the key advantage of deep learning." (LeCun et al 2015:438)

"A deep-learning architecture is a multilayer stack of simple modules, all (or most) of which are subject to learning, and many of which compute non-linear input-output mappings." (LeCun et al 2015:438)

"Many applications of deep learning use feedforward neural network architectures (Fig. 1), which learn to map a fixed-size input (for example, an image) to a fixed-size output (for example, a probability for each of several categories)." (LeCun et al 2015:438)

"Interest in deep feedforward networks was revived around 2006 (refs 31-34) by a group of researchers brought together by the Canadian Institute for Advanced Research (CIFAR)." (LeCun et al 2015:439)}
}
@article{Tu.2019, 
year = {2019}, 
keywords = {include}, 
title = {{Comparison of various machine learning algorithms for estimating generalized propensity score}}, 
author = {Tu, Chunhao}, 
journal = {Journal of Statistical Computation and Simulation}, 
issn = {0094-9655}, 
doi = {10.1080/00949655.2019.1571059}, 
url = {https://www.tandfonline.com/doi/full/10.1080/00949655.2019.1571059}, 
abstract = {{In this paper, we conducted a simulation study to evaluate the performance of four algorithms: multinomial logistic regression (MLR), bagging (BAG), random forest (RF), and gradient boosting (GB), for estimating generalized propensity score (GPS). Similar to the propensity score (PS), the ultimate goal of using GPS is to estimate unbiased average treatment effects (ATEs) in observational studies. We used the GPS estimates computed from these four algorithms with the generalized doubly robust (GDR) estimator to estimate ATEs in observational studies. We evaluated these ATE estimates in terms of bias and mean squared error (MSE). Simulation results show that overall, the GB algorithm produced the best ATE estimates based on these evaluation criteria. Thus, we recommend using the GB algorithm for estimating GPS in practice.}}, 
pages = {708--719}, 
number = {4}, 
volume = {89}, 
language = {en}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Journal%20of%20Statistical%20Computation%20and%20Simulation/Comparison%20of%20various%20machine%20learning%20algorithms%20for%20estimating%20generalized%20propensity%20score_2019_Journal%20of%20Statistical%20Computation%20and%20Simulation.pdf}
}
@article{Hill.2011soh, 
year = {2011}, 
keywords = {include,read}, 
title = {{Challenges With Propensity Score Strategies in a High-Dimensional Setting and a Potential Alternative}}, 
author = {Hill, Jennifer and Weiss, Christopher and Zhai, Fuhua}, 
journal = {Multivariate Behavioral Research}, 
issn = {0027-3171}, 
doi = {10.1080/00273171.2011.570161}, 
pmid = {26735884}, 
url = {https://doi.org/10.1080/00273171.2011.570161}, 
abstract = {{This article explores some of the challenges that arise when trying to implement propensity score strategies to answer a causal question using data with a large number of covariates. We discuss choices in propensity score estimation strategies, matching and weighting implementation strategies, balance diagnostics, and final analysis models. We demonstrate the wide range of estimates that can result from different combinations of these choices. Finally, an alternative estimation strategy is presented that may have benefits in terms of simplicity and reliability. These issues are explored in the context of an empirical example that uses data from the Early Childhood Longitudinal Study, Kindergarten Cohort to investigate the potential effect of grade retention after the 1st-grade year on subsequent cognitive outcomes.}}, 
pages = {477--513}, 
number = {3}, 
volume = {46}, 
note = {Notes on Reading 
Use an exaple of grade retention

Need to condition on a large number of covariates in order to justify the ignoralbity assumption




Method:

Treatment retention in grade

Covar 200+




In a high dimentional settig the logistic model may fall apaprt, or perform poorly due to problems of perfect seperation




Overfittig also becomes an issue, same in DNN but trimming helps in this regard




They tackle the problem of overfitting with high dimentional data, by looking at the region of common support with a dummy tx. BART seems to perform the best.




Propose a different way to extract a tratment effect, instrad of matching or weighting they propose a model of the outcome using BART




BART like other ML techniques are learnng algorith that will find the nonlinearities including interadtions in the data




Thus the research can avoid the complicated process of dignosing model for lineary models with high dimentions




How about using DNN to predict each tx unit counterfactual, in essense building that outcome model.}
}
@book{Imbens.2015, 
year = {2015}, 
keywords = {include,read}, 
title = {{Causal inference for statistics, social, and biomedical sciences}}, 
author = {Imbens, Guido and Rubin, Donald B.}, 
isbn = {978-0-521-88588-1}, 
publisher = {Cambridge University Press}
}
@article{Karim.2018, 
year = {2018}, 
keywords = {include}, 
title = {{Can We Train Machine Learning Methods to Outperform the High-dimensional Propensity Score Algorithm?}}, 
author = {Karim, Mohammad Ehsanul and Pang, Menglan and Platt, Robert W.}, 
journal = {Epidemiology}, 
issn = {1044-3983}, 
url = {https://journals.lww.com/epidem/Fulltext/2018/03000/Can\_We\_Train\_Machine\_Learning\_Methods\_to.5.aspx?casa\_token=ZxQu19dyu-0AAAAA:cVTWRbTk6ZM\_AukNOXHWGjq56uJLoyZm8SY\_bqlIXCcGB5fSG2Sv1ZeBSBPbf4LazDQqt7QGs1Oy2SYB5dZlZ1o}, 
abstract = {{The use of retrospective health care claims datasets is frequently criticized for the lack of complete information on potential confounders. Utilizing patient’s health status–related information from claims datasets as surrogates or proxies for mismeasured and unobserved confounders, the high-dimensional propensity score algorithm enables us to reduce bias. Using a previously published cohort study of postmyocardial infarction statin use (1998–2012), we compare the performance of the algorithm with a number of popular machine learning approaches for confounder selection in high-dimensional covariate spaces: random forest, least absolute shrinkage and selection operator, and elastic net. Our results suggest that, when the data analysis is done with epidemiologic principles in mind, machine learning methods perform as well as the high-dimensional propensity score algorithm. Using a plasmode framework that mimicked the empirical data, we also showed that a hybrid of machine learning and high-dimensional propensity score algorithms generally perform slightly better than both in terms of mean squared error, when a bias-based analysis is used.}}, 
pages = {191---198}, 
number = {2}, 
volume = {29}, 
language = {en-US}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Epidemiology/Can%20We%20Train%20Machine%20Learning%20Methods%20to%20Outperform%20the%20High-dimensional%20Propensity%20Score%20Algorithm-_2018_Epidemiology.pdf}
}
@article{Dorie.2019, 
year = {2019}, 
keywords = {include}, 
title = {{Automated versus Do-It-Yourself Methods for Causal Inference: Lessons Learned from a Data Analysis Competition}}, 
author = {Dorie, Vincent and Hill, Jennifer and Shalit, Uri and Scott, Marc and Cervone, Dan}, 
journal = {Statistical Science}, 
issn = {0883-4237}, 
url = {https://projecteuclid.org/euclid.ss/1555056030}, 
abstract = {{Statisticians have made great progress in creating methods that reduce our reliance on parametric assumptions. However, this explosion in research has resulted in a breadth of inferential strategies that both create opportunities for more reliable inference as well as complicate the choices that an applied researcher has to make and defend. Relatedly, researchers advocating for new methods typically compare their method to at best 2 or 3 other causal inference strategies and test using simulations that may or may not be designed to equally tease out ﬂaws in all the competing methods. The causal inference data analysis challenge, “Is Your SATT Where It’s At?”, launched as part of the 2016 Atlantic Causal Inference Conference, sought to make progress with respect to both of these issues. The researchers creating the data testing grounds were distinct from the researchers submitting methods whose efﬁcacy would be evaluated. Results from 30 competitors across the two versions of the competition (black-box algorithms and do-it-yourself analyses) are presented along with post-hoc analyses that reveal information about the characteristics of causal inference strategies and settings that affect performance. The most consistent conclusion was that methods that ﬂexibly model the response surface perform better overall than methods that fail to do so. Finally new methods are proposed that combine features of several of the top-performing submitted methods.}}, 
shorttitle = {Automated versus Do-It-Yourself Methods for Causal Inference}, 
pages = {43---68}, 
number = {1}, 
volume = {34}, 
language = {en}, 
note = {ZSCC: 0000062}
}
@article{Watkins.2013, 
year = {2013}, 
keywords = {ensemble methods,Propensity scores,tree-based methods,include,read}, 
title = {{An Empirical Comparison of Tree‐Based Methods for Propensity Score Estimation}}, 
author = {Watkins, Stephanie and Jonsson‐Funk, Michele and Brookhart, M. Alan and Rosenberg, Steven A. and O'Shea, T. Michael and Daniels, Julie}, 
journal = {Health Services Research}, 
issn = {0017-9124}, 
doi = {10.1111/1475-6773.12068}, 
pmid = {23701015}, 
pmcid = {PMC3796115}, 
url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/1475-6773.12068}, 
abstract = {{To illustrate the use of ensemble tree‐based methods (random forest classification [RFC] and bagging) for propensity score estimation and to compare these methods with logistic regression, in the context of evaluating the effect of physical and occupational therapy on preschool motor ability among very low birth weight (VLBW) children. We used secondary data from the Early Childhood Longitudinal Study Birth Cohort (ECLS‐B) between 2001 and 2006. We estimated the predicted probability of treatment using tree‐based methods and logistic regression (LR). We then modeled the exposure‐outcome relation using weighted LR models while considering covariate balance and precision for each propensity score estimation method. Among approximately 500 VLBW children, therapy receipt was associated with moderately improved preschool motor ability. Overall, ensemble methods produced the best covariate balance (Mean Squared Difference: 0.03–0.07) and the most precise effect estimates compared to LR (Mean Squared Difference: 0.11). The overall magnitude of the effect estimates was similar between RFC and LR estimation methods. Propensity score estimation using RFC and bagging produced better covariate balance with increased precision compared to LR. Ensemble methods are a useful alterative to logistic regression to control confounding in observational studies.}}, 
pages = {1798--1817}, 
number = {5}, 
volume = {48}, 
language = {en}, 
copyright = {© Health Research and Educational Trust}, 
note = {Notes from Reading 
Article is a comparison usingn ensemble tree based methods in comparison to logistic regression, using ECLS data

Ensemble methods outperformed LR, produced best covariate balance, least biased treatment effects.

LR assums the relation between continous and ordinal and the log of the depended must be leinar. Further more the model assume addivitiy. Not technicaly true? We can impose higher order polynomials to model non-linearities

Would be a good paper to look at for Hillman

Once again they show that baggind and RFC achieved best covariabe balance

Perhaps I should use more "ensemble" neural nets, can test this in the model.

Connections:

Refer back to Austin 2011 that most researches estimate PS scores using LR

Also refer back to Setoguchi et al. 2008 and Lee 2010 that random forest performed well.}
}
@article{Thoemmes.2011pt9, 
year = {2011}, 
keywords = {include,read}, 
title = {{A Systematic Review of Propensity Score Methods in the Social Sciences}}, 
author = {Thoemmes, Felix J. and Kim, Eun Sook}, 
journal = {Multivariate Behavioral Research}, 
issn = {0027-3171}, 
doi = {10.1080/00273171.2011.540475}, 
pmid = {26771582}, 
url = {https://doi.org/10.1080/00273171.2011.540475}, 
pages = {90--118}, 
number = {1}, 
volume = {46}, 
note = {Notes on Reading 



Lit review and highlight common errors in PS analysis. In line with the work Austin did in medical research around PS use.

PS assumes that if all relevant covariate have been accessed then we can yield unbiased causal effect, for this we need many covariates. That makes us deal with issue with high dimentionality and variable selection.

Shadish et al. 2006 show that PS models build on a few covariates do not yield unbiased causal effect estimates

Any model that produces estimates of probabilty can be used to estimate a propensity score.

Majority of articles used a logistic model for propensity score estimation




Connections to other literature:

Ushers in the review of ed and psych with PS methods as most of the other literarure has been in medicine and health. They also urge that reserachers are fixated on old estimation methods, and should look to new methos that discover non-linearities.



Extracted Annotations (7/24/2020, 12:10:52 AM)

"The use of propensity scores in psychological and educational research has been steadily increasing in the last 2 to 3 years. However, there are some common misconceptions about the use of different estimation techniques and conditioning choices in the context of propensity score analysis. In addition, reporting practices for propensity score analyses often lack important details that allow other researchers to confidently judge the appropriateness of reported analyses and potentially to replicate published findings. In this article we conduct a systematic literature review of a large number of published articles in major areas of social science that used propensity scores up until the fall of 2009. We identify common errors in estimation, conditioning, and reporting of propensity score analyses and suggest possible solutions." (Thoemmes and Kim 2011:91)

"The number of newly published articles each year that used propensity scores in the psychological and educational literature, as assessed by the database Web of Science, is rising nearly exponentially (see Figure 1). This development clearly demonstrates that there is interest in this methodological tool." (Thoemmes and Kim 2011:91)

"However, there exists much variability on how propensity score methods are implemented in practice and recent methodological advances are not fully incorporated in the substantive literature yet" (Thoemmes and Kim 2011:91)

"First, researchers are not using best available methods to conduct propensity score analyses in regard to estimation and conditioning on the propensity score." (Thoemmes and Kim 2011:92)

"Our main interest was to examine what kind of methods and" (Thoemmes and Kim 2011:92)

"92 THOEMMES AND KIM reporting patterns in regard to propensity score analyses were most prevalent in the social science literature, specifically in psychological and educational research. Before presentation of our methods and results we provide a brief overview of necessary steps in a propensity score analysis. For more detailed overviews we refer the reader to introductions and applied examples by Caliendo \& Kopeinig (2008), Stuart (2010), Stuart et al. (2009), or Shadish and Steiner (2010). A more thorough treatment involving general issues of the design and analysis of observational studies is given by Rosenbaum (2010)." (Thoemmes and Kim 2011:93)

"Briefly explained, the propensity score is a conditional probability that expresses how likely a participant is to be assigned or to select the treatment condition given certain observed baseline characteristics. In a propensity score analysis this conditional probability is used to condition observed data, for example, through matching or stratification on the propensity score." (Thoemmes and Kim 2011:93)

"Balance on covariates is desirable because a balanced covariate (which is by definition uncorrelated with treatment assignment) cannot bias the estimate of a treatment effect, even if the covariate itself is related to an outcome variable." (Thoemmes and Kim 2011:93)

"Under the assumption that all relevant covariates have been assessed, a propensity score analysis can yield unbiased causal effect estimates." (Thoemmes and Kim 2011:93)

"For more information on potential outcomes and the strongly ignorable treatment assignment assumption see Rosenbaum and Rubin (1983) or Rubin (2005)." (Thoemmes and Kim 2011:93)

"Sensitivity analyses (e.g., Robins, Rotnitzky, \& Scharfstein, 1999; Rosenbaum, 1991a, 1991b) that probe how much bias an unobserved covariate would have to exert to negate the observed treatment effect can help to bolster one's faith in the assumption that all important covariates and potential confounders have been assessed." (Thoemmes and Kim 2011:94)

"Because of this methodological variability associated with propensity score analyses, it is important that choices made in the process are explicated in published work." (Thoemmes and Kim 2011:94)

"A propensity score analysis can only be as good as the covariates that are at the disposal of the researcher. Only a rich set of covariates can make the" (Thoemmes and Kim 2011:94)

"94 THOEMMES AND KIM strongly ignorable treatment assignment assumption credible and therefore it is of importance that researchers give a detailed account of the variables that were collected." (Thoemmes and Kim 2011:95)

"Propensity score models that are conducted with only few covariates often do not yield unbiased causal effect estimates (Shadish, Luellen, \& Clark, 2006)." (Thoemmes and Kim 2011:95)

"Theoretically any model that produces estimates of the probability of group membership for each participant can be used to estimate propensity scores." (Thoemmes and Kim 2011:95)

"Common choices for model selection are nonparsimonious models (in which all variables are included) or approaches based on statistical significance, often with significance levels that are larger than the usual .05 cutoff (see, e.g., Shadish et al., 2006). No definite answer exists as to which cutoff value will produce the best balance and it is unclear if one single optimal cutoff value would work for a range of different data sets." (Thoemmes and Kim 2011:95)

"Two central properties of the propensity score model should always be assessed and reported: the balance property and the common support region." (Thoemmes and Kim 2011:97)

"Absence of significant differences is taken as evidence that balance has been achieved. Other authors (e.g., Austin, 2007; Ho, Imai, King, \& Stuart, 2007) criticize significance tests in the context of matching due to the dependence on sample size and suggest examining standardized differences before and after matching." (Thoemmes and Kim 2011:97)

"Currently an issue of debate is whether statistical procedures following matching on the propensity score need to be adjusted for the matched nature of the sample." (Thoemmes and Kim 2011:98)

"Hill (2008) argues that the exact nature of how this adjustment for matched data should be conducted is still open to debate." (Thoemmes and Kim 2011:98)

"The predominant mode of estimating the propensity score in our sample was a logistic regression followed by probit regression." (Thoemmes and Kim 2011:104)

"We encourage researchers to explore these new methods as there is evidence that these methods can in some circumstance yield better results (see McCaffrey et al., 2004). One of the advantages of these algorithmic approaches is that any nonlinear terms are automatically discovered and entered into the estimation model of the propensity score." (Thoemmes and Kim 2011:104)

"The use of more advanced estimation methods is encouraged. Often linear logistic regression models approximate the true relationship between co-" (Thoemmes and Kim 2011:110)

"110 THOEMMES AND KIM variates and treatment assignment well; however, the inclusion of nonlinear terms (interactions and polynomials) can improve the selection model. Methods such as boosted regression trees (McCaffrey et al., 2004) or genetic matching (Diamond \& Sekhon, 2005) attempt to find these complex relationships by applying automatic data mining algorithms" (Thoemmes and Kim 2011:111)}
}
@article{Cannas.2019, 
year = {2019}, 
keywords = {covariate balance,machine learning algorithms,caesarean section,labor induction,propensity score methods,include}, 
title = {{A comparison of machine learning algorithms and covariate balance measures for propensity score matching and weighting}}, 
author = {Cannas, Massimo and Arpino, Bruno}, 
journal = {Biometrical Journal}, 
issn = {0323-3847}, 
doi = {10.1002/bimj.201800132}, 
pmid = {31090108}, 
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.201800132}, 
abstract = {{Propensity score matching (PSM) and propensity score weighting (PSW) are popular tools to estimate causal effects in observational studies. We address two open issues: how to estimate propensity scores and assess covariate balance. Using simulations, we compare the performance of PSM and PSW based on logistic regression and machine learning algorithms (CART; Bagging; Boosting; Random Forest; Neural Networks; naive Bayes). Additionally, we consider several measures of covariate balance (Absolute Standardized Average Mean (ASAM) with and without interactions; measures based on the quantile‐quantile plots; ratio between variances of propensity scores; area under the curve (AUC)) and assess their ability in predicting the bias of PSM and PSW estimators. We also investigate the importance of tuning of machine learning parameters in the context of propensity score methods. Two simulation designs are employed. In the first, the generating processes are inspired to birth register data used to assess the effect of labor induction on the occurrence of caesarean section. The second exploits more general generating mechanisms. Overall, among the different techniques, random forests performed the best, especially in PSW. Logistic regression and neural networks also showed an excellent performance similar to that of random forests. As for covariate balance, the simplest and commonly used metric, the ASAM, showed a strong correlation with the bias of causal effects estimators. Our findings suggest that researchers should aim at obtaining an ASAM lower than 10\% for as many variables as possible. In the empirical study we found that labor induction had a small and not statistically significant impact on caesarean section.}}, 
pages = {1049--1072}, 
number = {4}, 
volume = {61}, 
language = {en}, 
copyright = {© 2019 WILEY‐VCH Verlag GmbH \textbackslash\& Co. KGaA, Weinheim}, 
note = {ZSCC: 0000004  \textbackslash\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.201800132}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Biometrical%20Journal/A%20comparison%20of%20machine%20learning%20algorithms%20and%20covariate%20balance%20measures%20for%20propensity%20score%20matching%20and%20weighting_2019_Biometrical%20Journal.pdf}
}

@article{Ornelas.2004, 
year = {2004}, 
title = {{A Critical Race Analysis of Latina/o and African American Advanced Placement Enrollment in Public High Schools}}, 
author = {Ornelas, Armida and Solorzano, Daniel Gilbert}, 
journal = {The High School Journal}, 
issn = {1534-5157}, 
doi = {10.1353/hsj.2004.0003}, 
abstract = {{In order to answer these questions, we examined a school district in California that serves a large population of Latina/o and African American students. Three different patterns emerged around access and availability of AP courses: Latina/o students are disproportionately underrepresented in AP enrollment district-wide; schools that serve urban, low-income Latina/o and African American communities have low student enrollment in AP courses; and even when Latina/o and African American students attend high schools with high numbers of students enrolled in AP courses, they are not equally represented in AP enrollment. We call this structure and process "Schools Within Schools."}}, 
pages = {15--26}, 
number = {3}, 
volume = {87}
}
@article{Oakes.2007, 
year = {2007}, 
title = {{Radical change through radical means: learning power}}, 
author = {Oakes, Jeannie and Rogers, John}, 
journal = {Journal of Educational Change}, 
issn = {1389-2843}, 
doi = {10.1007/s10833-007-9031-0}, 
abstract = {{The history of US schooling is a remarkable tale of expanding educational opportunities in the midst of educational inequality. Despite cultural commitments to equality and justice, the US educational system continues to provide clear and consistent advantages for white and wealthier Americans and disadvantages for low-income, students of color. This paper explores why efforts to equalize education have fared so poorly and how US schools and society might become more equitable and just. Our conclusions are straightforward: Equity reforms rarely take hold because they rely on conventional, technical approaches to policymaking and educational change. Instead, equitable, high-quality schooling for all students will likely fare better with social movement activism that addresses the societal norms and politics which cement the status quo. At the heart of such activism is a process we call “learning power.” (The ideas presented here are presented more fully in Jeannie Oakes and John Rogers, Learning Power: Organizing for Education and Justice (New York: Teachers College Press, 2006).)}}, 
pages = {193--206}, 
number = {3}, 
volume = {8}
}
@article{Oromaner.1986, 
year = {1986}, 
title = {{Keeping Track: How Schools Structure Inequality.}}, 
author = {Oromaner, Mark and Oakes, Jeannie}, 
journal = {Contemporary Sociology}, 
issn = {0094-3061}, 
doi = {10.2307/2070941}, 
pages = {93}, 
number = {1}, 
volume = {15}
}
@techreport{1956, 
year = {1956}, 
title = {{Higher Education in the United States}}, 
url = {http://pellinstitute.org/downloads/publications-Indicators\_of\_Higher\_Education\_Equity\_in\_the\_US\_2021\_Historical\_Trend\_Report.pdf}, 
pages = {1158--1158}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/The%20Pell%20Institute/Higher%20Education%20in%20the%20United%20States_1956_The%20Pell%20Institute_1.pdf}
}
@article{Guzman-Alvarez.2021, 
year = {2021}, 
title = {{Disproportionate Burden: Estimating the Cost of FAFSA Verification for Public Colleges and Universities}}, 
author = {Guzman-Alvarez, Alberto and Page, Lindsay C.}, 
journal = {Educational Evaluation and Policy Analysis}, 
issn = {0162-3737}, 
doi = {10.3102/01623737211001420}, 
abstract = {{Verification is a federally mandated process that requires selected students to further attest that the information reported on their Free Application for Federal Student Aid (FAFSA) is accurate and complete. In this brief, we estimate institutional costs of administrating the FAFSA verification mandate and consider variation in costs by institution type and sector. Using data from 2014, we estimate that compliance costs to institutions in that year totaled nearly US\$500 million with the burden falling disproportionately on public institutions and community colleges, in particular. Specifically, we estimate that 22\% of an average community college’s financial aid office operating budget is devoted to verification procedures, compared with 15\% at public 4-year institutions. Our analysis is timely, given that rates of FAFSA verification have increased in recent years.}}, 
pages = {545--551}, 
number = {3}, 
volume = {43}
}
@article{Page.2016x1k, 
year = {2016}, 
title = {{Improving college access in the United States: Barriers and policy responses}}, 
author = {Page, Lindsay C. and Scott-Clayton, Judith}, 
journal = {Economics of Education Review}, 
issn = {0272-7757}, 
doi = {10.1016/j.econedurev.2016.02.009}, 
abstract = {{Socioeconomic gaps in college enrollment and attainment have widened over time, despite increasing returns to postsecondary education and significant policy efforts to improve access. We describe the barriers that students face during the transition to college and review the evidence on potential policy solutions. We focus primarily on research that examines causal relationships using experimental or quasi-experimental methods, though we draw upon descriptive evidence to provide context. Our review is distinctive in three respects. First, in addition to the literature on financial aid, we examine the evidence on informational and behavioral interventions, academic programs, and affirmative action policies intended to improve college access. Second, we incorporate a wealth of recent research not included in prior reviews. Finally, we conceptualize college access broadly, as including not just whether but also where students attend and whether they have access to college-level courses. We conclude with a discussion of implications for policy and research.}}, 
pages = {4--22}, 
volume = {51}
}
@article{Castleman.2016, 
year = {2017}, 
title = {{Parental Influences on Postsecondary Decision Making: Evidence From a Text Messaging Experiment}}, 
author = {Castleman, Benjamin L. and Page, Lindsay C.}, 
journal = {Educational Evaluation and Policy Analysis}, 
issn = {0162-3737}, 
doi = {10.3102/0162373716687393}, 
abstract = {{Research increasingly points to the importance of parental engagement in children’s education. Yet, little research has investigated whether prompting parents to be more involved in college processes improves student outcomes. We investigate experimentally whether providing both students and their parents with personalized outreach about tasks students need to complete to enroll in college leads to improved college enrollment outcomes relative to providing outreach to students only. We utilize text messaging to provide information and advising to students and parents. Across treatment arms, the text outreach increased on-time college enrollment by a statistically significant 3.1 percentage points. Texting both parents and students, however, did not increase the efficacy of the outreach. We situate this result in the broader parental engagement literature.}}, 
pages = {361--377}, 
number = {2}, 
volume = {39}
}
@article{Arnold.2015, 
year = {2015}, 
title = {{Advisor and Student Experiences of Summer Support for College- intending, Low-income High School Graduates  intending, Low-income High School Graduates}}, 
author = {Arnold, Kar en D . and Chewning, Alexandra and Castleman, Benjamin and Page, Lindsay}, 
journal = {Journal of College Access}, 
url = {https://scholarworks.wmich.edu/jca/vol1/iss1/3/}, 
number = {1}, 
volume = {1}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Journal%20of%20College%20Access/Advisor%20and%20Student%20Experiences%20of%20Summer%20Support%20for%20College-%20intending,%20Low-income%20High%20School%20Graduates%20%20intending,%20Low-income%20High%20School%20Graduates_2015_Journal%20of%20College%20Access.pdf}
}
@article{Castleman.2015mue, 
year = {2015}, 
title = {{Stay late or start early? Experimental evidence on the benefits of college matriculation support from high schools versus colleges}}, 
author = {Castleman, Benjamin L. and Owen, Laura and Page, Lindsay C.}, 
journal = {Economics of Education Review}, 
issn = {0272-7757}, 
doi = {10.1016/j.econedurev.2015.05.010}, 
abstract = {{The summer melt and academic mismatch literatures have focused largely on college-ready, low-income students. Yet, a broader population of students may also benefit from additional support in formulating and realizing their college plans. We investigate the impact of a unique high school-university partnership to support college-intending students to follow through on their college plans. Specifically, we facilitated a collaborative effort between the Albuquerque Public Schools (APS) and the University of New Mexico (UNM), and randomly assigned 1602 APS graduates admitted to UNM across three experimental conditions: (1) outreach from an APS-based counselor; (2) outreach from a UNM-based counselor; or (3) the control group. Among Hispanic males, who are underrepresented at UNM compared to their APS graduating class, summer outreach improved timely postsecondary matriculation, with suggestive evidence that college-based outreach may be particularly effective. This finding is consistent with the social-psychological literature showing that increasing students’ sense of belonging at college can improve enrollment outcomes.}}, 
pages = {168--179}, 
volume = {47}
}
@article{Castleman.2014, 
year = {2014}, 
title = {{The Forgotten Summer: Does the Offer of College Counseling After High School Mitigate Summer Melt Among College‐Intending, Low‐Income High School Graduates?}}, 
author = {Castleman, Benjamin L. and Page, Lindsay C. and Schooley, Korynn}, 
journal = {Journal of Policy Analysis and Management}, 
issn = {0276-8739}, 
doi = {10.1002/pam.21743}, 
abstract = {{Despite decades of policy intervention to increase college entry and success among low‐income students, considerable gaps by socioeconomic status remain. To date, policymakers have overlooked the summer after high school as an important time period in students’ transition to college, yet recent research documents high rates of summer attrition from the college pipeline among college‐intending high school graduates, a phenomenon we refer to as “summer melt.” We report on two randomized trials investigating efforts to mitigate summer melt. Offering college‐intending graduates two to three hours of summer support increased enrollment by 3 percentage points overall, and by 8 to 12 percentage points among low‐income students, at a cost of \$100 to \$200 per student. Further, summer support has lasting impacts on persistence several semesters into college.}}, 
pages = {320--344}, 
number = {2}, 
volume = {33}
}
@article{Bird.2021, 
year = {2021}, 
title = {{Nudging at scale: Experimental evidence from FAFSA completion campaigns}}, 
author = {Bird, Kelli A. and Castleman, Benjamin L. and Denning, Jeffrey T. and Goodman, Joshua and Lamberton, Cait and Rosinger, Kelly Ochs}, 
journal = {Journal of Economic Behavior \& Organization}, 
issn = {0167-2681}, 
doi = {10.1016/j.jebo.2020.12.022}, 
abstract = {{Do successful local nudge interventions maintain efficacy when scaled state or nationwide? We investigate, through two randomized controlled trials, the impact of a national and state-level campaign encouraging students to apply for financial aid for college. The campaigns collectively reached over 800,000 students, with multiple treatment arms patterned after prior local interventions in order to explore potential mechanisms. We find no impacts on aid receipt or college enrollment overall or for any subgroups. We find no evidence that different approaches to message framing, delivery, or timing, or access to one-on-one advising affected campaign efficacy. We discuss why nudge strategies that work locally may be hard to scale effectively.}}, 
pages = {105--128}, 
volume = {183}
}
@techreport{Howell.2021, 
year = {2021}, 
author = {Howell, Jessica and Hurwitz, Mike and Ma, Jennifer and Pender, Matea and Perfetto, Greg and Wyatt, Jeffrey and Young, Linda}, 
title = {{College Enrollment and Retention in the Era of Covid}}, 
url = {https://research.collegeboard.org/media/pdf/enrollment-retention-covid2020.pdf}, 
institution = {The College Board}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/enrollment-retention-covid2020.pdf}
}
@article{Castleman.2012, 
year = {2012}, 
title = {{Stemming the Tide of Summer Melt: An Experimental Study of the Effects of Post-High School Summer Intervention on Low-Income Students’ College Enrollment}}, 
author = {Castleman, Benjamin L. and Arnold, Karen and Wartman, Katherine Lynk}, 
journal = {Journal of Research on Educational Effectiveness}, 
issn = {1934-5747}, 
doi = {10.1080/19345747.2011.618214}, 
abstract = {{The summer after high school graduation is a largely unexamined stage of college access among underrepresented populations in higher education. Yet two recent studies revealed that anywhere from 10\% to 40\% of low-income students who have been accepted to college and signaled their intent to enroll reconsider where, and even whether, to matriculate in the months after graduation. This experimental study investigates the effect of providing college counseling to low-income students during the summer. We randomly assigned students at 7 innovative high schools to receive proactive outreach from high school counselors. The treatment focused on addressing financial and information barriers students faced. Results show that providing college counseling to low-income students during the summer months leads to substantial improvements in both the rate and quality of college enrollment. Students in the treatment group were 14 percentage points more likely to enroll immediately in college and 19 percentage points more likely to keep the postsecondary plans they developed during senior year. Policy recommendations include strategies for high schools and/or colleges to provide effective support during the post–high school summer.}}, 
pages = {1--17}, 
number = {1}, 
volume = {5}
}
@article{Mitchall.2018, 
year = {2018}, 
title = {{Parental Influences on Low-Income, First-Generation Students’ Motivation on the Path to College}}, 
author = {Mitchall, Allison M. and Jaeger, Audrey J.}, 
journal = {The Journal of Higher Education}, 
issn = {0022-1546}, 
doi = {10.1080/00221546.2018.1437664}, 
abstract = {{Reports abound about the challenges that first-generation, low-income students face on the path to higher education. Yet despite these barriers, millions of low-income, first-generation students persevere. What or who influences their motivation to “stay the course” to higher education? Using self-determination theory as a framework, this cross-case study highlighted the motivational experiences of seven low-income, first-generation students and their parents on the path to college. Specifically, the study explored how parents augmented—or at times, undermined—students’ self-determination toward college, as evidenced by students’ feelings of relatedness, competency, and autonomy. Results showed that students’ self-determination was enhanced when parents were involved in college planning, served as positive examples, set high academic standards early, and fostered students’ sense of career volition. Motivation was undermined when families limited students’ choices, did not set clear expectations for college going, provided little feedback, or emphasized family obligations. The findings have the potential to facilitate deeper understanding of the impact of parents as motivational partners in the college access process.}}, 
pages = {582--609}, 
number = {4}, 
volume = {89}
}
@article{Szaszi.2022, 
year = {2022}, 
title = {{No reason to expect large and consistent effects of nudge interventions}}, 
author = {Szaszi, Barnabas and Higney, Anthony and Charlton, Aaron and Gelman, Andrew and Ziano, Ignazio and Aczel, Balazs and Goldstein, Daniel G. and Yeager, David S. and Tipton, Elizabeth}, 
journal = {Proceedings of the National Academy of Sciences}, 
issn = {0027-8424}, 
doi = {10.1073/pnas.2200732119}, 
pmid = {35858388}, 
pmcid = {PMC9351519}, 
pages = {e2200732119}, 
number = {31}, 
volume = {119}
}
@techreport{Soria.2020, 
year = {2020}, 
author = {Soria, Krista M. and Horgos, Bonnie and Chirikov, Igor and Jones-White, Daniel}, 
title = {{First-Generation Students' Experiences During the COVID-19 Pandemic}}, 
url = {https://conservancy.umn.edu/handle/11299/214934}, 
institution = {Student Experience in the Research University (SERU) Consortium}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/First-Generation_Students.pdf}
}
@article{Page.2022, 
year = {2022}, 
title = {{Financial Aid Nudges: A National Experiment With Informational Interventions}}, 
author = {Page, Lindsay C and Sacerdote, Bruce I and Goldrick-Rab, Sara and Castleman, Benjamin L}, 
journal = {Educational Evaluation and Policy Analysis}, 
issn = {0162-3737}, 
doi = {10.3102/01623737221111403}, 
abstract = {{Despite high prices, many college students do not re-file the Free Application for Federal Student Aid (FAFSA) or file late, making college less affordable. Low-cost technological interventions delivering personalized information and/or advising may improve refiling and academic outcomes, but questions remain regarding the efficacy of this approach at scale. This multi-pronged randomized experiment tested informational and framing text message interventions for a national sample of approximately 10,000 undergraduates. The text outreach caused earlier FAFSA re-filing for some students. However, gains in re-filing during the active intervention period were not sustained after the intervention concluded and did not translate into additional federal financial aid or improved postsecondary persistence or attainment. Implications for the scaling and targeting of nudging are discussed.}}, 
pages = {016237372211114}
}
@article{Barber.2020, 
year = {2020}, 
title = {{Systemic racism in higher education}}, 
author = {Barber, Paul H. and Hayes, Tyrone B. and Johnson, Tracy L. and Márquez-Magaña, Leticia and signatories, 10234}, 
journal = {Science}, 
issn = {0036-8075}, 
doi = {10.1126/science.abd7140}, 
pmid = {32943517}, 
pages = {1440.2--1441}, 
number = {6510}, 
volume = {369}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Science/Systemic%20racism%20in%20higher%20education_2020_Science.pdf}
}
@article{Nguyen.2017, 
year = {2017}, 
title = {{Double-adjustment in propensity score matching analysis: choosing a threshold for considering residual imbalance}}, 
author = {Nguyen, Tri-Long and Collins, Gary S. and Spence, Jessica and Daurès, Jean-Pierre and Devereaux, P. J. and Landais, Paul and Manach, Yannick Le}, 
journal = {BMC Medical Research Methodology}, 
doi = {10.1186/s12874-017-0338-0}, 
pmid = {28454568}, 
pmcid = {PMC5408373}, 
abstract = {{Double-adjustment can be used to remove confounding if imbalance exists after propensity score (PS) matching. However, it is not always possible to include all covariates in adjustment. We aimed to find the optimal imbalance threshold for entering covariates into regression. We conducted a series of Monte Carlo simulations on virtual populations of 5,000 subjects. We performed PS 1:1 nearest-neighbor matching on each sample. We calculated standardized mean differences across groups to detect any remaining imbalance in the matched samples. We examined 25 thresholds (from 0.01 to 0.25, stepwise 0.01) for considering residual imbalance. The treatment effect was estimated using logistic regression that contained only those covariates considered to be unbalanced by these thresholds. We showed that regression adjustment could dramatically remove residual confounding bias when it included all of the covariates with a standardized difference greater than 0.10. The additional benefit was negligible when we also adjusted for covariates with less imbalance. We found that the mean squared error of the estimates was minimized under the same conditions. If covariate balance is not achieved, we recommend reiterating PS modeling until standardized differences below 0.10 are achieved on most covariates. In case of remaining imbalance, a double adjustment might be worth considering.}}, 
pages = {78}, 
number = {1}, 
volume = {17}
}
@article{Dynarski.2022dch, 
year = {2022}, 
title = {{College Costs, Financial Aid, and Student Decisions}}, 
author = {Dynarski, Susan and Page, Lindsay and Scott-Clayton, Judith}, 
journal = {National Bureau of Economic Research}, 
doi = {10.3386/w30275}
}
@article{Covarrubias.2022, 
year = {2022}, 
title = {{What Institutions Can Learn From the Navigational Capital of Minoritized Students}}, 
author = {Covarrubias, Rebecca and Laiduc, Giselle and Valle, Ibette}, 
journal = {Journal of First-generation Student Success}, 
issn = {2690-6015}, 
doi = {10.1080/26906015.2022.2065109}, 
abstract = {{Minoritized students — low-income, first-generation students of color — leverage skills when navigating higher education. Yet, institutions often misrecognize this navigational capital, rendering it invaluable and missing an opportunity to learn from students. We explored how 16 first-year minoritized students — who participated in a counterspace aimed to affirm their experiences — translated their navigational capital into feedback for institutional change. Through surveys, students reported that institutional practices perpetuated misrecognition by privileging Whiteness; endorsing deficit assumptions about students’ abilities; and making campus resources inaccessible. Activating their navigational capital, students offered concrete advice for how institutions can better recognize and support their lived experiences.}}, 
pages = {36--53}, 
number = {1}, 
volume = {2}
}
@article{Harper.2010, 
year = {2010}, 
title = {{An anti‐deficit achievement framework for research on students of color in STEM}}, 
author = {Harper, Shaun R.}, 
journal = {New Directions for Institutional Research}, 
issn = {0271-0579}, 
doi = {10.1002/ir.362}, 
abstract = {{A framework adapted from the National Black Male College Achievement Study is introduced in this chapter as a lens through which to explore the enablers of student achievement in STEM. The chapter places an emphasis on reframing deficit‐oriented research questions regarding students of color and their trajectories in STEM fields.}}, 
pages = {63--74}, 
number = {148}, 
volume = {2010}
}
@article{Garriott.2020, 
year = {2020}, 
title = {{A Critical Cultural Wealth Model of First-Generation and Economically Marginalized College Students’ Academic and Career Development}}, 
author = {Garriott, Patton O}, 
journal = {Journal of Career Development}, 
issn = {0894-8453}, 
doi = {10.1177/0894845319826266}, 
abstract = {{First-generation and economically marginalized (FGEM) college students are attending higher education institutions with increasing regularity. The unique experiences of these students call for career frameworks that capture their specific strengths and challenges. This article outlines a new model from which to conceptualize FGEM college students’ academic and career development with a focus on structural, environmental, and intrapersonal factors previously shown to predict their academic and career success. Social–emotional crossroads and career self-authorship are positioned as central constructs in the model and proposed as critical pathways to FGEM students’ academic and career development. Cultural wealth is offered as a form of capital that may promote FGEM students’ academic and career success. Implications for future research, practice, and policy with FGEM students are described.}}, 
pages = {80--95}, 
number = {1}, 
volume = {47}
}
@article{Ayala.2019, 
year = {2019}, 
title = {{It’s Capital! Understanding Latina/o Presence in Higher Education}}, 
author = {Ayala, María Isabel and Contreras, Sheila Marie}, 
journal = {Sociology of Race and Ethnicity}, 
issn = {2332-6492}, 
doi = {10.1177/2332649218757803}, 
abstract = {{Latina/o educational differentials have been studied predominantly from a deficit standpoint that emphasizes a lack of cultural capital. More recently, researchers began to reject this deficit perspective, foregrounding instead the cultural capital that enables Latina/o students to succeed academically. The very idea that Latina/o students possess cultural capital is new to higher education, which has historically undervalued Latina/o student experience and community history. Cultural capital, however, plays a crucial role in Latina/o student academic achievement. The authors’ study of junior- and senior-level Latina/o students attending a primarily White four-year research university in the Midwest examines Latina/o community cultural wealth, particularly with regard to navigating the academy. The authors also suggest institutional changes to develop policies that address Latina/o student presence from a capital rather than a deficit perspective.}}, 
pages = {229--243}, 
number = {2}, 
volume = {5}
}
@article{Hernandez.2022, 
year = {2022}, 
title = {{An Anti-Deficit Investigation of Resilience Among University Students with Adverse Experiences}}, 
author = {Hernandez, Ruby and Covarrubias, Rebecca and Radoff, Sara and Moya, Elizabeth and Mora, Ángel Jesús}, 
journal = {Journal of College Student Retention: Research, Theory \& Practice}, 
issn = {1521-0251}, 
doi = {10.1177/15210251221109950}, 
abstract = {{Experiencing extreme adversity — such as homelessness, abuse, or incarceration — creates barriers for educational success. Yet, there is a dearth of literature on the higher education pathways of students who experienced adversity (SEA). A researcher-practitioner collaboration aimed to understand the structural challenges SEA navigated on campus and the factors promoting their resilience and retention. Ten SEA-identified students participated in semi-structured in-depth interviews. Using thematic analyses, we constructed three themes describing challenges they encountered, including experiencing a lack of community, safety concerns, and stigmatization. We also noted four themes reflecting resilience, including how SEA found a sanctuary through a campus program, gave back to similar others, reframed stigmatization, and gained personal strength through networks. An anti-deficit framework helped identify how SEA are leveraging their community strengths and past experiences to thrive in institutions still not fully prepared to serve them. We offer recommendations for how institutions can be more inclusive of SEA.}}, 
pages = {152102512211099}
}
@article{Oreopoulos.2020, 
year = {2020}, 
title = {{Promises and Limitations of Nudging in Education}}, 
author = {Oreopoulos, Philip}, 
journal = {SSRN Electronic Journal}, 
doi = {10.2139/ssrn.3695419}, 
abstract = {{This article takes stock of where the field of behavioral science applied to education policy seems to be at, which avenues seem promising and which ones seem like dead ends. I present a curated set of studies rather than an exhaustive literature review, categorizing interventions by whether they nudge (keep options intact) or "shove" (restrict choice), and whether they apply a high or low touch (whether they use face-to-face interaction or not). Many recent attempts to test large-scale low touch nudges find precisely estimated null effects, suggesting we should not expect letters, text messages, and online exercises to serve as panaceas for addressing education policy's key challenges. Programs that impose more choice-limiting structure to a youth's routine, like mandated tutoring, or programs that nudge parents, appear more promising.}}, 
keywords = {}
}
@article{Wang.2020, 
year = {2020}, 
title = {{Investigating Mental Health of US College Students During the COVID-19 Pandemic: Cross-Sectional Survey Study}}, 
author = {Wang, Xiaomei and Hegde, Sudeep and Son, Changwon and Keller, Bruce and Smith, Alec and Sasangohar, Farzan}, 
journal = {Journal of Medical Internet Research}, 
issn = {1439-4456}, 
doi = {10.2196/22817}, 
pmid = {32897868}, 
pmcid = {PMC7505693}, 
abstract = {{Evidence suggests that the COVID-19 pandemic has generally increased levels of stress and depression among the public. However, the impact on college students in the United States has not been well-documented. This paper surveys the mental health status and severity of depression and anxiety of college students in a large university system in the United States during the COVID-19 pandemic. An online survey was conducted among undergraduate and graduate students recruited from Texas A\&M University via email. The survey consisted of two standardized scales—the Patient Health Questionnaire-9 and the General Anxiety Disorder-7—for depression and anxiety, and additional multiple-choice and open-ended questions regarding stressors and coping mechanisms specific to COVID-19. Among the 2031 participants, 48.14\% (n=960) showed a moderate-to-severe level of depression, 38.48\% (n=775) showed a moderate-to-severe level of anxiety, and 18.04\% (n=366) had suicidal thoughts. A majority of participants (n=1443, 71.26\%) indicated that their stress/anxiety levels had increased during the pandemic. Less than half of the participants (n=882, 43.25\%) indicated that they were able to cope adequately with the stress related to the current situation. The proportion of respondents showing depression, anxiety, and/or suicidal thoughts is alarming. Respondents reported academic-, health-, and lifestyle-related concerns caused by the pandemic. Given the unexpected length and severity of the outbreak, these concerns need to be further understood and addressed.}}, 
pages = {e22817}, 
number = {9}, 
volume = {22}
}
@article{Hoover.2020, 
year = {2020}, 
title = {{How Is Covid-19 Changing Prospective Students’ Plans? Here’s an Early Look}}, 
author = {Hoover, Eric}, 
journal = {The Chronicle of Higher Education}, 
url = {https://www.chronicle.com/article/how-is-covid-19-changing-prospective-students-plans-heres-an-early-look/}, 
journaltitle = {How Is Covid-19 Changing Prospective Students’ Plans? Here’s an Early Look}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/The%20Chronicle%20of%20Higher%20Education/How%20Is%20Covid-19%20Changing%20Prospective%20Students’%20Plans-%20Here’s%20an%20Early%20Look_2020_The%20Chronicle%20of%20Higher%20Education_1.pdf}
}
@article{Molock.2022, 
year = {2022}, 
title = {{The impact of COVID-19 on college students from communities of color}}, 
author = {Molock, Sherry Davis and Parchem, Benjamin}, 
journal = {Journal of American College Health}, 
issn = {0744-8481}, 
doi = {10.1080/07448481.2020.1865380}, 
pmid = {33502970}, 
abstract = {{Objective: To assess the impact of the COVID-19 pandemic on daily living, mental well-being, and experiences of racial discrimination among college students from communities of color. Participants: Sample comprised 193 ethnically diverse college students, aged 18 to 25 years (M = 20.5 years), who were participating in virtual internships due to the COVID-19 pandemic. Methods: A cross-sectional 16-item survey was developed as a partnership between two nonprofit organizations. The survey included both close-ended and open-ended questions assessing the impact of COVID-19. Results: The students of color reported disruptive changes in finances (54\%), living situation (35\%), academic performance (46\%), educational plans (49\%), and career goals (36\%). Primary mental health challenges included stress (41\%), anxiety (33\%), and depression (18\%). Students also noted challenges managing racial injustice during the COVID-19 pandemic. Conclusions: Higher education institutions will benefit from financially and emotionally supporting students of color during the COVID-19 pandemic and growing visibility of systemic racism.}}, 
pages = {2399--2405}, 
number = {8}, 
volume = {70}
}
@article{Oblinger.2018, 
year = {2018}, 
title = {{It's Not Just About the Technology—It's What You Do With It That Counts}}, 
author = {Oblinger, Diana G.}, 
journal = {Change: The Magazine of Higher Learning}, 
issn = {0009-1383}, 
doi = {10.1080/00091383.2018.1509584}, 
pages = {40--43}, 
number = {3-4}, 
volume = {50}
}
@article{Carayannopoulos.2018, 
year = {2018}, 
title = {{Using chatbots to aid transition}}, 
author = {Carayannopoulos, Sofy}, 
journal = {The International Journal of Information and Learning Technology}, 
issn = {2056-4880}, 
doi = {10.1108/ijilt-10-2017-0097}, 
abstract = {{The purpose of this paper is to examine how chatbots can be used to address two key struggles that students face in first year – a sense of being disconnected from the instructor, and information overload. The authors propose that chatbots can be a useful tool for helping students navigate the volumes of information that confront them as they begin attending university, while at the same time feeling somewhat personally connected with the instructor. This is achieved without increasing instructor time commitment, and perhaps reducing it in large classes. The paper reveals the results of applying this tool in a large first year class and proposes improvements for future iterations. A tool was designed and implemented and tested against research insights. Chatbots are an effective means to reduce student transition challenges. Technology which feels social and personal as well as functioning on a tool that students use will make the student feel more connected to the course and the instructor. Tools aiding transition should be easy to use and allow customizable information access. Chatbots are an unexplored tool. They have the benefit of addressing information overload as well as making the student feel socially connected without increasing instructor workload.}}, 
pages = {118--129}, 
number = {2}, 
volume = {35}
}
@article{LeBouef.2021, 
year = {2021}, 
title = {{First-Generation College Students and Family Support: A Critical Review of Empirical Research Literature}}, 
author = {LeBouef, Samantha and Dworkin, Jodi}, 
journal = {Education Sciences}, 
doi = {10.3390/educsci11060294}, 
abstract = {{The majority of empirical literature on first generation college students (FGCSs) in the U.S. asserts that because their parents did not attend college, FGCSs are lacking important resources to be successful in college. However, this results in a deficit-based approach to the study of FGCSs that tends to highlight the differences between first-generation and continuing-education students. However, FGCSs possess a wealth of resources from parents and families that make them successful, and that are often ignored in research. Asset-based approaches to the study of FGCSs are becoming more frequent in the form of books, book chapters, and white papers; however, published empirical research has yet to adopt this approach. As a result, a deeper understanding of FGCSs’ experiences is essential to advancing diversity and equity in higher education. To begin to address this gap, a systematic literature review of empirical studies following the PRISMA framework was conducted on first generation college students and family support; the literature was critically reviewed and future directions for the field were identified. Applying a critical, cultural, and familial lens to the study of first-generation college students will contribute to reframing the research narrative towards an asset-based narrative.}}, 
pages = {294}, 
number = {6}, 
volume = {11}
}
@article{Schuyler.2021, 
year = {2021}, 
title = {{Promoting Success for First-Generation Students of Color: The Importance of Academic, Transitional Adjustment, and Mental Health Supports}}, 
author = {Schuyler, Sophie W. and Childs, Jonique R. and Poynton, Timothy A.}, 
journal = {Journal of College Access}, 
url = {https://files.eric.ed.gov/fulltext/EJ1313619.pdf}, 
number = {1}, 
volume = {6}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Journal%20of%20College%20Access/Promoting%20Success%20for%20First-Generation%20Students%20of%20Color-%20The%20Importance%20of%20Academic,%20Transitional%20Adjustment,%20and%20Mental%20Health%20Supports_2021_Journal%20of%20College%20Access.pdf}
}
@techreport{Clearinghouse.2021, 
year = {2021}, 
author = {Clearinghouse, National Student}, 
title = {{High School Benchmarks 2021 - National College Progression Rates}}, 
url = {https://nscresearchcenter.org/wp-content/uploads/2021\_HSBenchmarksCovidReport.pdf}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/2021_HSBenchmarksReport_2.pdf}
}
@article{McLewis.2021, 
year = {2021}, 
title = {{Higher Education: Handbook of Theory and Research, Volume 36}}, 
author = {McLewis, Channel C.}, 
journal = {Higher Education: Handbook of Theory and Research}, 
issn = {0882-4126}, 
doi = {10.1007/978-3-030-44007-7\_6}, 
abstract = {{Research on the college decision-making process is extensive. However, fewer approaches have employed a critical lens to explore how power and its relation to students, schools, and higher education institutions shape students’ college pathways and trajectories. In this current chapter, Black Feminist Thought (Collins, Social Problems, 33(6):s14–s32, 1986; Collins, Black feminist thought: Knowledge, consciousness, and the politics of empowerment, Routledge, 2002) is employed to examine how intersecting systems of oppression (i.e., institutionalized racism, sexism, capitalism, etc.) and power shape the college “choice” process. I extend on previous literature on educational inequities to consider the structural forces that constrain educational opportunities. In particular, through the standpoint of Black women and girls, I rely on constructs such as the matrix of domination and controlling images to highlight the limits of college “choice.” The aim is to the examine the various ways “choice” is constrained for Black women and girls, in order to develop transformative mechanisms to improve access to adequate education, increase college participation, and enhance life opportunities. Findings include how narrow depictions of Black women and girls and the trope of the advantaged Black woman in education stifle educational opportunity.}}, 
pages = {105--160}
}
@phdthesis{Resendez.2022, 
year = {2022}, 
title = {{Latinx Pandemic Melt:  A Phenomenological Study of Summer Melt during the COVID -19 Pandemic}}, 
author = {Resendez, Jessica}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/ResendezJessica_Summer2022_1.pdf}
}
@article{Tichavakunda.20207j, 
year = {2020}, 
title = {{The Summer Before College: A Case Study of First-Generation, Urban High School Graduates}}, 
author = {Tichavakunda, Antar and Galan, Carlos}, 
journal = {Urban Education}, 
issn = {0042-0859}, 
doi = {10.1177/0042085920914362}, 
abstract = {{Often without guidance in completing college-related tasks, first-generation students face unique challenges during the summer before college. This case study investigates this critical time period by studying a cohort of 33 newly graduated students from the same urban, public high school. Guided by social capital, college readiness, and nepantla frameworks, results shed light on students’ barriers and pathways to transitioning to postsecondary education. The authors call for an extension of college readiness frameworks to the summer before college and also problematize the notion of a college-ready student.}}, 
pages = {004208592091436}
}
@article{Oreopoulos.2021, 
year = {2021}, 
title = {{What Limits College Success? A Review and Further Analysis of Holzer and Baum’s Making College Work}}, 
author = {Oreopoulos, Philip}, 
journal = {Journal of Economic Literature}, 
issn = {0022-0515}, 
doi = {10.1257/jel.20191614}, 
abstract = {{Harry J. Holzer and Sandy Baum’s recent book, Making College Work: Pathways to Success for Disadvantaged Students, provides an excellent up-to-date review of higher education. My review first summarizes its key themes: (i) who gains from college and why, (ii) mismatch and the need for more structure, (iii) problems with remediation, (iv) financial barriers, and (v) the promise of comprehensive support. I then critique the book’s proposed solutions using some of my own qualitative and quantitative data. Some recommendations are worth considering, while others are too expensive or unlikely to make a meaningful difference without addressing the underlying lack of preparedness and motivation of college students. I argue that making mandatory some existing services, such as application assistance and advice, proactive tutoring and advising, and greater career transition support, has the most immediate potential. (JEL I22, I23, I24)}}, 
pages = {546--573}, 
number = {2}, 
volume = {59}
}
@article{Naughton.2021, 
year = {2021}, 
title = {{Cracks to Craters: College Advising During COVID-19}}, 
author = {Naughton, Meredith R.}, 
journal = {AERA Open}, 
issn = {2332-8584}, 
doi = {10.1177/23328584211018715}, 
abstract = {{The COVID-19 (coronavirus disease–2019) pandemic disrupted the education of students across the globe in the spring of 2020. Students who were previously at most risk for falling behind their peers and through the cracks because of academic, financial, racial, and/or generational disadvantage faced a wide range of additional obstacles in the pursuit of their college goals. This qualitative study sought to uncover postsecondary advising implications for students through the perspectives of near-peer college advisers (n = 23) serving in high-need schools in two different states as intensive, in-person advising was forced to adapt to virtual formats. Two key thematic findings reveal that advisers faced new communication challenges and existing systemic barriers for marginalized students became even larger. For seniors who had not yet made final postsecondary decisions or who had remaining to-dos, the impact of school closures and distanced advising may have fatally widened existing cracks in the path to college.}}, 
pages = {23328584211018715}, 
volume = {7}
}
@article{Liu.2021, 
year = {2021}, 
title = {{Disparities in Disruptions to Postsecondary Education Plans During the COVID-19 Pandemic}}, 
author = {Liu, Ran}, 
journal = {AERA Open}, 
issn = {2332-8584}, 
doi = {10.1177/23328584211045400}, 
abstract = {{This study examines disruptions to postsecondary education plans in the United States during the COVID-19 pandemic. Using nationally representative data from the U.S. Census Bureau’s Household Pulse Survey from August 2020 through March 2021, we investigate the prevalence, forms, reasons, and disparities of education disruption across different sociodemographic groups. While nearly three in four households report education plan disruption, the forms and reasons are drastically different. Black and Latinx respondents are more likely to report plan cancellation, while Whites are more likely to report taking classes in different formats. Non-White groups are more likely to cancel plans due to health or financial concerns, while Whites are more likely to cancel plans due to concerns about changes to campus life. Results also reveal nuanced intersections of race, type of education plans, and household vulnerability in affecting education disruption, pointing to the necessity of well-targeted initiatives to address long-term consequences and resulting inequality.}}, 
pages = {23328584211045400}, 
volume = {7}
}
@article{Tippetts.2022, 
year = {2022}, 
title = {{Thx 4 the msg: Assessing the Impact of Texting on Student Engagement and Persistence}}, 
author = {Tippetts, Megan M. and Davis, Bobbi and Nalbone, Stephanie and Zick, Cathleen D.}, 
journal = {Research in Higher Education}, 
issn = {0361-0365}, 
doi = {10.1007/s11162-022-09678-8}, 
pmid = {35194300}, 
pmcid = {PMC8853065}, 
abstract = {{As colleges and universities strive to increase persistence and aid students in reaching graduation, they are utilizing alternative communication strategies like text messaging. Behavioral economics researchers suggest personalized, regular nudges can help college students make decisions that positively impact their college career and keep them on track for graduation. The current study presents the results of a randomized field experiment where a text messaging program was implemented in a large college at a public university. The intervention utilized a mixture of automated and personalized text messages from academic advisors and allowed for two-way communication between individual students and their major advisor. Mulitvariate analyses revealed the intervention had no impact on university persistence, but it did increase the odds of persisting in the college to the end of the semester, moving the average, overall college persistence rate from 93 to 95\%. Effects were concentrated on underclass students, whose persistence rate moved from 87 to 93\% at the college level. Underclass students also showed statistically significant university persistence effects, moving from 90 to 95\%. Students who received texts but never engaged with the texting program were significantly less likely to request an advising appointment or to apply to be a student ambassador than were students in the control group. More research is needed to understand what motivates a student to engage with the texting software and to identify what the longer-term consequences of using text messaging to communicate with students might be.}}, 
pages = {1073--1093}, 
number = {6}, 
volume = {63}
}
@article{Ives.2020, 
year = {2020}, 
title = {{First-Generation College Students as Academic Learners: A Systematic Review}}, 
author = {Ives, Jillian and Castillo-Montoya, Milagros}, 
journal = {Review of Educational Research}, 
issn = {0034-6543}, 
doi = {10.3102/0034654319899707}, 
abstract = {{The literature on first-generation college students largely focuses on the challenges and barriers they may experience in college. Yet, we do not have a clear understanding of who these students are as learners. To address this gap, this systematic review examines how scholars study and conceptualize first-generation college students as learners. We found the majority of the literature we reviewed conceptualized them as learners based on their academic performance and the influence of cultures on their learning. These two conceptualizations positioned first-generation college students against normative ways of learning, and in doing so promulgate an assimilation approach in higher education. We found a smaller body of literature that conceptualized first-generation college students as learners whose lived experiences, when connected to academic content, can contribute to their academic learning, advancement of disciplines, self-growth, and community development. We use this alternative view to provide recommendations for studying and working with first-generation college students.}}, 
pages = {139--178}, 
number = {2}, 
volume = {90}
}
@techreport{Cataldi.2018, 
year = {2018}, 
author = {Cataldi, Emily Forrest and Bennet, Christopher T. and Chen, Xianglei}, 
title = {{First-Generation Students: College Access, Persistence, and Postbachelor's Outcomes}}, 
url = {https://nces.ed.gov/pubs2018/2018421.pdf}, 
institution = {U.S. Department of Education}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Stats%20in%20Brief/First-Generation%20Students-%20College%20Access,%20Persistence,%20and%20Postbachelor's%20Outcomes_2018_Stats%20in%20Brief.pdf}
}
@article{Page.2019, 
year = {2019}, 
title = {{Customized Nudging to Improve FAFSA Completion and Income Verification}}, 
author = {Page, Lindsay C. and Castleman, Benjamin L. and Meyer, Katharine}, 
journal = {Educational Evaluation and Policy Analysis}, 
issn = {0162-3737}, 
doi = {10.3102/0162373719876916}, 
abstract = {{Informational and behavioral barriers hinder social benefit take-up. We investigate the impact of mitigating these barriers through providing personalized information on benefits application status and application assistance on filing the Free Application for Federal Student Aid (FAFSA), the gateway to college financial aid. Through a multidistrict experiment, we assess the impact of this outreach, delivered via text message. This data-driven strategy improves FAFSA completion and college matriculation and potentially reduces the negative consequences of additional procedural hurdles such as FAFSA income verification, required of approximately one third of filers nationally.}}, 
pages = {3--21}, 
number = {1}, 
volume = {42}, 
keywords = {}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/Educational%20Evaluation%20and%20Policy%20Analysis/Customized%20Nudging%20to%20Improve%20FAFSA%20Completion%20and%20Income%20Verification_2019_Educational%20Evaluation%20and%20Policy%20Analysis.pdf}
}
@article{Dynarski.2022, 
year = {2022}, 
title = {{Addressing non- financial barriers to college access and success:  Evidence and policy implications}}, 
author = {Dynarski, Susan and Nushatayeva, Aizat and Page, Lindsay C. and Scott-Clayton, Judith}, 
journal = {National Bureau of Economic Research}, 
doi = {10.3386/w30054}, 
journaltitle = {NBER Working Paper Series}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/National%20Bureau%20of%20Economic%20Research/Addressing%20non-%20financial%20barriers%20to%20college%20access%20and%20success-%20%20Evidence%20and%20policy%20implications_2022_National%20Bureau%20of%20Economic%20Research_1.pdf}
}
@article{Gurantz.2019, 
year = {2019}, 
title = {{Realizing Your College Potential? Impacts of  College Board ’ s RYCP Campaign on  Postsecondary Enrollment}}, 
author = {Gurantz, Oded and Howell, Jessica and Hurwitz, Michael and Larson, Cassandra and Pender, Matea and White, Brooke}, 
journal = {EdWorkingPaper}, 
doi = {10.26300/nqn3-sp29}, 
journaltitle = {Realizing Your College Potential? Impacts of  College Board
’
s RYCP Campaign on 
Postsecondary Enrollment}
}
@article{Goodman.2017, 
year = {2017}, 
title = {{Access to 4-Year Public Colleges and Degree Completion}}, 
author = {Goodman, Joshua and Hurwitz, Michael and Smith, Jonathan}, 
journal = {Journal of Labor Economics}, 
issn = {0734-306X}, 
doi = {10.1086/690818}, 
pages = {829--867}, 
number = {3}, 
volume = {35}
}
@article{Page.2017jj, 
year = {2017}, 
title = {{How an Artificially Intelligent Virtual Assistant Helps Students Navigate the Road to College}}, 
author = {Page, Lindsay C. and Gehlbach, Hunter}, 
journal = {AERA Open}, 
issn = {2332-8584}, 
doi = {10.1177/2332858417749220}, 
abstract = {{Deep reinforcement learning using convolutional neural networks is the technology behind autonomous vehicles. Could this same technology facilitate the road to college? During the summer between high school and college, college-related tasks that students must navigate can hinder successful matriculation. We employ conversational artificial intelligence (AI) to efficiently support thousands of would-be college freshmen by providing personalized, text message–based outreach and guidance for each task where they needed support. We implemented and tested this system through a field experiment with Georgia State University (GSU). GSU-committed students assigned to treatment exhibited greater success with pre-enrollment requirements and were 3.3 percentage points more likely to enroll on time. Enrollment impacts are comparable to those in prior interventions but with substantially reduced burden on university staff. Given the capacity for AI to learn over time, this intervention has promise for scaling personalized college transition guidance.}}, 
pages = {2332858417749220}, 
number = {4}, 
volume = {3}
}
@article{Wallace.2021, 
year = {2021}, 
title = {{College students’ experiences early in the COVID-19 pandemic: Applications for ongoing support}}, 
author = {Wallace, Kate F and Putnam, Natalia I and Chow, Eva and Fernandes, Meghan and Clary, Kelsey M and Goff, Sarah L}, 
journal = {Journal of American College Health}, 
issn = {0744-8481}, 
doi = {10.1080/07448481.2021.1954011}, 
pmid = {34403622}, 
abstract = {{To explore U.S. college students' experiences during the onset of the COVID-19 pandemic.}}, 
pages = {1--10}
}
@article{Potts.2021, 
year = {2021}, 
title = {{Seen and Unseen: First-Year College Students’ Sense of Belonging During the Covid-19 Pandemic}}, 
author = {Potts, Charlie}, 
journal = {College Student Affairs Journal}, 
doi = {10.1353/csj.2021.0018}, 
pages = {214--224}, 
number = {2}, 
volume = {39}
}
@article{Lee.2021, 
year = {2021}, 
title = {{Impact of COVID-19 on the mental health of US college students}}, 
author = {Lee, Jenny and Solomon, Matthew and Stead, Tej and Kwon, Bryan and Ganti, Latha}, 
journal = {BMC Psychology}, 
doi = {10.1186/s40359-021-00598-3}, 
pmid = {34103081}, 
pmcid = {PMC8185692}, 
abstract = {{In the beginning of 2020, the novel Coronavirus disease (COVID-19) caused by the SARS-CoV-2 virus, became a public health emergency in the U.S. and rapidly escalated into a global pandemic. Because the SARS-CoV-2 virus is highly contagious, physical distancing was enforced and indoor public spaces, including schools and educational institutions, were abruptly closed and evacuated to ensure civilian safety. Accordingly, educational institutions rapidly transitioned to remote learning. We investigated the impacts of the COVID-19 pandemic on domestic U.S. college students, ages 18–24 years. Through Pollfish®’s survey research platform, we collected data from 200 domestic U.S. college students in this age range (N = 200) regarding the physical, emotional, and social impacts of COVID-19 as well as key background information (e.g. whether or not they are first-generation or if they identify with the LGBTQIA+ community). Our results indicate that students closer to graduating faced increases in anxiety (60.8\%), feeling of loneliness (54.1\%), and depression (59.8\%). Many reported worries for the health of loved ones most impacted their mental health status (20.0\%), and the need to take care of family most affected current and future plans (31.8\%). Almost one-half of students took to exercising and physical activity to take care of their mental health (46.7\%). While a third did not have strained familial relationships (36.5\%), almost one half did (45.7\%). A majority found it harder to complete the semester at home (60.9\%), especially among those who had strained relationships with family (34.1\%). Seventy percent spent time during the pandemic watching television shows or movies. Significantly more men, first-generation, and low-income students gained beneficial opportunities in light of the pandemic, whereas their counterparts reported no impact. First-generation students were more likely to take a gap year or time off from school. Although students found ways to take care of themselves and spent more time at home, the clear negative mental health impacts call for schools and federal regulations to accommodate, support, and make mental health care accessible to all students.}}, 
pages = {95}, 
number = {1}, 
volume = {9}
}
@article{Liu.2022, 
year = {2022}, 
title = {{Priorities for addressing the impact of the COVID-19 pandemic on college student mental health}}, 
author = {Liu, Cindy H. and Pinder-Amaker, Stephanie and Hahm, Hyeouk “Chris” and Chen, Justin A.}, 
journal = {Journal of American College Health}, 
issn = {0744-8481}, 
doi = {10.1080/07448481.2020.1803882}, 
pmid = {33048654}, 
pmcid = {PMC8041897}, 
abstract = {{The COVID-19 pandemic has already produced profound impacts on college students, with unprecedented directives for student relocation from their college campuses and dormitories mid-semester and coursework that took place through virtual learning. The current disruptions and anticipated potential long-term changes call for immediate prioritization regarding next steps for addressing college mental health and well-being. This viewpoint article highlights two urgent priorities for addressing current college mental health needs: the development of strategies for ensuring mental health service access, and intentional outreach to college students with special circumstances. The current crisis also represents an opportunity for campus administrators, mental health professionals, researchers, and policymakers to leverage innovative models of care as well as identity-related student assets, strengths, and resilience-promoting factors to support students’ eventual return to campus and to respond more effectively to future massive disruptions.}}, 
pages = {1356--1358}, 
number = {5}, 
volume = {70}
}
@article{Knight.2022, 
year = {2022}, 
title = {{Reducing Frictions in College Admissions: Evidence from the Common Application}}, 
author = {Knight, Brian and Schiff, Nathan}, 
journal = {American Economic Journal: Economic Policy}, 
issn = {1945-7731}, 
doi = {10.1257/pol.20190694}, 
abstract = {{College admissions in the United States are decentralized, creating frictions that limit student choice. We study the Common Application (CA) platform, under which students submit a single application to member schools, potentially reducing frictions and increasing student choice. The CA increases the number of applications received by schools, reflecting a reduction in frictions, and reduces the yield on accepted students, reflecting increased choice. The CA increases out-of-state enrollment, especially from other CA states, consistent with network effects. Entry into the CA changes the composition of students, with evidence of more racial diversity and more high-income students and imprecise evidence of increases in SAT scores. (JEL I23, I28)}}, 
pages = {179--206}, 
number = {1}, 
volume = {14}
}
@article{Herbaut.2019, 
year = {2019}, 
title = {{What works to reduce inequalities in higher education? A systematic review of the (quasi-)experimental literature on outreach and financial aid}}, 
author = {Herbaut, Estelle and Geven, Koen}, 
journal = {Research in Social Stratification and Mobility}, 
issn = {0276-5624}, 
doi = {10.1016/j.rssm.2019.100442}, 
abstract = {{ The quickly growth of the (quasi-)experimental literature on policy interventions in higher education provides an opportunity to identify the causal effects of these interventions on disadvantaged students. It also allows for a reflection on the mechanisms driving exclusion the last stage of the educational system. We selected 71 studies and rigorously gathered and compared more than 200 causal effects of outreach and financial aid interventions on access and completion rates of disadvantaged students in higher education. We find that outreach policies are broadly effective in raising access of disadvantaged students when they include active counselling or simplify the university application process, but not when they only provide general information on higher education. In terms of financial aid, we find that need-based grants do not systematically raise enrolment rates but only lead to improvements when they provide enough money to cover unmet need and/or include an early commitment during high school. Still, need-based grants quite consistently appear to improve completion rates of disadvantaged students. In contrast, the evidence indicate that merit-based grants only rarely improve outcomes of disadvantaged students. Finally, interventions combining outreach and financial aid have brought promising results although more research on these mixed-interventions is still needed. We also discuss the theoretical implications of these findings for our understanding of the mechanisms driving social inequalities in higher education.}}, 
pages = {100442}, 
volume = {65}
}
@article{Avery.2021, 
year = {2021}, 
title = {{Digital messaging to improve college enrollment and success}}, 
author = {Avery, Christopher and Castleman, Benjamin L. and Hurwitz, Michael and Long, Bridget Terry and Page, Lindsay C.}, 
journal = {Economics of Education Review}, 
issn = {0272-7757}, 
doi = {10.1016/j.econedurev.2021.102170}, 
abstract = {{We investigate the efficacy of text messaging campaigns to support students with key steps in the college search, application, selection and transition process with concurrent cluster randomized control trials. Collaborating with national non-profit organizations, we implemented text-message based outreach to students in the high school graduating class of 2016 in over 700 high schools that primarily serve low-income students. Collaborating with several Texas school districts, we implemented a school-based version of the intervention. In contrast to the national version which produced null and negative effects, the school-based intervention yielded positive and significant impacts on several college-going steps but still only yielded significant impacts on college enrollment and second-year persistence for certain subgroups.}}, 
pages = {102170}, 
volume = {84}
}
@article{Nurshatayeva.2021, 
year = {2021}, 
title = {{Are Artificially Intelligent Conversational Chatbots Uniformly Effective in Reducing Summer Melt? Evidence from a Randomized Controlled Trial}}, 
author = {Nurshatayeva, Aizat and Page, Lindsay C. and White, Carol C. and Gehlbach, Hunter}, 
journal = {Research in Higher Education}, 
issn = {0361-0365}, 
doi = {10.1007/s11162-021-09633-z}, 
abstract = {{Our field experiment extends prior work on college matriculation by testing the extent to which an artificially intelligent (AI) chatbot’s outreach and support to college students (N = 4442) reduced summer melt and improved first-year college enrollment at a 4-year university. Specifically, we investigate which students the intervention proves most effective for. We find that the AI chatbot increased overall success with navigating financial aid processes, such that student take up of educational loans increased by four percentage points. This financial aid effect was concentrated among would-be first-generation college goers, for whom loan acceptances increased by eight percentage points. In addition, the outreach increased first-generation students’ success with course registration and fall semester enrollment each by three percentage points. Our findings suggest that proactive chatbot outreach to students is likely to be most successful in reducing summer melt among those who may need the chatbot support the most.}}, 
pages = {392--402}, 
number = {3}, 
volume = {62}
}
@article{Castleman.2020, 
year = {2020}, 
title = {{Can Text Message Nudges Improve Academic Outcomes in College? Evidence from a West Virginia Initiative}}, 
author = {Castleman, Benjamin L and Meyer, Katharine E}, 
journal = {The Review of Higher Education}, 
doi = {10.1353/rhe.2020.0015}, 
pages = {1125--1165}, 
number = {4}, 
volume = {43}
}
@article{Page.2020, 
year = {2020}, 
title = {{Conditions under which college students can  be responsive to nudging}}, 
author = {Page, Lindsay and Lee, Jeonghyun and Gehlbach, Hunter}, 
journal = {EdWorkingPaper}, 
doi = {10.26300/vjfs-kv29}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/EdWorkingPaper/Conditions%20under%20which%20college%20students%20can%20%20be%20responsive%20to%20nudging_2020_EdWorkingPaper_2.pdf}
}
@article{Barr.2021, 
year = {2021}, 
title = {{The Bottom Line on College Advising: Large  Increases in Degree Attainment}}, 
author = {Barr, Andrew C. and Castleman, Benjamin L.}, 
journal = {EdWorkingPaper}, 
doi = {10.26300/xdsa-5e22}, 
local-url = {file://localhost/Users/albertoguzman-alvarez/Documents/Papers%20Library/EdWorkingPaper/The%20Bottom%20Line%20on%20College%20Advising-%20Large%20%20Increases%20in%20Degree%20Attainment_2021_EdWorkingPaper.pdf}
}
@article{Pérez.2020, 
year = {2020}, 
title = {{Rediscovering the use of chatbots in education: A systematic literature review}}, 
author = {Pérez, José Quiroga and Daradoumis, Thanasis and Puig, Joan Manuel Marquès}, 
journal = {Computer Applications in Engineering Education}, 
issn = {1061-3773}, 
doi = {10.1002/cae.22326}, 
abstract = {{Chatbots have been around for years and have been used in many areas such as medicine or commerce. Our focus is on the development and current uses of chatbots in the field of education, where they can function as service assistants or as educational agents. In this research paper, we attempt to make a systematic review of the literature on educational chatbots that address various issues. From 485 sources, 80 studies on chatbots and their application in education were selected through a step‐by‐step procedure based on the guidelines of the PRISMA framework, using a set of predefined criteria. The results obtained demonstrate the existence of different types of educational chatbots currently in use that affect student learning or improve services in various areas. This paper also examines the type of technology used to unravel the learning outcome that can be obtained from each type of chatbots. Finally, our results identify instances where a chatbot can assist in learning under conditions similar to those of a human tutor, while exploring other possibilities and techniques for assessing the quality of chatbots. Our analysis details these findings and can provide a solid framework for research and development of chatbots for the educational field.}}, 
pages = {1549--1565}, 
number = {6}, 
volume = {28}
}
@article{Carrell.2017, 
year = {2017}, 
title = {{Why Do College-Going Interventions Work?}}, 
author = {Carrell, Scott and Sacerdote, Bruce}, 
journal = {American Economic Journal: Applied Economics}, 
issn = {1945-7782}, 
doi = {10.1257/app.20150530}, 
abstract = {{We present evidence from a series of field experiments in college coaching/mentoring. We find large impacts on college attendance and persistence, but only in the treatments where we use an intensive boots-on-the-ground approach to helping students. Our treatments that provide financial incentives or information alone do not appear to be effective. For women, assignment to our mentoring treatment yields a 15 percentage point increase in the college-going rate while treatment on the treated estimates are 30 percentage points (against a control complier mean rate of 43 percent). We find much smaller treatment effects for men, and the difference in treatment effects across genders is partially explained by the differential in self-reported labor market opportunities. We do not find evidence that the treatment effect derives from simple behavioral mistakes, student disorganization, or a lack of easily obtained information. Instead our mentoring program appears to substitute for the potentially expensive and often missing ingredient of skilled parental or teacher time and encouragement. (JEL I21, I23, I28)}}, 
pages = {124--151}, 
number = {3}, 
volume = {9}
}
@article{Zick.2022, 
year = {2022}, 
title = {{Money Well Spent? The Cost-Effectiveness of a Texting Intervention Targeting College Persistence}}, 
author = {Zick, Cathleen D. and Tippetts, Megan and Davis, Bobbi}, 
journal = {Journal of Research on Educational Effectiveness}, 
issn = {1934-5747}, 
doi = {10.1080/19345747.2021.1992813}, 
abstract = {{Past studies of texting interventions designed to promote college student persistence have given little attention to assessing both the fixed and variable costs of such initiatives. In addition, they typically have not assessed costs relative to the desired outcome(s). In this case study, we assessed the costs of implementing a text messaging program relative to persistence outcomes using a randomized field experiment in one college. The intervention’s cost per student was \$18.76 and the cost per text was \$0.85 per student. Mean cost-effectiveness ratios per student retained ranged from \$1,312 to \$750. Sensitivity analyses revealed increases in the number of students texted or gains in academic advisors’ efficiencies both contributed to sizable declines in the cost-effectiveness ratios. We conclude the texting intervention was cost-effective relative to other published interventions that have targeted college persistence.}}, 
pages = {394--412}, 
number = {2}, 
volume = {15}
}
@article{Castleman.2015p2c, 
year = {2016}, 
title = {{Freshman Year Financial Aid Nudges: An Experiment to Increase FAFSA Renewal and College Persistence}}, 
author = {Castleman, B L and Page, L C}, 
journal = {Journal of Human Resources}, 
issn = {0022-166X}, 
doi = {10.3368/jhr.51.2.0614-6458r}, 
abstract = {{In this paper we investigate, through a randomized controlled trial design, the impact of a personalized text messaging intervention designed to encourage college freshmen to refile their Free Application for Federal Student Aid (FAFSA) and maintain their financial aid for sophomore year. The intervention produced large and positive effects among freshmen at community colleges where text recipients were almost 14 percentage points more likely to remain continuously enrolled through the Spring of sophomore year. By contrast, the intervention did not improve sophomore year persistence among freshmen at four-year institutions among whom the rate of persistence was already high.}}, 
pages = {389--415}, 
number = {2}, 
volume = {51}
}
@article{Castleman.2015, 
year = {2015}, 
title = {{Summer nudging: Can personalized text messages and peer mentor outreach increase college going among low-income high school graduates?}}, 
author = {Castleman, Benjamin L. and Page, Lindsay C.}, 
journal = {Journal of Economic Behavior \& Organization}, 
issn = {0167-2681}, 
doi = {10.1016/j.jebo.2014.12.008}, 
abstract = {{ Several recent low-cost interventions demonstrate that simplifying information about college and financial aid and helping students access professional assistance can generate substantial improvements in students’ postsecondary outcomes. We build on this growing literature by investigating the impact of two applications of behavioral principles to mitigate summer “melt,” the phenomenon that college-intending high school graduates fail to matriculate in college anywhere in the year following high school. One intervention utilized an automated and personalized text messaging campaign to remind college-intending students of required pre-matriculation tasks and to connect them to counselor-based support. Another employed near-aged peer mentors to provide summer outreach and support. The interventions substantially increased college enrollment among students who had less academic-year access to quality college counseling or information. Both strategies are cost–effective approaches to increase college entry among populations traditionally underrepresented in higher education and, more broadly, highlight the potential for low-cost behavioral nudges and interventions to achieve meaningful improvements in students’ educational outcomes.}}, 
pages = {144--160}, 
volume = {115}
}
@article{Castleman.2014g0s, 
year = {2014}, 
title = {{A Trickle or a Torrent? Understanding the Extent of Summer “Melt” Among College‐Intending High School Graduates}}, 
author = {Castleman, Benjamin L. and Page, Lindsay C.}, 
journal = {Social Science Quarterly}, 
issn = {0038-4941}, 
doi = {10.1111/ssqu.12032}, 
abstract = {{The object of this study was to examine whether college‐intending, low‐income high school graduates are particularly susceptible to having their postsecondary education plans change, or even fall apart, during the summer after high school graduation. College access research has largely overlooked this time period. Yet, previous research indicates that a sizeable share of low‐income students who had paid college deposits reconsidered where, and even whether, to enroll in the months following graduation. We assess the extent to which this phenomenon—commonly referred to as “summer melt”—is broadly generalizable. We employ two data sources, a national survey and administrative data from a large metropolitan area, and regression analysis to estimate the prevalence of summer melt. Our analyses reveal summer melt rates of sizeable magnitude: ranging from 8 to 40 percent. Our results indicate that low‐income, college‐intending students experience high rates of summer attrition from the college pipeline. Given the goal of improving the flow of low‐income students to and through college, it is imperative to investigate how to effectively intervene and mitigate summer melt.}}, 
pages = {202--220}, 
number = {1}, 
volume = {95}
}
@book{murnane2010methods,
  title={Methods matter: Improving causal inference in educational and social science research},
  author={Murnane, Richard J and Willett, John B},
  year={2010},
  publisher={Oxford University Press}
}
@article{daniel2019big,
  title={Big Data and data science: A critical review of issues for educational research},
  author={Daniel, Ben Kei},
  journal={British Journal of Educational Technology},
  volume={50},
  number={1},
  pages={101--113},
  year={2019},
  publisher={Wiley Online Library}
}
@book{williamson2017big,
  title={Big data in education: The digital future of learning, policy and practice},
  author={Williamson, Ben},
  year={2017},
  publisher={Sage}
}
@article{d2020underspecification,
  title={Underspecification presents challenges for credibility in modern machine learning},
  author={D’Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D and others},
  journal={Journal of Machine Learning Research},
  year={2020}
}
@article{ho2007matching,
  title={Matching as nonparametric preprocessing for reducing model dependence in parametric causal inference},
  author={Ho, Daniel E and Imai, Kosuke and King, Gary and Stuart, Elizabeth A},
  journal={Political analysis},
  volume={15},
  number={3},
  pages={199--236},
  year={2007},
  publisher={Cambridge University Press}
}
@article{ho2007matching,
  title={Matching as nonparametric preprocessing for reducing model dependence in parametric causal inference},
  author={Ho, Daniel E and Imai, Kosuke and King, Gary and Stuart, Elizabeth A},
  journal={Political analysis},
  volume={15},
  number={3},
  pages={199--236},
  year={2007},
  publisher={Cambridge University Press}
}
@inproceedings{hu2015face,
  title={When face recognition meets with deep learning: an evaluation of convolutional neural networks for face recognition},
  author={Hu, Guosheng and Yang, Yongxin and Yi, Dong and Kittler, Josef and Christmas, William and Li, Stan Z and Hospedales, Timothy},
  booktitle={Proceedings of the IEEE international conference on computer vision workshops},
  pages={142--150},
  year={2015}
}
@article{hernandez2019systematic,
  title={A systematic review of deep learning approaches to educational data mining},
  author={Hern{\'a}ndez-Blanco, Antonio and Herrera-Flores, Boris and Tom{\'a}s, David and Navarro-Colorado, Borja},
  journal={Complexity},
  volume={2019},
  year={2019},
  publisher={Hindawi}
}
@article{schmidhuber2015deep,
  title={Deep learning in neural networks: An overview},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural networks},
  volume={61},
  pages={85--117},
  year={2015},
  publisher={Elsevier}
}
@article{holland1986statistics,
  title={Statistics and causal inference},
  author={Holland, Paul W},
  journal={Journal of the American statistical Association},
  volume={81},
  number={396},
  pages={945--960},
  year={1986},
  publisher={Taylor \& Francis}
}
@article{holland1986statistics,
  title={Statistics and causal inference},
  author={Holland, Paul W},
  journal={Journal of the American statistical Association},
  volume={81},
  number={396},
  pages={945--960},
  year={1986},
  publisher={Taylor \& Francis}
}
@article{rubin2005causal,
  title={Causal inference using potential outcomes: Design, modeling, decisions},
  author={Rubin, Donald B},
  journal={Journal of the American Statistical Association},
  volume={100},
  number={469},
  pages={322--331},
  year={2005},
  publisher={Taylor \& Francis}
}
@article{rubin1976inference,
  title={Inference and missing data},
  author={Rubin, Donald B},
  journal={Biometrika},
  volume={63},
  number={3},
  pages={581--592},
  year={1976},
  publisher={Oxford University Press}
}
@article{lehmann1993fisher,
  title={The Fisher, Neyman-Pearson theories of testing hypotheses: one theory or two?},
  author={Lehmann, Erich L},
  journal={Journal of the American statistical Association},
  volume={88},
  number={424},
  pages={1242--1249},
  year={1993},
  publisher={Taylor \& Francis}
}
@article{neyman1923application,
  title={On the application of probability theory to agricultural experiments. essay on principles. section 9.(tlanslated and edited by dm dabrowska and tp speed, statistical science (1990), 5, 465-480)},
  author={Neyman, Jerzy S},
  journal={Annals of Agricultural Sciences},
  volume={10},
  pages={1--51},
  year={1923}
}
@inproceedings{athey2015machine,
  title={Machine learning and causal inference for policy evaluation},
  author={Athey, Susan},
  booktitle={Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={5--6},
  year={2015}
}
@article{pearl2019seven,
  title={The seven tools of causal inference, with reflections on machine learning},
  author={Pearl, Judea},
  journal={Communications of the ACM},
  volume={62},
  number={3},
  pages={54--60},
  year={2019},
  publisher={ACM New York, NY, USA}
}
@article{grimmer2015we,
  title={We are all social scientists now: How big data, machine learning, and causal inference work together},
  author={Grimmer, Justin},
  journal={PS: Political Science \& Politics},
  volume={48},
  number={1},
  pages={80--83},
  year={2015},
  publisher={Cambridge University Press}
}
@inproceedings{cui2020causal,
  title={Causal inference meets machine learning},
  author={Cui, Peng and Shen, Zheyan and Li, Sheng and Yao, Liuyi and Li, Yaliang and Chu, Zhixuan and Gao, Jing},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3527--3528},
  year={2020}
}
@article{li1984classification,
  title={Classification and regression trees (CART)},
  author={Li, Bin and Friedman, J and Olshen, R and Stone, C},
  journal={Biometrics},
  volume={40},
  number={3},
  pages={358--361},
  year={1984}
}
@inproceedings{gelfand1989iterative,
  title={An iterative growing and pruning algorithm for classification tree design},
  author={Gelfand, Saul B and Ravishankar, CS and Delp, Edward J},
  booktitle={Conference Proceedings., IEEE International Conference on Systems, Man and Cybernetics},
  pages={818--823},
  year={1989},
  organization={IEEE}
}
@article{biau2016random,
  title={A random forest guided tour},
  author={Biau, G{\'e}rard and Scornet, Erwan},
  journal={Test},
  volume={25},
  number={2},
  pages={197--227},
  year={2016},
  publisher={Springer}
}
@book{james2013introduction,
  title={An introduction to statistical learning},
  author={James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  volume={112},
  year={2013},
  publisher={Springer}
}
@article{richardson2011eta,
  title={Eta squared and partial eta squared as measures of effect size in educational research},
  author={Richardson, John TE},
  journal={Educational research review},
  volume={6},
  number={2},
  pages={135--147},
  year={2011},
  publisher={Elsevier}
}