# Paper 2: When in-person support is not possible, can virtual outreach help? Evaluating the impact of an artificially intelligent conversational chatbot to promote college enrollment during the COVID-19 pandemic

## Introduction

In spring 2020, schools and colleges across the US began to shut down and pivot to remote learning to protect students and their communities from the rapid spread of the COVID-19 virus. At the same time, high school seniors who had applied to college before the pandemic began receiving acceptance notifications, even though the pandemic's impact on college enrollment was unknown. On the one hand, the COVID-19 crisis hampered business-as-usual operating procedures for colleges. As a result, students, particularly those planning to live on campus during college, may have been more likely to postpone their college enrollment. On the other hand, higher education is a counter-cyclical industry, such that college enrollment rates tend to be higher during economic downturns. For recent high school graduates, the impact of the pandemic on the retail and service sectors severely limited job opportunities. By April 2020, the unemployment rate in the United States reached 14.5% -- the highest rate recorded in recent history -- with the service and retail sectors experiencing the bulk of job losses. (US Bureau of Labor Statistics, 2021). As a result, college may have become more appealing to young adults who would otherwise enter the workforce after high school.

To address this uncertainty, the Common Application (Common App), a non-profit organization dedicated to assisting students with college applications, partnered with Mainstay, an EdTech company focused on using behavioral intelligence for college access, and the College Advising Corps (CAC), a non-profit that places recent college graduates as college counselors in underserved high schools. These partnered organizations acted quickly to provide students with proactive outreach and guidance on college-going tasks through an innovative, large-scale chatbot campaign. This outreach targeted around 174,000 US high school students who were the first in their families to attend college and came from low-income families. The majority (83%) of these students were also racially marginalized. Due to various systemic barriers, this subset of students were disproportionately likely to have had limited access to quality health care, living, and working conditions during the pandemic [@Oromaner.1986; @Ornelas.2004; @Soria.2020; @Hernandez.2022]. Additionally, this group of students were less likely to have ready access to personalized college-going guidance and potentially stood to benefit from additional support during their precarious college transition.

Over 38 weeks, an artificially intelligent (AI) chatbot named *Oli* sent students scripted messages on various college search, application, and enrollment-related topics. Staff developed the scripted messages at the Common App in collaboration with CAC. They included messages reminding students about upcoming deadlines, such as "Hey <STUDENT NAME>, #CollegeSigningDay is this Friday, May 1st. Have you decided what school you're going to attend?" To better target the information, Oli solicited information directly from students about the types of resources they needed and pressing questions they had. The chatbot "learned" how to interact with students by first having staff at the partnered organizations seed the chatbot with frequently asked questions and answers. As Oli interacted with students, it built an ever-increasing knowledge base for student questions that aided the chatbot in providing more targeted and individualized real-time responses. As the intervention rolled out, Common App and CAC staff could monitor interactions and correct and add to Oli's knowledge base. Student questions that Oli could not answer were forwarded to college advisers at the CAC, who would follow up directly with individual students.

In recent years, researchers have developed an interest in understanding how insights from behavioral economics can inform strategies to support students to and through college [@Oreopoulos.2020]. Several of these studies have involved designing and testing proactive communication strategies using text messaging as the primary mode of communication [@Page.2016x1k]. Recent studies have investigated text messaging communication employing artificially intelligent chatbot capability [@Nurshatayeva.2021; @Page.2017jj; @Page.2020]. These studies have demonstrated that chatbots can be effective in assisting students to complete pre-matriculation tasks and enroll in college [\@Nurshatayeva.2021; \@Page.2017jj; \@Page.2020]. However, there are numerous open questions regarding the scalability of these chatbot interventions. Text nudge interventions have mixed results when implemented at scale, with many studies finding minimal to no effects on college-related outcomes [@Bird.2021; @Oreopoulos.2020].

Therefore, my second dissertation study aims to contribute to the literature on the scalability of chatbot interventions by evaluating the large-scale chatbot campaign implemented by the Common App, College Advising Corps, and Mainstay aimed at low-income, first-generation students. Specifically, I investigate whether the college applications and enrollment rates were higher among students targeted for this chatbot outreach compared to their observationally similar peers who were not targeted. In addition, I examine treatment heterogeneity based on students' application behavior, racial/ethnic identity, and the level of chatbot engagement.

The remainder of the section is structured as follows: First, I review the relevant literature for this study. Then, I describe the data and my analytic approach. Finally, I conclude with a discussion of findings.

## Literature Review

In this section, I review the literature on college access in general as well as during the COVID-19 pandemic. In addition, I draw on literature examining how behavioral text message nudge interventions and chatbots can assist high school students in overcoming obstacles to college access.

Before reviewing the literature, I first define how I conceptualize first-generation students -- students whose parents do not have any postsecondary education experience -- and the systemic barriers they face in college access. I take an anti-deficit approach to my framing of first-generation students -- many of whom also tend to be economically and racially marginalized [@Harper.2010; @Hernandez.2022]. Anti-deficit practices explore how marginalized students persist in the face of social and structural barriers to reframe students' experiences from ones of struggling to ones of resiliency [@Harper.2010]. Since most deficit approaches focus on what students lack, anti-deficit strategies focus on what students bring that leads to their resilience and persistence. Although I take an anti-deficit stance in my review of the literature, I recognize that the majority of the existing literature on college access, especially that focuses on economically and racially marginalized students, is written from a deficit perspective [@McLewis.2021].

It is important to acknowledge the structural and social barriers that lead to disparate postsecondary outcomes for first-generation students. For example, economically and racially marginalized individuals face structural barriers to accessing quality education, housing, and employment conditions . Decades of federal redlining in housing and the policy of using local taxes to fund schools, for instance, produce racial and socioeconomic disparities in educational opportunities and family wealth [\@Oromaner.1986; \@Ornelas.2004; \@Soria.2020; \@Hernandez.2022]. In addition, racially marginalized students face social and systemic barriers and federal policies that have historically limited their access to high-quality primary and secondary education [@McLewis.2021; @Ornelas.2004; @Hernandez.2022]. For example, policies that funnel racially marginalized students into underserved schools with limited access to college prep courses, and school tracking policies that exclude racially marginalized students from gaining access to the college prep tracks that would qualify them to apply to college [@Oakes.2007; @Ornelas.2004; @Oromaner.1986]. As a result, disparities in college access and completion based on parental education, as well as social and structural barriers, are significant contributors to the rising income inequality in the US.

#### College access and returns to a college education

Despite increased access to higher education over the past several decades, disparities remain in college attainment by socioeconomic status and parental education [@Dynarski.2022; @Dynarski.2022dch]. Students whose parents have a college degree are more likely to attend college than those whose parents did not complete high school @Avery.2021; @Dynarski.2022dch. Moreover, students from higher-income backgrounds are more likely to enroll and complete postsecondary education than students from lower-resourced families [@Ornelas.2004; @Garriott.2020]. By age 24, roughly 13% of students from the lowest family income quartile earn a bachelor's degree, compared to 64% of students from the highest quartile [@1956].

Despite the rising cost of higher education, the non-pecuniary and long-term economic returns to a college education remain positive [@Dynarski.2022dch]. Adults with a bachelor's degree typically earn and accumulate more wealth than those without a degree [@Dynarski.2022dch]. Higher education is associated with lower unemployment rates, greater civic engagement, and better health outcomes [@Dynarski.2022dch; @Dynarski.2022; @Avery.2021]. Nevertheless, the rate of economic return is higher for continuing education students -- students whose parents had some postsecondary education experience -- than for first-generation students (Fry, 2021) . Given that more than half of college students identify as first-generation (56%), substantial policy efforts have focused on helping first-generation college-bound students enroll in college [@Carrell.2017; @Page.2016x1k; @Herbaut.2019]. However, the COVID-19 pandemic introduced unprecedented complexity to these efforts and substantially hindered college access for first-generation students [@Hoover.2020].

#### College access during the COVID-19 pandemic

The COVID-19 pandemic added uncertainty to the lives of high school seniors planning to attend college in the fall of 2020. In the early stages of the pandemic, one in every six recent high school graduates reconsidered enrolling in college, and nearly two-thirds expressed reservations about attending their first choice institution [@Howell.2021]. The most frequently cited reason for not enrolling at their first-choice institution was concern about their family's ability to pay for college [@Howell.2021].

Now, nearly three years into the pandemic, the adverse effects of the pandemic on college enrollment have become clear. Enrollment for first-time undergraduates fell to historic lows in fall 2020, falling nearly 10 percentage points from fall 2019, with lower immediate college enrollment rates for students from low-income schools compared to high-income schools, where low-income schools are defined as schools where at least 50 percent of the students are eligible for free or reduced-price lunch [@Clearinghouse.2021]. Furthermore, students from low-minority schools were more likely to enroll in college right away than those from majority-minority high schools, defined as schools where at least 40 percent of students are Black or Latinx [@Clearinghouse.2021].

A recent report from the College Board provides additional information about the detrimental effects of the pandemic on college enrollment. Using data from nearly 10 million US students, the College Board calculated high-level descriptives and regression-adjusted models to determine the proportion of fall 2020 enrollment rates attributable to COVID-19 [@Howell.2021]. As a result of the pandemic, enrollment rates at community colleges decreased by roughly 12 percentage points. Most concerning is the decline in enrollment rates among first-generation, low-income students [@Howell.2021].

The decline in college enrollment rates for first-generation students is likely a result of the pandemic's disproportionate impact on racially and economically marginalized communities. During the pandemic, these communities experienced reduced access to high-quality healthcare and working conditions and lowered expected wages [@Hoover.2020; @Lee.2021; @Molock.2022]. Prior to the pandemic, the average Black and Latino household had a net worth of approximately \$17,000, while the average White household held \$171,000 (Center for American Progress, 2021). As a result, Black and Latino households may have had a more challenging time adjusting to the economic shock caused by the pandemic. These households, on average, have fewer liquid assets and wealth to cover unexpected expenses and wage loss (Center for American Progress, 2021). This may have resulted in first-generation students -- notably racially marginalized students -- prioritizing immediate health and economic stresses, such as supporting their families financially or dealing with lost wages, over college enrollment. Indeed, students from higher-income households were far more likely to enroll in college immediately after graduation (65%) than those from low-income high schools (49%) [@Clearinghouse.2021]. Furthermore, first-generation students were nearly twice as likely to be concerned about paying for education expenses in fall 2020 than continuing education students [@Soria.2020]. In sum, there has been an unprecedented change in college enrollment caused by the COVID-19 pandemic, particularly for first-generation, economically, and racially marginalized students. Next, I review the literature on well-documented challenges to college access.

#### Challenges in college access

First-generation students face many barriers during the college application, financing, and enrollment processes [@Page.2022; @Page.2016x1k; @Page.2019; @Avery.2021]. To apply for college, students must complete several complex tasks, including creating a list of colleges, deciding which colleges to apply to, and applying for financial aid via a time-consuming financial aid application (the Free Application for Federal Student Aid - FAFSA). Students typically obtain federal, state, and grant aid -- aid that does not have to be repaid -- by completing the FAFSA form, which contains over 100 questions about the students' and their families' financial assets. Several behavioral nudge interventions have demonstrated that proactive outreach to students in completing the FAFSA increases college enrollment [@Page.2022; @Page.2016x1k; @Page.2019; @Castleman.2014g0s]. Assisting students with FAFSA submission is only half the battle. The FAFSAs of a subset of first-generation and low-income students are selected for additional scrutiny through a federal auditing system, leading to lower college enrollment rates among low-income, first-generation students [@Guzman-Alvarez.2021]. Additional obstacles remain once a student accepts a college admission offer, notably during the summer before college.

The summer before college enrollment is fraught with additional obstacles to college enrollment [@Castleman.2012; @Castleman.2015; @Resendez.2022; @Castleman.2014], given that students must complete a number of pre-matriculation administrative tasks before stepping foot on campus, such as submitting high school transcripts, determining which and how much financial aid to accept, taking placement exams, and successfully enrolling in courses [@Page.2020; @Page.2022; @Page.2016x1k; @Avery.2021; @Castleman.2014; @Castleman.2012; @Castleman.2020; @Castleman.2014g0s; @Castleman.2015]. If students fail to complete these steps, they may ultimately fail to enroll in college, a process termed "summer melt" [@Castleman.2014g0s; @Castleman.2014; @Castleman.2015]. Summer melt affects 10 to 20 percent of all college-bound high school students, with higher rates for first-generation, racial, and economically marginalized students. Students who, due to an array of systemic barriers, have reduced access to information and support to help them navigate these administrative tasks [@Cataldi.2018; @Ives.2020].

#### Unique challenges in college access among first-generation students

For first-generation students with limited family or school-based resources that can facilitate the college application process, navigating these pre-matriculation tasks adds an additional layer of complexity. Moreover, these students must navigate the transition to college while balancing work and family obligations [@Resendez.2022; @Barber.2020; @Hernandez.2022]. First-generation students are more likely to attend schools with fewer college prep or Advanced Placement (AP) courses, which have been linked to higher college enrollment [@Ornelas.2004]. Additionally, first-generation students are more likely to attend schools with limited or no access to college counselors, who could assist students with completing college applications and navigating the complex financial aid process [@Page.2019; @Naughton.2021; @Castleman.2014g0s]. Nevertheless, even when students *do* have access to a college counselor, these counselors often manage high caseloads and cannot provide individualized support to students who would benefit the most [@Naughton.2021]. Therefore, many scholars have focused on developing interventions focused on assisting these students with pre-matriculation tasks by providing access to individualized advising from a professional college counselor [@Castleman.2012; @Castleman.2014g0s; @Castleman.2014; @Castleman.2015].

#### Importance of college advising

A large body of work has been conducted to assess the efficacy of behavioral interventions designed to assist students in meeting the key steps and deadlines required to enroll in college. One favored approach is to deliver targeted information about the college-going process to students through "low-touch" efforts such as proactive text-based outreach or "high-touch" face-to-face college advising [@Oreopoulos.2020]. Access to college advising can positively affect college enrollment and persistence [@Arnold.2015]. Castleman et al. (2012) provide one of the earliest pieces of evidence of the positive impact of providing proactive outreach and support around key college-going tasks to low-income students during the summer prior to college. Using a randomized trial, Castleman et al. (2012) demonstrate that access to "college-transition" counselors during the summer before college increases students' immediate college enrollment by 14 percentage points. These counselors supported students in reviewing financial aid packages, addressing gaps in the aid they were offered, and completing required pre-matriculation paperwork. One limitation of this study was that only "choice" schools were included in their sample. Therefore, questions remained unanswered as to whether this support was generalizable to public schools.

A follow-up study by Castleman, Page and Schooley (2014) provided evidence of the generalizability of counseling support in public schools. The researchers conducted two college counseling interventions with large urban public school districts that were more nationally representative in Boston and Fulton County, Georgia. Like Castleman et al. (2012) the study provided high school seniors with college counseling support in the summer before college. For example, counselors helped students develop a list of personalized tasks they needed to complete to enroll in college in the fall. Counselors communicated with students using various modalities, including in-person, text, phone, and email. Students who received proactive college counseling in the summer before college increased their college enrollment by 8 to 12 percentage points among low-income students.

According to Naughton (2021), the pandemic exacerbated pre-existing "cracks" in the ability to provide students with advising services, transforming them into "craters." This is primarily due to high schools shifting from in-person to virtual advising. College advisors felt ineffective in providing virtual support and guidance once high schools transitioned to virtual advising. Advisors' ability to contact students was severely limited, leaving students without support to help them transition to college [@Naughton.2021]. Naughton explains that advisers in high schools with higher college enrollment rates were significantly less affected than those in high schools with lower college enrollment rates, i.e., the students who required the most assistance. Therefore, strategies that bring virtual proactive outreach and support around critical college-going tasks directly to students who require it most have the potential to improve college-going outcomes for students in these settings.

#### Behavioral nudge interventions

Behavioral nudge interventions have proven to be a low-cost means of providing college students with counseling support [@Oreopoulos.2020; @Page.2016x1k]. These interventions condense complex information and deadlines into brief, to-the-point messages that are delivered to students via a familiar mode of communication -- text messages [@Oreopoulos.2020]. In the context of college access, these studies have focused on preventing "summer melt" by assisting students with well-defined but complex tasks such as submitting financial aid applications, submitting high school transcripts, and registering for classes [@Castleman.2012; @Castleman.2014g0s; @Castleman.2014; @Castleman.2015].

Castleman and Page have conducted several randomized controlled studies utilizing informational text nudges to remedy summer melt. For instance, Castleman and Page (2015) use a personalized text message campaign that reminds students about key college-related tasks required to enroll in their receiving college institution. In addition to reminders, students were invited to request follow-up support from a counselor via text message. The text message campaign increased college enrollment by approximately 7 percentage points among students with limited access to college access supports. The low-cost nature of these interventions has increased interest in their use over the past decade [@Oreopoulos.2020]. Recent studies have investigated the scalability of text nudge interventions, with findings that have been much more mixed. A recent study by Bird et al. (2021) suggests that nudges may fail when scaled up. Bird and colleagues investigated student nudges using multiple modalities, such as email and text messages, to encourage students to complete the FAFSA and enroll in college. This study is unique because it targeted nearly 800,000 students across the US who used the Common Application or the Texas admission system. No form of outreach (email or text message) increased financial aid receipt, college enrollment, or persistence. One possible explanation for this finding is that scaling nudge campaigns can be difficult when implemented by a large organization, such as the Common App, because students may lack a personal connection to a large organization, as opposed to their local high school. In order to deliver nudges to a large number of students, the authors suggest that the messaging may be too "generic and one-way" and lack personalization, resulting in students not taking actionable steps to enroll in college.

In a recent working paper, Page et al. (2022) hypothesize what factors may contribute to the (in)effectiveness of informational text-based nudges in the college access space. One of their primary arguments is that students may be more receptive to text-based nudging when the messaging focuses on tasks and processes that are time-sensitive and have real consequences if not completed, such as resolving a registration hold. In addition, they provide more nuance to the claim made by Bird et al. (2021) that student messaging needs to be personalized to be salient to students. The ostensible sender of the messaging matters. Students may be more receptive to outreach and communication delivered by organizations or individuals that students know and trust (Debnam, 2017; Page et al., 2020). Recent studies have utilized technological advancements to help bridge this personalization gap.

#### Chatbots in college access

Recent efforts in the literature on behavioral nudges have focused on utilizing artificial intelligence (AI) chatbots to provide personalized assistance to students navigating college applications and enrollment. These conversational chatbots can provide students with personalized information about deadlines and answers to specific questions about the college application process. These bots are trained by college staff to develop a "database" of answers to frequently asked questions regarding college attendance. Over time, the chatbot "learns" how to respond to a broader range of student questions. Once trained and deployed successfully, the chatbot can assist students with answering questions about vital college-going tasks and requirements.

To date, three empirical studies have demonstrated the effectiveness of chatbots on college access [@Nurshatayeva.2021; @Page.2017jj; @Page.2020]. The first of these studies was conducted by Page and Gelbach (2017) at Georgia State University (GSU). Page and Gelbach collaborated with GSU and an ed-tech startup (Mainstay; formerly AdmitHub) to develop "Pounce," a conversational chatbot. Pounce assisted students accepted to GSU with navigating pre-matriculation tasks. The chatbot outreach led to a 3.3 percentage point increase in timely enrollment at GSU and helped students with pre-matriculation tasks like signing up for orientation.

A follow-up study at GSU (Page et al., 2022) shifted the focus of the chatbot outreach from helping students enroll at GSU to supporting them once they arrived at GSU by assisting them in completing tasks such as resolving registration holds and seeking campus-based resources. A novel aspect of this randomized study was its focus on the main campus of GSU-Atlanta, a four-year university, and the Perimeter campus of GSU, a two-year college. The chatbot effectively changed student behavior regardless of institution type when outreach addressed serious and time-sensitive administrative processes, such as resolving registration holds. In contrast, outreach was ineffective when chatbots assisted students with "non-urgent" tasks, such as gaining access to supplemental, academic, social, and career-related support. This study demonstrates that chatbot interventions work best when they target discrete, time-sensitive, and well-defined tasks. These findings are consistent with previous text-nudge interventions in which students were less responsive to nudges related to less time-sensitive tasks such as future job prospects [@Oreopoulos.2021]. It is important to note that the previously mentioned studies occurred within the GSU system. Consequently, it was unknown whether the positive effects of chatbot interventions could be replicated in institutions outside the GSU system.

Nushatayeva et al. (2021) argue that the effectiveness of chatbot interventions are context-dependent. Nushatayeva and colleagues replicate the work of Page and Gelbach (2017) using "PeeDee," a chatbot at East Carolina University (ECU). This study provides additional evidence of treatment heterogeneity by determining for which students and under what circumstances the intervention was most effective. Researchers were primarily interested in the use of PeeDee to improve students' completion of pre-matriculation tasks and their eventual enrollment at ECU. They find no overall effect of the chatbot on college enrollment but did find an impact on loan acceptance. Students in the treatment group had an 8 percentage point increase in accepting a student loan compared to the control group. Nushatayeva et al. attribute the null overall results on college enrollment to ECU's comparatively more affluent student body. The authors did find that the chatbot increased enrollment at ECU and course enrollment by 3 percentage points, specifically for first-generation students. This positive effect is likely a result of first-generation students receiving crucial information regarding college enrollment tasks that they cannot obtain through familial support.

Collectively, these studies demonstrate that chatbots can help to improve student success with the transition to college. Nevertheless, chatbot-based nudges are not effective in all contexts. These interventions appear most effective when chatbots assist students with time-sensitive, actionable tasks, such as clearing registration holds, rather than "non-urgent" tasks, such as searching for on-campus resources. In addition, the sender of the messages matter. Students appear most receptive to communication and messaging from a source they know and trust. Text-based nudge interventions in which the ostensive messenger is well-known to the student or affiliated with a receiving institution -- such as Pounce at GSU -- have relatively low opt-out rates (5 percent). In contrast, when the messaging comes from a less well-known messenger, the opt-out rates are significantly higher, as high as 25 percent. These findings are consistent with the results of the more mature literature on general nudges in college access -- context matters. Yet, little is known about the scalability of these interventions, such as at the national level, where hundreds of thousands of students are targeted.

Therefore, my second dissertation study contributes to the literature on the scalability of chatbot interventions by evaluating the large-scale chatbot campaign implemented by the Common App, College Advising Corps, and Mainstay aimed at low-income, first-generation students.

## Data

We leveraged several different data sources to evaluate the effectiveness of the outreach campaign. First, we utilized administrative records from the Common App, which provided a rich data source of any information a student provided on their Common App application, including student demographics, high school academic achievement, and college entrance exams. Second, Mainstay provided access to the data from the chatbot platform utilized to provide outreach to students. This platform captured rich data on the content and timing of the chatbot communication among students, the chatbot, and CAC advisers. Using the Mainstay data, we generated variables that captured student opt-out requests and engagement throughout the intervention. Finally, we matched students in our analytic file to data from the National Student Clearinghouse (NSC) to observe if and where students enrolled in college in fall 2020 and persistence in subsequent terms.

In order to evaluate the effectiveness of the outreach campaign, our analysis relied on matching students selected to receive outreach to similar students in the same high school who were not selected for outreach. Common App and partners selected for outreach all students from the high school class of 2020 who had created a Common App account and met two specific criteria: 1) they would be first-generation college students, and 2) they had a low family income, as indicated by qualifying for a Common App fee waiver. In particular, some Common App students met one but not both criteria for inclusion in the outreach (i.e., first-generation college student *or* qualified for a fee-waiver). We use these students as our key source of comparison in the analyses. Therefore, our outreach ("treatment") group includes students who met both inclusion criteria (i.e., first-generation college student *and* qualified for a fee-waiver) while our control students met one but not *both* criteria.

From the Common App we received student-level data for the entire cohort of students who had created a Common App account and had intended to apply to college -- hoping to enroll in fall 2020 -- including students who were selected for the outreach $(n = 173,776)$ and those who were not $(n = 1,229,232)$, for a total of nearly 1.5 million student records. Once we cleaned the data, we reduced our analytic sample to include students who lived in the US, had a valid cell phone number, and attended a high school where at least one student was targeted for outreach. Our final analytic sample included nearly half a million students $(N = 406,236)$, including 142,837 students who were targeted for outreach and 263,399 who were not treated and therefore qualified as potential control students.

## Analytic Strategy

### Propensity Score Matching

It is important to note that the outreach team did not randomly assign the outreach for this campaign. Randomly assigning the outreach would ensure that students who received the outreach and those who did not would be balanced on both observable *and* unobservable characteristics. Given the lack of randomization, a simple comparison of outcomes between students who received the outreach and those who did not may -- incorrectly -- conclude that the outreach affected the outcome. To guard against this to the fullest extent possible, we matched students in the outreach group to demographically similar students in the same high school who were not treated. These matched students are then observationally similar to students in the outreach group and, therefore, a reasonable comparison group. Analytically, we do this via propensity score matching.

Our matching approach relies on estimating a propensity score, which is the conditional probability that a student would have been exposed to treatment, conditional on a set of observable characteristics. We then use this propensity score to match treated students to control students with similar estimated propensity scores, resulting in a control group that is demographically similar to the outreach group. Propensity score matching allows us to more accurately evaluate the impact of the outreach than a simple comparison of outcomes between those students who received the outreach and those who did not.

Specifically, we estimate a propensity score model of the following general form @eq-int-ps:

$$
Outreach_{is} = \beta_{0} + \beta_{1}X + \lambda_{is}
$$ {#eq-int-ps}

Where $Outreach$ represents a binary indicator coded as 1 for students who received the outreach and 0 if a student did not. $X$ is a vector of student-level characteristics. These include age, gender, English spoken at home, dependent flag, number of high schools attended, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator, sibling college indicator, submitted at least one Common App application before start of outreach, and race/ethnicity indicators.

To increase balance on our covariates, we placed additional restrictions on our matching algorithm to exact match on specific variables $(\lambda)$. $\lambda$ represents a vector of covariates on which we exact matched students. These include students' high school, underrepresented minority indicator, an indicator for submitting at least one Common App application before the start of outreach, and missing indicators for high school GPA and SAT/ACT performance. Exact matching on these variables allows us to match outreach students to control students who *exactly* match them on the predefined set of variables. For example, exact matching on high school forces the matching algorithm to only find potential matches for treated students within the same high school. This allows us to control for the fact that students across different high schools have varied school experiences; therefore, matching within schools provides more reasonable matched control students.

Given our nearest-neighbor matching approach, one control student can be a perfect match for more than one treatment student. Therefore, we allowed the matching algorithm to match an outreach student to multiple control students (i.e., matching with replacement). Additionally, we restrict matches to within .50 standard deviations of a propensity score. This allows us to guard against having treatment students matched with control students with a large difference in propensity scores [@Guo.2009].

To examine how well our matching procedure balanced student characteristics, we look at the overall distribution of the propensity score among treatment and untreated students and covariate balance before and after matching. @fig-psdist presents the propensity score distribution between treatment and control students before and after matching. Overall, our matching procedure did an excellent job of matching treated students to control students, as indicated by the overlap in the distributions on the right panel in @fig-psdist, indicating that our matched sample has propensity scores that fall within the region of common support [@Bai.2011].

In addition to analyzing the distributional differences in the propensity score, we also looked at baseline characteristics between treatment and control before and after matching. @tbl-balance compares balance of student characteristics before and after matching. Overall, control group students (who are either "low-income" or "first generation" but not both) had higher GPA class rank (70.1% vs. 64.3%) and SAT scores (794.9 vs. 671.3) than treatment group students (who are both "low-income" and "first generation"). The treatment group also included a substantially larger percentage of minority students (28% Black and 41% Latinx) than the control group (19% Black and 23% Latinx). We achieve excellent balance on all covariates included in our matching approach, with no standard mean difference below 0.1 [@Rosenbaum.2022]. Our final matched sample includes a treatment group consisting of 99,593 students and a matched control of 61,553.

Finally, we modify our primary propensity score model to explore variation in outreach impact on college enrollment by students' application behavior, racial/ethnic subgroups, and level of engagement with the chatbot. Where this is the case, we rerun our matching algorithm within our subgroup of interest.

### Outcome Model

We ran a series of regression models to estimate the relationship between outreach and student outcomes. Specifically, we regressed each outcome on an outreach indicator and student-level characteristics, including matching weights.

Our regressions took the following general form @eq-out:

$$
Y_{is} = \beta_{0} + \beta_{1}Outreach_{is} + X\theta + \lambda_s + \epsilon_{is}
$$ {#eq-out}

Where $Y_{is}$ represents our outcome of interest for student $i$ in school $s$. $Outreach$ is a binary indicator coded as 1 for students who received the outreach and 0 if a student did not. $X$ is a vector of student-level characteristics such as age and gender. In addition, we included high school fixed effects $(\lambda)$, which soak up any remaining variation in the outcome due to differences across high schools. $\beta_{1}$ is our coefficient of interest and represents the mean controlled difference in the outcome between those who received the outreach and those who did not. We clustered our robust standard errors at the high school level.

Although we took a robust matching approach in constructing our control group, there could still be differences between our outreach and control group on student-level characteristics. Therefore, by including the same vector of student-level covariates $(X)$ as was included in @eq-int-ps, in our regressions, we can account for any remaining imbalance between student characteristics that we observe and increase the precision of our impact estimates. This approach is commonly referred to as a "doubly robust" approach [@Rosenbaum.2010].

In order to account for our nearest-neighbor matching approach with replacement, we include matching weights ($w$) in our regression models. Where within each matched group -- which includes at least one treated unit and at least one control unit -- each treated unit gets a \$w=1\$, and each control unit is given a preliminary weight of $w=n_{ti}/n_{ci}$ , where $n_{ti}$ is the number of treated units in matched group $i$ and $n_{ci}$ is the number of control units in matched group $i$. Then each control unit's weight is added up across the groups in which it was matched and scaled to sum to the number of uniquely matched control units.

## Limitations

The main limitation in our analysis is our ability to attribute a causal relationship between the outreach and the effects we estimate. Given that students were not randomly assigned to the outreach, we cannot make causal claims about the effectiveness of the outreach. However, our robust matching approach allows us to reduce some -- but not all -- the bias that could exist in explaining the relationship between receiving outreach and outcomes. Note that we are only matching on *observable* characteristics. There could be *unobservable* characteristics that the matching procedure cannot consider that could influence outcomes absent the intervention.

Furthermore, important observable differences do remain between the outreach group and the comparison group, even after matching. Students selected for outreach were first-generation *and* low-income, while control students were either first-generation *or* low-income. The outreach team selected students in this manner to reach as many students as possible, specifically students who would likely benefit from this sort of outreach. Although this was a wise approach for targeting students in need of support, it created some analytic obstacles for our impact analysis that cannot be remedied through matching. In particular, in any matched pair of low-income students, the treatment group student is first-generation and the control group student is not, and similarly, in any matched pair of first-generation students, the treatment group student is low-income and the control group student is not.

Students in the outreach group have two factors that -- due to an array of systemic issues -- may disadvantage them in terms of college enrollment. Therefore, we may expect that -- even absent the intervention -- students in the outreach group likely have lower college enrollment rates than those who did not receive the outreach. Given how the outreach groups were defined, we cannot observe college enrollment outcomes for control students who were first-generation *and* low-income, which would be the natural comparison for our outreach group. This is because, for the class of 2020, the intervention was targeted to *all* students who met these two criteria. After presenting our findings, we discuss two different strategies we used to investigate what the differences between these two groups might have been absent the chatbot outreach. Overall, we find that students who met both factors had somewhat lower enrollment rates than those who exhibit only one of the two factors.

## Results

We primarily focus on college application submission and enrollment in the fall term after students graduate high school as our primary outcome of interest. Additionally, we analyzed whether effects varied according to student characteristics and engagement in chatbot communication. We specifically look at impacts for racially marginalized students, students who opted out of the chatbot outreach, and students who had a high level of engagement with the chatbot.

### College Application Submission

@tbl-submit shows the impact of the outreach on submitting at least one college application via the Common App. Students targeted for outreach had somewhat lower application submission rates than their matched controls. 86.1% of the control group submitted at least one college application, while 84.7% of outreach students did so. This differential is equal to around -1.5 percentage points and is statistically significant.

Next, we analyzed whether impacts varied based on whether a student submitted at least one college application before or after the start of the intervention. The outreach had no impact on submitting at least one application prior to the start of the intervention, as expected. However, outreach students were less likely to submit an application after the start of the intervention (-1.3 percentage point difference).

### Overall College Enrollment

Next, we present the effects on college enrollment outcomes (@tbl-overenroll). Students in the outreach group had somewhat lower fall 2020 enrollment rates compared to the control group. 76.7% of students in the outreach group enrolled in college in fall 2020, compared to 78.7% of control students, a 2 percentage point differential. We observed effects of a similar magnitude across all other terms.

Additionally, we explored if the outreach impacted whether a student took a gap semester or year. The outreach did not seem to impact whether students took a gap semester. However, we observed a slight increase in outreach students taking a gap year. Oli and the CAC advisors provided no information or advice about taking time off before enrolling in college, so it seems unlikely that the outreach influenced these choices given the underlying uncontrolled differences between treatment and control groups.

We further unpacked enrollment impacts by subsetting our sample to only students who had submitted no college applications prior to the start of the intervention. Given the timing of the outreach in late spring, these students missed most of the college application deadlines and therefore could benefit from this kind of outreach. However, the outreach had no impact across all enrollment outcomes (@tbl-overenroll_submit).

### Fall 2020 College Enrollment

The Common App and its partners were primarily interested in immediate college enrollment after students completed high school. Therefore, we analyzed the impact of the outreach on fall 2020 enrollment by 4-year versus 2-year institutions, private versus public, and full-time enrollment (@tbl-20enroll). The somewhat lower enrollment we observed for outreach students compared to students who did not receive the outreach was likely driven by students forgoing enrolling in 4-year institutions -- specifically 4-year privates (-3.3 percentage point difference) -- and enrolling in 2-year institutions (1.9 percentage point difference).The CAC advisors sometimes suggested two-year colleges to students who expressed concerns about college affordability but did not otherwise take any position on choosing between a two-year and a four-year college.

Additionally, outreach students had lower full-time enrollment rates (63.9%) than control students (66.8%).

### Racially Marginalized Students

Next, we examined whether the outreach had a differential impact on students who belonged to a racially marginalized group. We defined a student as racially marginalized if a student self-identified as non-white or bi/multi-racial.

In @tbl-20enrollurm we present the impact of the outreach on fall 2020 college enrollment outcomes. We observed impacts of a similar scale as the entire sample. Racially marginalized students in the outreach group had relatively lower fall 2020 enrollment. There was a modest negative difference in enrollment in fall 2020 of around 2.1 percentage points, specifically driven by outreach students not enrolling at 4-year institutions by around -4.4 percentage points, but instead enrolling at 2-year institutions (2.2 percentage point difference).

### Outreach Participation

Using data from Mainstay regarding student engagement with the bot, and whether students decided to opt-out of receiving outreach from Oli, we constructed high engagement and opt out measures. Specifically, we defined high engagers as a student in the top 25th percentile of the total number of text messages sent throughout the outreach. We created the opt-out indicator by flagging students who explicitly messaged "STOP" to Oli at any point during the outreach or received a "goodbye" message from Oli, indicating that the student had requested to opt-out .[^03-commonapp_over-1]

[^03-commonapp_over-1]: Due to data quality issues, we received text message data for 70% of the outreach group. Furthermore, engagement across the intervention was relatively low. On average, students sent around 8 messages throughout the 38 weeks of the outreach.

### Opt-Out

Throughout the outreach campaign, students had the option to opt-out of receiving outreach by directly messaging Oli. Around 16% of students requested to opt out. In @tbl-20enrollopt we examine to what extent impacts varied among students who opted out. Across all outcomes, we see no effect on college enrollment. This finding is not surprising, given that most of the students who opted out did so in the first few weeks of the intervention and therefore received little to no communication from the chatbot.

### Oli Engagement

A final question we explored was whether impacts varied for those who engaged highly with Oli, i.e., those in the top 25th percentile of total messages sent throughout the outreach. Due to the low engagement throughout the intervention, a student who sent more than 9 messages was flagged as a "high" engager, which equaled around 17% of all outreach students.

@tbl-20enrolleng presents results for students with high engagement throughout the outreach. As opposed to our overall modest negative impacts for the whole sample, here we found a slightly positive impact on fall 2020 enrollment for highly engaged students, around a 3 percentage point improvement over the matched controls. Students who engaged with Oli at a high rate were, on average, more likely female, came from a racially marginalized group, and had slightly lower SAT/ACT performance than students who didn't have high engagement with the chatbot. Furthermore, these students also had higher rates of college application submission than non-highly engaged students.

### Results in Context

We want to underscore that important, observable differences remain between the outreach group and the comparison group -- even after matching. As mentioned previously, the students selected for outreach were first-generation *and* low-income, while the control students were first-generation *or* low-income. This presented analytic obstacles in estimating outreach impacts.

In this section, we outline two different strategies we took to investigate what differences between these two groups might have been absent the chatbot outreach.

### High School Longitudinal Study of 2009

First, we used the High School Longitudinal Study of 2009 (HSLS-09), a nationally representative sample of 9th graders in 2009 who were observed through 2016. The HSLS-09 dataset includes information on college-going. We subsetted the HSLS-09 data to a sample of students who had indicated college interest in 9th grade and had submitted at least one college application. This allowed us to mimic -- although imperfectly -- our 2020 Common App cohort of students who were college intending.

Next, we calculated enrollment differentials for first-generation *and* low-income students and first-generation *or* low-income students. In our 2020 cohort, low-income was defined as qualifying for a Common App fee waiver. Given the information available in the HSLS-09, we defined low-income as a student's family income falling below 130% of the poverty line (i.e., qualified for free or reduced lunch).

Overall, we found that students who were first-generation and low-income had lower college enrollment rates than students with only one of those characteristics. Students who were first-generation *and* low-income had a college enrollment rate of 79.1%. In comparison, students who exhibited only one of these factors had an enrollment rate of around 84.2% (a -5.1 percentage point difference).

Drawbacks of relying on the HSLS-09 as a source for informing this differential include its age (it observes a cohort of students who completed high school several years prior to 2020) and differences in how we define the subsample of students, relative to the focal groups of interest in our analysis of the Common App data.Therefore, we turn to another data source to generate an additional comparison.

### Common App 2021 Cohort

During the 2021 school year, Common App conducted an unrelated randomized controlled study (RCT) with a cohort of students similar to our 2020 sample. We were able to capitalize on the sample employed in this RCT to further investigate enrollment differentials. We worked with Common App to receive high-level descriptives for the cohort of students who did not receive the 2021 intervention. Specifically, we examined fall 2021 enrollment overall and disaggregated by 4-year versus 2-year institutions for students in the 2021 study control group. These differentials allowed us to observe enrollment outcomes for first-generation *and* low-income students and first-generation *or* low-income students from the class of 2021. Unlike the HSLS-09 dataset, the 2021 Common App dataset allows us to define low-income similarly to our 2020 cohort (i.e., qualified for a Common App fee waiver).

Overall, we found that students who met both factors had somewhat lower enrollment rates than those who exhibit only one of the two factors. Students who were both first-generation *and* low-income had a fall 2021 enrollment rate of 72%, while students who only exhibited one of these factors had a 73% enrollment rate (a -1 percentage point difference).

Ideally, we would have matched students in our 2020 sample to demographically similar students in the 2021 sample, allowing us to estimate the group differentials and adjust our impact estimates directly. Unfortunately, this was not possible due to data sharing agreements. However, we were able to reduce the 2021 sample to students who attended high schools in our final matched 2020 sample. We found a similar enrollment differential in fall 2021 of around -1 percentage point.

Based on the differentials of both these data sources, we conclude that -- absent any intervention -- students who are first-generation *and* low-income have lower college-going rates than those who only hold one of those identities. Therefore, we reason that absent the intervention we would expect that students in the outreach group would have lower college enrollment rates than the comparison group, with differences on the order of -1 to -5 percentage points. These differences suggest that to find a positive effect, the impact of the outreach would have had to be larger than these differentials. Therefore, the modest negative enrollment for outreach students we estimate for the entire sample (-2 percentage points) likely reflect these differentials -- and not -- detrimental effects of the chatbot outreach.

## Discussion and Conclusion

The outreach undertaken by Common App and its partners cast a wide net in hopes of helping students through the typically challenging transition to college during the uncertainty of the COVID-19 pandemic. As part of our investigation we investigated whether the outreach improved students' college-going outcomes. The core analytic approach to our analysis relied on comparisons between observationally similar students who nevertheless differed in a critical way. Those in the treatment group were both first-generation *and* from low-income households. Those in the comparison group had one but not both of these characteristics.

Historical evidence suggests that low family income and status as a first-generation college student serve as complementary risk factors for not enrolling in college. On average, high school seniors who are first-generation *and* low-income are less likely to enroll in college than those who are either first-generation *or* low-income but not both.

While the design choice to provide outreach to all students in the graduating class of 2020 directed services to the subgroup of students that were likely most at risk in the early stages of the pandemic, it also adds an additional layer of complexity to the task of evaluating the effect of this outreach. Since students with two risk factors all received outreach, any contemporaneous comparison group will consist of students facing fewer barriers on average to college enrollment.

In the absence of outreach, students in any comparison group can be expected to be more likely to enroll in college than those selected for the intervention. That is, a typical "treatment" vs. "control" group comparison likely leads to bias and an underestimate of the effects of the outreach on college enrollment. The large size of the intervention and the availability of Common App data for students who were not offered outreach enabled us to estimate the difference in enrollment rates for "treatment" vs. "control" groups with standard errors well less than 1 percentage point. Still, large sample sizes cannot compensate on their own for underlying differences in these two groups of students.

We used propensity score matching to create an analysis sample of students who received outreach that is best suited for comparison to a similar group of students who were not offered outreach. More specifically, our matching procedure identified pairs of students who did and did not receive outreach where each pair attended the same high school and are broadly similar in terms of six demographic variables and four academic indicators. To the degree that first-generation *and* low-income students face greater barriers than first-generation *or* low-income students do, using these variables in the matching procedure should help to reduce the underlying difference in college enrollment rates for treatment and control group students expected in the absence of the intervention. Nevertheless, we still anticipated that within each matched pair, the student who received outreach would face greater barriers to college enrollment and that a comparison of enrollment rates for those in matched pairs would still underestimate the effect of advising.

Despite these challenging issues, it was still quite plausible that our evaluation would find statistically significant evidence that virtual advising increased college enrollment in 2020. One recent multi-district study estimated a 5.2 percentage point increase in four-year college enrollment as the result of text-based FAFSA outreach and support from students' own high school counselors [@Avery.2021]. An effect of that size could well have been enough to provide a positive and significant result in this study. On the other hand, recent studies of similar outreach and support implemented at scale have found much smaller effects of virtual advising than in-person advising designed to deliver the same kind of support [@Avery.2021; @phillips2022does]. Overall and based on our empirical results, we conclude that the chatbot campaign had no impact on the targeted class of 2020 students submitting college applications or enrolling in college. The limited engagement of students with the bot likely explains these results. While some students were asking the bot important questions about how to enroll in college, decipher financial aid letters, and choose a major, most students did not message Oli in substantively meaningful ways. On average, students sent 8 text messages throughout the 38 week intervention, while 2.5% of students messaged a CAC adviser. A sizable portion of the study population--16%--opted out of the intervention and thus did not receive all of Oli's guidance. However, there was evidence to suggest that the chatbot had a positive impact on college enrollment for students who were highly engaged with the bot during the intervention. This group of highly engaged students may warrant further investigation, as they could shed light on what types of conversations with the bot were most helpful in supporting their postsecondary transition. However, with an intervention that cast such a wide net and that provided relatively general guidance related to college-going processes, it is perhaps unsurprising that the overall effects of the outreach were null.

Taken together, we reasoned that this outreach faced an uphill battle to improve outcomes beyond the differentials that exist absent the intervention. Even though we cannot know what the true causal impact of the outreach was, the evidence we gathered leads us to conclude that the outreach likely neither helped nor harmed students in applying to and enrolling in college.

\newpage

## Tables and Figures

\newpage

|                                                   |           |         | **Before Matching** |           |         | **After Matching** |
|---------------------------------------------------|:---------:|:-------:|:-------------------:|:---------:|:-------:|:------------------:|
|                                                   | Treatment | Control |    Std.Mean.Diff    | Treatment | Control |   Std.Mean.Diff    |
|                                                   |           |         |                     |           |         |                    |
| Age                                               |  17.082   | 17.023  |        0.118        |   17.05   | 17.025  |       0.050        |
| Male                                              |   0.363   |  0.376  |        0.029        |   0.369   |  0.357  |       0.024        |
| English spoken at home                            |   0.562   |  0.558  |        0.009        |   0.569   |  0.573  |       0.009        |
| Has dependent                                     |   0.011   |  0.007  |        0.042        |   0.009   |  0.009  |       0.008        |
| Attended \> 1 High School                         |   0.181   |  0.161  |        0.041        |   0.164   |  0.166  |       0.003        |
| GPA rank                                          |   0.643   |  0.701  |        0.150        |   0.698   |  0.705  |       0.018        |
| Missing GPA rank                                  |   0.238   |  0.207  |        0.071        |   0.178   |  0.178  |       0.000        |
| SAT score                                         |   671.3   | 794.868 |        0.230        |  712.224  | 712.861 |       0.001        |
| Missing SAT Score                                 |   0.376   |  0.317  |        0.122        |   0.347   |  0.347  |       0.000        |
| College credit exams                              |   0.708   |  1.135  |        0.222        |   0.806   |  0.848  |       0.022        |
| TOEFL                                             |   0.000   |  0.001  |        0.093        |   0.000   |  0.000  |       0.009        |
| Sibling attended college                          |   0.337   |  0.387  |        0.105        |   0.353   |  0.345  |       0.017        |
| Submitted one college application before outreach |   0.777   |  0.796  |        0.044        |   0.853   |  0.853  |       0.000        |
| American Indian/Alaskan Native                    |   0.005   |  0.003  |        0.021        |   0.003   |  0.003  |       0.003        |
| Asian                                             |   0.084   |  0.091  |        0.027        |   0.096   |  0.093  |       0.010        |
| Black                                             |   0.28    |  0.185  |        0.213        |   0.287   |  0.307  |       0.045        |
| Latinx                                            |   0.41    |  0.231  |        0.364        |   0.408   |  0.396  |       0.023        |
| Native Hawaiian/Pacific Islander                  |   0.003   |  0.002  |        0.011        |   0.002   |  0.002  |       0.011        |
| White                                             |   0.169   |  0.383  |        0.570        |   0.164   |  0.164  |       0.002        |
| Multi-racial                                      |   0.044   |  0.051  |        0.031        |   0.036   |  0.031  |       0.025        |
| Race unknown                                      |   0.004   |  0.026  |        0.328        |   0.003   |  0.004  |       0.006        |
| Non-resident                                      |   0.000   |  0.028  |        4.261        |   0.000   |  0.000  |       0.042        |
| URM                                               |   0.826   |  0.563  |        0.694        |   0.833   |  0.833  |       0.000        |
|                                                   |           |         |                     |           |         |                    |
| Num.Obs                                           |  142,827  | 263,299 |                     |  99,593   | 61,553  |                    |

: Balance of student characteristics {#tbl-balance }

\newpage

|                          |   Overall    | Before outreach | After outreach |
|--------------------------|:------------:|:---------------:|:--------------:|
| Application Differential | -0.015\*\*\* |      0.004      |  -0.013\*\*\*  |
|                          |   (0.003)    |     (0.003)     |    (0.002)     |
|                          |              |                 |                |
| Control Mean             |    0.861     |      0.848      |     0.036      |
| Num.Obs.                 |    161146    |     161146      |     161146     |
| R2                       |    0.318     |      0.324      |     0.079      |
| FE: school_id            |      X       |        X        |       X        |

: Impacts on submitting at least one college application {#tbl-submit}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement. Baseline covariates include fee waiver indicator, first-generation status, age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator, sibling college indicator, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance. Robust standard errors clustered at the school level, reported in parentheses.

\newpage

|                         |  Fall 2020   | Spring 2021  |  Fall 2021   | Spring 2022  | Gap semester | Gap year  |
|:------------------------|:------------:|:------------:|:------------:|:------------:|:------------:|:---------:|
| Enrollment Differential | -0.020\*\*\* | -0.031\*\*\* | -0.033\*\*\* | -0.038\*\*\* |    0.003     | 0.004\*\* |
|                         |   (0.004)    |   (0.005)    |   (0.005)    |   (0.005)    |   (0.002)    |  (0.002)  |
|                         |              |              |              |              |              |           |
| Control Mean            |    0.787     |    0.724     |    0.701     |    0.648     |    0.024     |   0.023   |
| Num.Obs.                |    161146    |    161146    |    161146    |    161146    |    161146    |  161146   |
| R2                      |    0.155     |    0.177     |    0.179     |    0.194     |    0.073     |   0.079   |
| FE: school_id           |      X       |      X       |      X       |      X       |      X       |     X     |

: Impacts on college enrollment outcomes, by term {#tbl-overenroll}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement.Baseline covariates include fee waiver indicator, first-generation status,age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator,sibling college indicator, submitted at least one CommonApp application before start of intervention, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance.The gap semester outcome is a binary indicator coded as 1 if a student did not enroll in Fall 2020 but did enroll in Spring 2021. The gap year outcome is also a binary indicator coded as 1 if a student did not enroll in Fall 2020 or Spring 2021, but did enroll in Fall 2022. Robust standard errors clustered at the school level, reported in parentheses.

\newpage

|                         | Fall 2020 | Spring 2021 | Fall 2021 | Spring 2022 | Gap semester | Gap year |
|:------------------------|:---------:|:-----------:|:---------:|:-----------:|:------------:|:--------:|
| Enrollment Differential |   0.008   |    0.030    |  -0.007   |    0.016    |    0.001     |  0.000   |
|                         |  (0.016)  |   (0.016)   |  (0.016)  |   (0.016)   |   (0.006)    | (0.006)  |
|                         |           |             |           |             |              |          |
| Control Mean            |   0.683   |    0.605    |   0.593   |    0.528    |    0.036     |  0.041   |
| Num.Obs.                |   14842   |    14842    |   14842   |    14842    |    14842     |  14842   |
| R2                      |   0.311   |    0.338    |   0.337   |    0.344    |    0.224     |  0.248   |
| FE: school_id           |     X     |      X      |     X     |      X      |      X       |    X     |

: Impacts on college enrollment outcomes, by term - Submitted no application prior to intervention {#tbl-overenroll_submit}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement.Baseline covariates include fee waiver indicator, first-generation status,age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator,sibling college indicator, submitted at least one CommonApp application before start of intervention, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance.The gap semester outcome is a binary indicator coded as 1 if a student did not enroll in Fall 2020 but did enroll in Spring 2021. The gap year outcome is also a binary indicator coded as 1 if a student did not enroll in Fall 2020 or Spring 2021, but did enroll in Fall 2022. Robust standard errors clustered at the school level, reported in parentheses.

\

\newpage

|                         |  Fall 2020   |    4-year    | 4-year public | 4-year private |   2-year    |  Full-time   |
|:------------------------|:------------:|:------------:|:-------------:|:--------------:|:-----------:|:------------:|
| Enrollment Differential | -0.020\*\*\* | -0.040\*\*\* |    -0.007     |  -0.033\*\*\*  | 0.019\*\*\* | -0.029\*\*\* |
|                         |   (0.004)    |   (0.005)    |    (0.005)    |    (0.004)     |   (0.003)   |   (0.005)    |
|                         |              |              |               |                |             |              |
| Control Mean            |    0.787     |    0.671     |     0.457     |     0.217      |    0.121    |    0.668     |
| Num.Obs.                |    161146    |    161146    |    161146     |     161146     |   161146    |    161146    |
| R2                      |    0.155     |    0.207     |     0.173     |     0.141      |    0.145    |    0.175     |
| FE: school_id           |      X       |      X       |       X       |       X        |      X      |      X       |

: Impacts on Fall 2020 college enrollment outcomes {#tbl-20enroll}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement. Baseline covariates include fee waiver indicator, first-generation status, age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator, sibling college indicator, submitted at least one CommonApp application before start of intervention, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance. Robust standard errors clustered at the school level, reported in parentheses.

\newpage

|                         |  Fall 2020   |    4-year    | 4-year public | 4-year private |   2-year    |  Full-time   |
|:------------------------|:------------:|:------------:|:-------------:|:--------------:|:-----------:|:------------:|
| Enrollment Differential | -0.021\*\*\* | -0.044\*\*\* |    -0.007     |  -0.037\*\*\*  | 0.022\*\*\* | -0.034\*\*\* |
|                         |   (0.005)    |   (0.006)    |    (0.007)    |    (0.006)     |   (0.004)   |   (0.006)    |
|                         |              |              |               |                |             |              |
| Control Mean            |    0.784     |    0.661     |     0.452     |     0.212      |    0.129    |    0.661     |
| Num.Obs.                |    97537     |    97537     |     97537     |     97537      |    97537    |    97537     |
| R2                      |    0.160     |    0.213     |     0.181     |     0.144      |    0.152    |    0.180     |
| FE: school_id           |      X       |      X       |       X       |       X        |      X      |      X       |

: Impacts on Fall 2020 college enrollment outcomes - Racially marginalized students {#tbl-20enrollurm}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement. Baseline covariates include fee waiver indicator, first-generation status, age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator, sibling college indicator, submitted at least one CommonApp application before start of intervention, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance. Robust standard errors clustered at the school level, reported in parentheses.

\newpage

|                         | Fall 2020 | Spring 2021 | Fall 2021 | Spring 2022 | Gap semester | Gap year |
|:------------------------|:---------:|:-----------:|:---------:|:-----------:|:------------:|:--------:|
| Enrollment Differential |  -0.017   |   -0.016    |  -0.011   |   -0.011    |    -0.001    |  0.006   |
|                         |  (0.010)  |   (0.011)   |  (0.011)  |   (0.011)   |   (0.004)    | (0.003)  |
|                         |           |             |           |             |              |          |
| Control Mean            |   0.787   |    0.724    |   0.701   |    0.651    |    0.025     |  0.022   |
| Num.Obs.                |   20527   |    20527    |   20527   |    20527    |    20527     |  20527   |
| R2                      |   0.282   |    0.295    |   0.303   |    0.315    |    0.212     |  0.222   |
| FE: school_id           |     X     |      X      |     X     |      X      |      X       |    X     |

: Impacts on college enrollment outcomes - Opt out {#tbl-20enrollopt}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: Opt-out was constructed by flagging students who explicitly messaged "STOP" to the bot at any point during the intervention and/or received a "goodbye" message from Oli, indicating that they had requested to opt out. Due to data quality issues we only have text message data for 70% of all treated students. Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement.Baseline covariates include fee waiver indicator, first-generation status,age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator,sibling college indicator, submitted at least one CommonApp application before start of intervention, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance.The gap semester outcome is a binary indicator coded as 1 if a student did not enroll in Fall 2020 but did enroll in Spring 2021. The gap year outcome is also a binary indicator coded as 1 if a student did not enroll in Fall 2020 or Spring 2021, but did enroll in Fall 2022. Robust standard errors clustered at the school level, reported in parentheses.

\newpage

|                         | Fall 2020 | 4-year  | 4-year public | 4-year private |   2-year    | Full-time |
|:------------------------|:---------:|:-------:|:-------------:|:--------------:|:-----------:|:---------:|
| Enrollment Differential | 0.026\*\* |  0.002  |   0.032\*\*   |  -0.032\*\*\*  | 0.024\*\*\* |   0.005   |
|                         |  (0.009)  | (0.010) |    (0.011)    |    (0.009)     |   (0.007)   |  (0.010)  |
|                         |           |         |               |                |             |           |
| Control Mean            |   0.793   |  0.674  |     0.461     |     0.215      |    0.123    |   0.676   |
| Num.Obs.                |   26281   |  26281  |     26281     |     26281      |    26281    |   26281   |
| R2                      |   0.256   |  0.304  |     0.279     |     0.253      |    0.251    |   0.272   |
| FE: school_id           |     X     |    X    |       X       |       X        |      X      |     X     |

: Impacts of text message engagement on Fall 2020 college enrollment - High text engagement {#tbl-20enrolleng}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: High engagement indicator is coded as 1 for students who are in the top 25th percentile (sent more than 9 text messages) of total number of text messages students sent throughout the intervention. Due to data quality issues we only have text message data for 70% of all treated students. Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement.Baseline covariates include fee waiver indicator, first-generation status,age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator,sibling college indicator, submitted at least one CommonApp application before start of intervention, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance.Robust standard errors clustered at the school level, reported in parentheses.

\newpage

![Propensity score distribution](figures/ps_dist.png){#fig-psdist alt="Propensity score distribution" width="340"}
