# Paper 2: Navigating college enrollment during the COVID-19 pandemic: examining the effectiveness of an artificially intelligent chatbot in helping students navigate the road to college

## Introduction

In spring 2020, schools and colleges across the US began to shut down and pivot to remote learning to protect students and their communities from the rapid spread of the COVID-19 virus. While all the while, high school seniors began receiving acceptance letters from the colleges to which they had applied before the pandemic. At the time, the effect of the pandemic on college-going was uncertain. On the one hand, the COVID-19 crisis hampered business-as-usual operating procedures for colleges. Because of this, students -- particularly those planning to live on campus -- may have been more likely to delay college entry. On the other hand, higher education is a counter-cyclical industry such that college enrollment rates tend to be higher during economic downturns. For recent high school graduates, in particular, the impact of the pandemic on service and retail, among other sectors, may severely limit job opportunities. This, in turn, may make higher education an attractive option for young adults who might otherwise enter the labor market after high school.

In response to this uncertainty, the Common Application (Common App), in partnership with Mainstay and the College Advising Corps (CAC), acted quickly to provide students with proactive outreach and guidance on college-going tasks via an innovative large-scale artificially intelligent (AI) chatbot campaign. This outreach targeted nearly 174,000 US high school students who were the first in their families to go to college and came from underresourced families, most of whom were racially marginalized (83%). Due to an array of systemic barriers, this subgroup of students likely faced reduced access to quality health care, living, and work conditions during the pandemic (CITE) and, therefore, would likely benefit from additional support during their precarious college transition.

Over 38 weeks, a Mainstay AI chatbot named *Oli* sent scripted messages to students on various topics related to the college search, application, and matriculation processes. To better target the information, Oli solicited information directly from students about the types of resources they needed and pressing questions they had. The chatbot "learned" from these students' interactions and developed a knowledge base to provide more targeted and individualized real-time responses. Student questions that Oli could not answer were forwarded to college advisers, who would follow up directly with individual students.

In recent years, researchers have become increasingly interested in leveraging behavioral economics insights to evaluate the effectiveness of AI chatbots in helping students transition to and through college (CITE). These studies have demonstrated that AI chatbots deployed within a local university context are effective in assisting students complete pre-matriculation tasks and ultimately enroll in college (Page..Gelbach..Aizat..2017;2020;2021). However, many questions remain about the scalability of AI chatbot interventions. Previous research has highlighted that non-chatbot nudge interventions fail to maintain efficacy when scaled to a national level, with many finding modest to null effects on college enrollment (CITE). 

Therefore, the aim of my second dissertation is to add to the literature on the scalability of AI chatbot campaigns by evaluating the effectiveness of a large-scale AI chatbot campaign that targeted first-generation, low-income students. Specifically, I focus on whether the outreach campaign resulted in more students applying to and enrolling in college. In addition, I explore heterogeneity in outreach impact on college enrollment by students' application behavior, racial/ethnic subgroups, and level of engagement with the chatbot.

In the following sections, I review the relevant literature for this study, followed by a description of the data and the analytical strategy that I employ. I end by presenting preliminary results of the evaluation.

## Literature Review

In this section, I review the literature on college access broadly and during the COVID-19 pandemic. In addition, I draw on literature addressing barriers to college access for high school students through behavioral text message nudge interventions and chatbots.

However, before reviewing the literature, I want to define how I conceptualize first-generation students. I take an anti-deficit approach to my grounding of first-generation students -- many of whom also tend to be economically and racially marginalized (CITE). Anti-deficit practices explore how marginalized students persist in the face of social and structural barriers (CITE, Harper). The goal is to reframe students' experiences from struggling to ones of resiliency. Since most deficit approaches focus on what students lack, anti-deficit strategies focus on what students bring that leads to their resiliency and persistence. Although I take an anti-deficit stance in my review of the literature, I acknowledge that most existing literature around college access, especially literature focused on economically and racially marginalized students, is written from a deficit perspective (CITE).

### College access and returns to a college education

Despite increasing access to higher education in recent decades, there remain disparities in college attainment by socioeconomic status and parental education (CITE). Students whose parents received a college degree enroll in college at higher rates than those parents who did not complete high school (CITE). Additionally, students from higher-income backgrounds are more likely to enroll and complete postsecondary education than students from less-resourced families (CITE). For example, around 13% of students from the lowest income brackets earn a bachelor's degree by age 24, compared to 64% of students from the highest income brackets (CITE). However, much of the quantitative literature fails to acknowledge systemic barriers and the role of US policies in creating these disparities.

Lower-income and particularly racially marginalized people face structural challenges in accessing quality education, housing, and working conditions (CITE). For example, decades of federal redlining in housing and the policy of using local taxes to fund schools creates raced and classed outcomes in educational opportunities and family wealth (CITE). Therefore, disparities in college access and completion by parental education -- along with social and structural barriers -- are vital contributors to the growing income inequality in the US.

Despite the rising cost of college (CITE), the non-pecuniary and long-term economic returns to a college education remain positive. Adults who earn a bachelor's degree tend to earn and accumulate more wealth than those who have not completed college (CITE). On average, individuals with a college degree have lower unemployment rates, higher civic involvement, and better health outcomes (CITE). However, researchers note that the rate of economic return is substantially higher for continuing education students than for first-generation students (CITE). Given that more than half of all college students identify as first-generation (56%), substantial policy efforts have focused on supporting college-intending first-generation students to enroll in college (CITE). However, the COVID-19 pandemic has added additional complexity to the college enrollment process for first-generation students.

### College access during the COVID-19 pandemic

The COVID-19 pandemic increased the uncertainty for high school seniors who planned to enroll in college in fall of 2020. In the early stages of the pandemic, one in six recent high school graduates reported rethinking their decision to enroll in college, and nearly two-thirds expressed concern about attending their first choice college (CITE). The most cited reason for not enrolling in their first choice school was concern about their family's ability to afford college (CITE).

Now, nearly three years into the pandemic, we have some evidence of the detrimental effects of the pandemic on college-going. Enrollment for first-time undergraduates in fall 2020 fell to unprecedented levels, dropping nearly ten percentage points compared to fall 2019, with higher rates for students from low-income high schools (CITE). Furthermore, students from low-minority schools were more likely to immediately enroll in college than those from high schools with more diverse student bodies, 64% versus 52%, respectively (CITE).

A recent report by the College Board provides additional details on the detrimental effects of the pandemic on college enrollment. Drawing on data from nearly 10 million US students, the College Board calculated high-level descriptives and regression-adjusted models identifying the portion of fall 2020 enrollment rates attributable to COVID-19. The most significant change in student enrollment occurred within community colleges, where enrollment rates decreased by nearly 12% due to the pandemic. Most worrying is that enrollment rates fell mainly for first-generation low-income students (CITE).

A likely reason for the drop in college enrollment rates for first-generation students is the pandemics' disproportionate effect on racially and economically marginalized communities. During the pandemic, these communities experienced reduced access to high-quality healthcare and working conditions and lower expected wages (CITE). Before the pandemic, the typical Black and Latinx household had a net wealth of approximately $17,000 compared to $171,000 held by the typical white household (CITE). Therefore, Black and Latinx households may have found it more difficult to cope with the economic shock caused by the pandemic. These households had to grapple with less liquidity and wealth to cover unexpected expenses and loss in wages (CITE, Center for american progress). This, in turn, may have caused first-generation students to prioritize immediate health and economic stressors, such as supporting their families financially or dealing with lost wages, rather than prioritizing college enrollment. Data from the College Board may support this hypothesis. Students from higher-income households were far more likely to enroll in college immediately after graduation (65%) than those from low-income high schools (49%). Furthermore, first-generation students were nearly twice as likely to be concerned about paying for education expenses in fall 2020 than continuing education students (CITE). The review of the literature highlights the unprecedented change in college enrollment caused by the COVID-19 pandemic, particularly for first-generation, economically and racially marginalized students. Next, I review the literature on well-documented challenges to college access.

### Challenges in college access 

Considerable research attention has been paid to the barriers in college application, financing, and enrollment processes, especially for students who are the first in their families to go to college (CITE). To apply for college, students must complete a series of complex tasks such as developing a list of colleges, deciding which colleges to apply to, and applying for financial aid through a cumbersome financial aid application (the Free Application for Federal Financial Aid - FAFSA). Students typically access federal, state, and grant aid -- aid that *does not* have to be repaid -- by completing the FAFSA form, which has more than 100 questions that ask students about their and their families' financial assets.

Many scholars have stressed the importance of supporting students in completing and submitting the FAFSA, since a sizable portion of first-generation students come from less-resourced families and, therefore, must rely on financial support to finance their post-secondary aspirations (CITE). Scholars have argued that due to federal and state divestment from funding postsecondary institutions, the focus has changed from need-based grants to merit-based aid and loans (CITE). This shift has negatively impacted first-generation and low-income students (Long & Riley, 2007).

Several studies have shown how supporting students in submitting the FAFSA has led to positive college enrollment outcomes (CITE). For example, a recent clustered randomized trial by Castleman and Page (2019) demonstrated that supporting students in completing the FAFSA led to an increase of 6 percentage points in FAFSA submission and completion rates and an increase in college enrollment of around 3 percentage points. However, helping students submit the FAFSA is only half the battle. For a subset of students -- primarily first-generation and low-income students -- their FAFSA is selected for further scrutiny by a process termed verification. Verification requires students to attest and provide documentation that the information reported on their FAFSA is correct (CITE). Scholars have recently discovered that verification leads to lower college matriculation rates, particularly for first-generation low-income students (CITE). However, once a student accepts an admission offer, additional barriers remain to college enrollment, especially in the summer before college.

Scholars have suggested that the summer before college enrollment is fraught with additional barriers to college matriculation (CITE). Given that students must complete a series of pre-matriculation administrative tasks, including submitting high school transcripts, deciding which and how much aid to accept, taking placement exams, and successfully enrolling in classes (CITE). If students do not complete these steps, they may eventually fail to enroll in college, a process termed "summer melt" (CITE). Researchers have found that summer melt affects 10 to 20% of college-intending high school students, with higher rates for first-generation, racially, and economically marginalized students (CITE) who may lack access to information and support to help navigate these complex administrative tasks (CITE).

### Unique challenges in college access among first-generation students

However, for first-generation students, there is an added layer of complexity in navigating these tasks with little to no access to social and cultural capital around the college application process. For example, scholars have emphasized how first-generation students tend to reside in schools with less access to college prep or Advanced Placement (AP) courses, which have been positively associated with college enrollment (CITE). Additionally, first-generation students tend to attend schools with little or no access to college counselors, which could help students complete college applications and navigate the complex financial aid process (CITE). However, even when students have access to a college counselor, these counselors are burdened with high caseloads and cannot provide individualized support to students (CITE).

In addition, first-generation students who are racially marginalized must endure social and systemic barriers and policies that traditionally left them with less access to high-quality primary and secondary education (CITE). For example, policies that funnel racially marginalized students into underserved schools, with limited access to college prep courses, and school tracking policies that exclude racially marginalized students from gaining access to the college prep tracks that would make them eligible to apply to college (Oakes, 2005; Oakes, Rogers, Lipton, & Morell, 2002; Solorzano & Ornelas, 2002, 2004). Additionally, due to decades of systemic oppression, these students have less access to intergenerational wealth, which could finance college. Therefore, they must rely on financial aid to fund their postsecondary education (CITE). However, substantial interventions have focused on supporting these students in completing pre-matriculation tasks by providing access to college advising (CITE).

### College advising as a tool to increase college access

A rich body of literature has evaluated the effectiveness of behavioral interventions that examine ways to support students in completing the key steps and deadlines to enroll in college. One favored approach to remedying students' information asymmetries in the college-going process is access to high-quality college advising that can support students' transition to college. 

Empirical evidence confirms that providing student access to college advising positively affects college enrollment and persistence (CITE). Castleman et al. (2012) provide one of the earliest pieces of evidence of the positive impact of providing college counseling to low-income students during the summer before college. Through a randomized trial, Castleman et al. show that offering college counseling support in the summer before college increases student immediate college enrollment by 14 percentage points. However, this study was conducted with "choice" schools. Therefore, open questions remained about whether this kind of support generalized to public schools.

However, a similar study by Castleman, Page and Schooley (2014) provided evidence of the generalizability of counseling support within public schools. The researcher conducted two college counseling interventions with a more nationally representative sample of public schools in Boston and Fulton County, Georgia. They found positive effects similar to those of the previous study by Castleman et al. (2012). Students who received proactive college counseling in the summer before college had an increase in college enrollment, from 8 to 12 percentage points among low-income students. However, in the context of a global pandemic, access to college counseling was severely impacted.

Against the backdrop of a global pandemic, students faced challenges in accessing quality college advising. Naughton (2021) describes the difficulties in college advising during the pandemic as turning preexisting "cracks" in the ability to provide students advising services into "craters." This is primarily driven by the transition of high schools from in person to virtual advising. Naughton found that college advisors felt ineffective in providing virtual support and guidance once high schools transitioned to virtual advising. The ability of advisors to contact students was severely diminished and left students without support to facilitate their college transition (CITE). Naughton explains that advisers in high schools with higher college enrollment rates were far less impacted than students in lower college enrollment high schools, precisely those needing the most support (CITE). Therefore, approaches that bring college counseling directly to students who need it most are crucial, especially virtual support.

### Behavioral nudge interventions - summer melt

A low-cost approach to providing college students with counseling support has been behavioral nudge interventions. These interventions distill complex information and deadlines into short, concise messages delivered to students via a mode of communication that students are well aware of -- text messages (CITE). In the context of college access, these studies have focused on remedying "summer melt" by helping students navigate well-defined but complex tasks such as completing financial aid applications, submitting high school transcripts, and enrolling in classes (CITE).

Castleman and Page (DATE) have conducted several randomized controlled studies utilizing text nudges to remedy summer melt. For example, Castleman and Page (2015) employ an automated and personalized text message campaign to "nudge" students on pre-matriculation tasks. They found that the text message campaign increased college enrollment among students with less access to college access support by approximately 7 percentage points. The authors and other scholars have highlighted the relatively low cost of these text nudge interventions as opposed to more intensive in-person counselor support (CITE). The low-cost nature of these interventions has spurred an interest in using nudge interventions in the last 10 years (CITE). However, the evidence on the scalability of these interventions is mixed.

A recent line of research has explored the scalability of behavioral text nudge interventions, given that most large college access nudge interventions partner with local high schools, colleges, or non-profits (CITE). A recent study by Bird et al. (2021) offered evidence that nudges may be unsuccessful when scaled up. Bird and colleagues examined student nudges using multiple modalities, including email and text messages, that encouraged students to complete the FAFSA and enroll in college. What made this study unique is the sheer number of students targeted, nearly 800,000 students across the US using the Common App or the Texas admission system. The researchers found that no form of outreach (i.e., email or text message) increased financial aid receipt, college enrollment, or persistence. A suggested reason for this finding is that scaling nudge campaigns can be challenging when implemented by a large organization, since the student may not have a solid connection to a large organization like the Common App as opposed to their local high school. Additionally, the author's reason that in order to deliver nudges to a broad group of students, the messaging may be too "generic and one-way" and lack personalization, leading to students not taking actionable steps to enroll in college. However, recent studies have used technological advances to help close this personalization divide. 

### AI chatbots in college access

A recent push in the behavioral nudge literature has been to leverage artificial intelligence (AI) chatbots to provide personalized support to students navigating college applications and enrollment. These AI conversational chatbots can provide personalized information to students about deadlines and specific college-going questions. These AI bots are trained by college staff to develop a "bank" of responses to frequently asked questions about college-going. Over time, the AI "learns" how to handle increasingly complex student queries. Once successfully trained and deployed to students, the chatbot can help students answer questions about important deadlines, such as how to submit their high school transcripts.

A total of three studies to date have provided empirical evidence of the effectiveness of AI chatbots in college access (CITE). Page and Gelbach (2017) conducted the first of these studies at Georgia State University (GSU). Page and Gelbach partnered with GSU and an ed-tech start-up (Mainstay; formerly AdmitHub) to create an AI conversation chatbot named "Pounce." Pounce provided support in navigating pre-matriculation tasks to students accepted to GSU in 2017. The researchers found that the chatbot led to a 3.3 percentage point increase in timely enrollment at GSU and helped students with pre-matriculation tasks like signing up for orientation.

A follow-up study at GSU (Page et al., 2022) shifted the focus of the chatbot outreach from helping students enroll at GSU to supporting them once they arrived at GSU by assisting them in completing administrative processes, such as resolving registration holds and seeking campus-based resources (CITE). A novel aspect of this randomized study was its focus on the main campus of GSU-Atlanta, a four-year institution, and the perimeter campus of GSU, a two-year institution. Researchers found that regardless of institution type, the chatbot effectively changed student behavior when outreach addressed serious and time-sensitive administrative processes, such as resolving administrative holds (CITE). However, they found that outreach was ineffective when chatbot messaging focused on helping students with "non-urgent" tasks such as accessing supplemental, academic, social, and career-related support. This study provides evidence that chatbot interventions are most effective when they target discrete, time-sensitive, and well-defined tasks. These findings parallel findings from previous text-nudge interventions that have found students were less responsive to nudges related to less time-sensitive tasks such as future-job prospects (CITE). However, it is important to note that both studies mentioned previously occurred within the GSU system. Therefore, an open question was whether the positive effects of AI chatbot intervention replicate to other institutions outside the GSU system. 

However, Nushatayeva et al. (2021) argue that context matters regarding chatbot interventions for college access. Nushatayeva and colleagues replicate the work of Page and Gelbach (2017) and Page et al. (2022) using "PeeDee," an AI chatbot at East Carolina University (ECU), a four-year institution in North Carolina. Furthermore, this study provides evidence of treatment heterogeneity by understanding to whom and under what conditions the intervention was most effective. Researches test the use of PeeDee in improving students' completion of pre-matriculation tasks and ultimate college enrollment at ECU. They find no overall effect of the chatbot on college enrollment but did find an impact on loan acceptance rates. Students in the treatment group had an 8 percentage point increase in accepting a student loan compared to the control group. Nushatayeva et al. attribute the null overall results on college enrollment to the relatively advantaged student body at ECU versus GSU. However, the authors find that the chatbot increased enrollment at ECU and course enrollment by 3 percentage points, specifically for first-generation students.

Together, these studies provide evidence on the effectiveness of AI bots in helping students transition into and through college. However, chatbot-based nudges are not uniformly effective across various contexts. The literature points to these interventions being most salient when chatbots help nudge students on time-sensitive, actionable tasks, such as resolving registration holds. In addition, chatbot outreach is most effective at helping students with little prior knowledge about college application or enrollment, that is, first-generation students. These findings parallel the results of the much more mature literature on general nudges in college access -- context matters. However, little is known about the scalability of these types of interventions outside universities, such as at the national level targeting hundreds of thousands of students.


## Methods

It is important to note that the outreach team did not randomly assign the outreach for this campaign. Randomly assigning the outreach would ensure that students who received the outreach and those who did not would be balanced on both observable *and* unobservable characteristics. Given the lack of randomization, a simple comparison of outcomes between students who received the outreach and those who did not may -- incorrectly -- conclude that the outreach affected the outcome. To guard against this to the fullest extent possible, we matched students in the outreach group to demographically similar students in the same high school who were not treated. These matched students are then observationally similar to students in the outreach group and, therefore, a reasonable comparison group. Analytically, we do this matching via a procedure termed propensity score matching.

### Matching

Propensity score matching is used in non-experimental studies (i.e., studies in which students were not randomized to outreach or control) to balance the observed characteristics between treated and untreated students. It involves estimating a *propensity score*, which is the probability that a student would have been exposed to outreach, conditional on a set of observable characteristics. This propensity score is then used to match treated students to control students with similar estimated propensity scores, resulting in a control group that is demographically similar to the outreach group. Propensity score matching allows us to more accurately evaluate the impact of the outreach than a simple comparison of outcomes between those students who received the outreach and those who did not.

We estimated the propensity score as a function of student-level characteristics available from the Common App data, including: include race/ethnicity, age, gender, English spoken at home, dependent status, number of high schools attended, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator, whether they had a sibling in college, and whether they submitted at least one Common App application before the start of outreach.

To increase balance on our covariates, we placed additional restrictions on our matching algorithm to exact match on specific variables, such as students' high school. Exact matching forced the matching algorithm to *only* find potential matches for treated students in the same high school. This allowed us to control for the fact that students across different high schools have varied school experiences; therefore, matching within schools provides more reasonable matched control students.

Given the algorithm we used, it is possible for one control student to be a perfect match for more than one treatment student. Therefore, we allowed the matching algorithm to match an outreach student to multiple control students. Where this occurred, the control student was up-weighted in our analysis to essentially be "counted" as a comparison for numerous treated students. After concluding our matching, we had a final outreach group of 99,593 students matched to 61,553 control students. @tbl-balance presents balance on student characteristics before and after our matching procedure. After matching, we found no difference above 0.1 standardized mean difference, a commonly accepted benchmark for achieving balance. For a technical explanation of our matching approach, please refer to Appendix A.

## Data

Common App and partners selected for outreach all students from the high school class of 2020 who met two specific criteria: 1) they would be first-generation college students, and 2) they had a low family income, as indicated by qualifying for a Common App fee waiver. In total, the outreach targeted nearly 174,000 students in the class of 2020.

We received student-level data from the Common App for the entire cohort of students who had created a Common App account and had intended to apply to college -- hoping to enroll in fall 2020 -- including students who were selected for the outreach $(n = 173,776)$ and those who were not $(n = 1,229,232)$, for a total of nearly 1.5 million student records.

To evaluate the effectiveness of the outreach campaign, our analysis relied on matching students selected to receive outreach to similar students in the same high school who were not selected for outreach. In particular, some Common App students met one but not both criteria for inclusion in the outreach (i.e., first-generation college student *or* qualified for a fee-waiver). We use these students as our key source of comparison in the analyses. Therefore, our outreach ("treatment") group includes students who met both inclusion criteria (i.e., first-generation college student *and* qualified for a fee-waiver) while our control students met one but not *both* criteria.

Once we cleaned and subset the data to match the outreach inclusion criteria, we arrived at a final analytic sample of 142,837 students who were targeted for the outreach and 263,399 students who were not treated and therefore qualified as potential control students.

## Analytic strategy

In this appendix, we provide a technical explanation of our analytic approach to constructing our matched control group using propensity score matching. Propensity score matching allows us to balance observed characteristics between treated and untreated students, leading to a more accurate estimate of the impact of the outreach.

We begin by estimating a propensity score, defined as the probability that a student would have been exposed to outreach, conditional on a set of observable characteristics. Our propensity score model took the following general form @eq-int-ps:

$$
Outreach_{is} = \beta_{0} + \beta_{1}X + \lambda_{is}
$$ {#eq-int-ps}

Where $outreach_{i.s}$ represents a binary indicator coded as 1 for students who received the outreach and 0 if a student did not. $X$ is a vector of student level characteristics. These include age, gender, English spoken at home, dependent flag, number of high schools attended, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator, sibling college indicator, submitted at least one Common App application before start of outreach, and race/ethnicity indicators.

We also included $(\lambda)$, which represents a vector of covariates for which we exact matched students on. These include students' high school, underrepresented minority indicator, an indicator for submitting at least one Common App application before the start of outreach, and missing indicators for high school GPA and SAT/ACT performance. Exact matching on these variables allows us to match outreach students to control students who *exactly* match them on the set of variables.

It is possible for one control student to be a perfect match for more than one treatment student. Therefore, we allow the matching algorithm to match an outreach student to multiple control students (i.e., matching with replacement). Additionally, we restrict matches to within .50 standard deviations of a propensity score. This allows us to guard against having treatment students matched with control students with a large difference in propensity scores.

To examine how well our matching procedure was able to balance student characteristics, we look at the overall distribution of the propensity score among treatment and untreated students as well as covariate balance before and after matching. @fig-psdist presents the propensity score distribution between treatment and control students before and after matching. Overall, our matching procedure did an excellent job matching treated students to control students, as indicated by the overlap in the distributions on the right panel.

In addition to analyzing the distributional differences in the propensity score, we also looked at baseline characteristics between treatment and control before and after matching. @tbl-balance compares balance of student characteristics before and after matching. Overall, control group students (who are either "low-income" or "first generation" but not both) had higher GPA class rank (70.1% vs. 64.3%) and SAT scores (794.9 vs. 671.3) than treatment group students (who are both "low-income" and "first generation"). The treatment group also included a substantially larger percentage of minority students (28% Black and 41% Latinx) than the control group (19% Black and 23% Latinx). We achieve excellent balance on all covariates included in our matching approach, with no standard mean difference below 0.1. Our final matched sample includes a treatment group consisting of 99,593 students and a matched control of 61,553.

To estimate the impact of outreach on student outcomes, we ran a series of regression models. Specifically, we regressed each outcome on an outreach indicator and student-level characteristics. We also included weights to account for our matching approach.[^03-commonapp_over-1] Our regressions took the following general form[^03-commonapp_over-2] @eq-out:

[^03-commonapp_over-1]: Matching weights were constructed to account for our matching approach that allowed multiple control students to be matched to treated students. Each treated unit gets a weight of 1, while each control student is weighted as the sum of the inverse of the number of control units matched to the same treated unit across its matches.

[^03-commonapp_over-2]: We used robust standard errors clustered at the high school level.

$$
Y_{is} = \beta_{0} + \beta_{1}Outreach_{is} + X\theta + \lambda_s + \epsilon_{is}
$$ {#eq-out}

Where $Y_{is}$ represents our outcome of interest for student $i$ in school $s$. $Outreach$ is a binary indicator coded as 1 for students who received the outreach and 0 if a student did not. $X$ is a vector of student level characteristics such as age and gender. In addition we included high school fixed effects $(\lambda)$ which soak up any remaining variation in the outcome due to differences across high schools. $\beta_{1}$ is our coefficient of interest and represents the mean controlled difference in the outcome between those who received the outreach and those who did not.

Note that although we took a robust matching approach in constructing our control group, there could still be differences between our outreach and control group on student-level characteristics. Therefore, by including the vector of student level covariates $(X)$ in our regressions, we can account for any remaining imbalance between student characteristics that we observe and increase the precision of our impact estimates.[^03-commonapp_over-3]

[^03-commonapp_over-3]: This is commonly referred to as a "doubly robust" approach.

### Limitations

While this matching procedure was successful, we want to highlight the limitations in our ability to attribute a causal relationship between the outreach and the effects we estimate. Given that students were not randomly assigned to the outreach, we cannot make causal claims about the effectiveness of the outreach. However, our robust matching approach allows us to reduce some -- but not all -- the bias that could exist in explaining the relationship between receiving outreach and outcomes. Note that we are only matching on *observable* characteristics. There could be *unobservable* characteristics that the matching procedure cannot consider that could influence outcomes absent the intervention.

Furthermore, important observable differences do remain between the outreach group and the comparison group, even after matching. Students selected for outreach were first-generation *and* low-income, while control students were either first-generation *or* low-income. The outreach team selected students in this manner to reach as many students as possible, specifically students who would likely benefit from this sort of outreach. Although this was a wise approach for targeting students in need of support, it created some analytic obstacles for our impact analysis that can't be remedied through matching. In particular, in any matched pair of low-income students, the treatment group student is first-generation and the control group student is not, and similarly, in any matched pair of first-generation students, the treatment group student is low-income and the control group student is not.

Students in the outreach group have two factors that -- due to an array of systemic issues -- may disadvantage them in terms of college enrollment. Therefore, we may expect that -- even absent the intervention -- students in the outreach group likely have lower college enrollment rates than those who did not receive the outreach. Given how the outreach groups were defined, we cannot observe college enrollment outcomes for control students who were first-generation *and* low-income, which would be the natural comparison for our outreach group. This is because, for the class of 2020, the intervention was targeted to *all* students who met these two criteria. After presenting our findings, we discuss two different strategies we used to investigate what the differences between these two groups might have been absent the chatbot outreach. Overall, we find that students who met both factors had somewhat lower enrollment rates than those who exhibit only one of the two factors

## Preliminary Results

In the following section, we present key findings from our impact analysis. We primarily focus on college application submission and enrollment in the fall term after students graduate high school. Additionally, we analyzed whether effects varied according to student characteristics and engagement in the chatbot communication. We specifically look at impacts for racially marginalized students, students who opted out of the chatbot outreach, and students who had a high level of engagement with the chatbot.

### College Application Submission

> *What is the impact of being targeted for outreach on college application submission?*

@tbl-submit shows the impact of the outreach on submitting at least one college application via the Common App. The *Differential* row refers to the difference in the mean outcome between those students who were targeted for the outreach and their demographically matched controls. For example,the first column shows the impact of the intervention on overall college application submission. Students targeted for outreach had somewhat lower application submission rates than their matched controls. 86.1% of the control group (see *Control Mean* row) submitted at least one college application, while 84.7% of outreach students did so. This differential is equal to around -1.5 percentage points.

Next, we analyzed whether impacts varied based on whether a student submitted at least one college application before or after the start of the intervention. The outreach had no impact on submitting at least one application prior to the start of the intervention, as expected. However, outreach students were less likely to submit an application after the start of the intervention (-1.3 percentage point difference).

### Overall College Enrollment

> *What is the impact of the outreach on college enrollment?*

Next, we present the effects on college enrollment outcomes (@tbl-overenroll). Students in the outreach group had somewhat lower fall 2020 enrollment rates compared to the control group. 76.7% of students in the outreach group enrolled in college in fall 2020, compared to 78.7% of control students, a 2 percentage point differential. We observed effects of a similar magnitude across all other terms.

Additionally, we explored if the outreach impacted whether a student took a gap semester or year. The outreach did not seem to impact whether students took a gap semester. However, we observed a slight increase in outreach students taking a gap year. Olli and the CAC advisors provided no information or advice about taking time off before enrolling in college, so it seems unlikely that the outreach influenced these choices given the underlying uncontrolled differences between treatment and control groups.

We further unpacked enrollment impacts by subsetting our sample to only students who had submitted no college applications prior to the start of the intervention. Given the timing of the outreach in late spring, these students missed most of the college application deadlines and therefore could benefit from this kind of outreach. However, the outreach had no impact across all enrollment outcomes (@tbl-overenroll_submit).

### Fall 2020 College Enrollment

The Common App and its partners were primarily interested in immediate college enrollment after students completed high school. Therefore, we analyzed the impact of the outreach on fall 2020 enrollment by 4-year versus 2-year institutions, private versus public, and full-time enrollment (@tbl-20enroll). The somewhat lower enrollment we observed for outreach students compared to students who did not receive the outreach was likely driven by students forgoing enrolling in 4-year institutions -- specifically 4-year privates (-3.3 percentage point difference) -- and enrolling in 2-year institutions (1.9 percentage point difference).The CAC advisors sometimes suggested two-year colleges to students who expressed concerns about college affordability but did not otherwise take any position on choosing between a two-year and a four-year college.

Additionally, outreach students had lower full-time enrollment rates -- 3 percentage points -- than control students (66.8%).

### Racially Marginalized Students

Next, we examined whether the outreach had a differential impact on students who belonged to a racially marginalized group. We defined a student as racially marginalized if a student self-identified as non-white or bi/multi-racial.

In @tbl-20enrollurm we present the impact of the outreach on fall 2020 college enrollment outcomes. We observed impacts of a similar scale as the entire sample. Racially marginalized students in the outreach group had relatively lower fall 2020 enrollment. There was a modest negative difference in enrollment in fall 2020 of around 2.1 percentage points, specifically driven by outreach students not enrolling at 4-year institutions by around -4.4 percentage points, but instead enrolling at 2-year institutions (2.2 percentage point difference).

### Outreach Participation

Using data from the qualitative analysis team about student engagement with the bot, including whether students decided to opt-out of receiving outreach from Oli. We constructed high engagement and opt out measures. Specifically, we defined high engagers as a student in the top 25th percentile of the total number of text messages sent throughout the outreach. We created the opt-out indicator by flagging students who explicitly messaged "STOP" to Oli at any point during the outreach or received a "goodbye" message from Oli, indicating that the student had requested to opt-out .[^03-commonapp_over-4]

[^03-commonapp_over-4]: Due to data quality issues, we received text message data for 70% of the outreach group. Furthermore, engagement across the intervention was relatively low. On average, students sent around 8 messages throughout the 38 weeks of the outreach. For a thorough explanation of the text analysis, please refer to Part II of this report.

### Opt-Out

Throughout the outreach campaign, students had the option to opt-out of receiving outreach by directly messaging Oli. Around 16% of students requested to opt out. In @tbl-20enrollopt we examine to what extent impacts varied among students who opted out. Across all outcomes, we see no effect on college enrollment. This finding is not surprising, given that most of the students who opted out did so in the first few weeks of the intervention and therefore received little to no communication from the chatbot.

### Oli Engagement

A final question we explored was whether impacts varied for those who engaged highly with Oli, i.e., those in the top 25th percentile of total messages sent throughout the outreach. Due to the low engagement throughout the intervention, a student who sent more than 9 messages was flagged as a "high" engager, which equaled around 17% of all outreach students.

@tbl-20enrolleng presents results for students with high engagement throughout the outreach. As opposed to our overall modest negative impacts for the whole sample, here we found a slightly positive impact on fall 2020 enrollment for highly engaged students, around a 3 percentage point improvement over the matched controls. Students who engaged with Oli at a high rate were, on average, more likely female, came from a racially marginalized group, and had slightly lower SAT/ACT performance than students who didn't have high engagement with the chatbot. Furthermore, these students also had higher rates of college application submission than non-highly engaged students.

### Enrollment Differentials in Context

We want to underscore that important, observable differences remain between the outreach group and the comparison group -- even after matching. As mentioned previously, the students selected for outreach were first-generation *and* low-income, while the control students were first-generation *or* low-income. This presented analytic obstacles in estimating outreach impacts.

In this section, we outline two different strategies we took to investigate what differences between these two groups might have been absent the chatbot outreach.

#### High School Longitudinal Study of 2009

First, we used the High School Longitudinal Study of 2009 (HSLS-09), a nationally representative sample of 9th graders in 2009 who were observed through 2016. The HSLS-09 dataset includes information on college-going. We subsetted the HSLS-09 data to a sample of students who had indicated college interest in 9th grade and had submitted at least one college application. This allowed us to mimic -- although imperfectly -- our 2020 Common App cohort of students who were college intending.

Next, we calculated enrollment differentials for first-generation *and* low-income students and first-generation *or* low-income students. In our 2020 cohort, low-income was defined as qualifying for a Common App fee waiver. Given the information available in the HSLS-09, we defined low-income as a student's family income falling below 130% of the poverty line (i.e., qualified for free or reduced lunch).

Overall, we found that students who were first-generation and low-income had lower college enrollment rates than students with only one of those characteristics. Students who were first-generation *and* low-income had a college enrollment rate of 79.1%. In comparison, students who exhibited only one of these factors had an enrollment rate of around 84.2% (a -5.1 percentage point difference).

Drawbacks of relying on the HSLS-09 as a source for informing this differential include its age (it observes a cohort of students who completed high school several years prior to 2020) and differences in how we define the subsample of students, relative to the focal groups of interest in our analysis of the Common App data.Therefore, we turn to another data source to generate an additional comparison.

#### Common App 2021 Cohort

During the 2021 school year, Common App conducted an unrelated randomized controlled study (RCT) with a cohort of students similar to our 2020 sample. We were able to capitalize on the sample employed in this RCT to further investigate enrollment differentials. We worked with Common App to receive high-level descriptives for the cohort of students who did not receive the 2021 intervention. Specifically, we examined fall 2021 enrollment overall and disaggregated by 4-year versus 2-year institutions for students in the 2021 study control group. These differentials allowed us to observe enrollment outcomes for first-generation *and* low-income students and first-generation *or* low-income students from the class of 2021. Unlike the HSLS-09 dataset, the 2021 Common App dataset allows us to define low-income similarly to our 2020 cohort (i.e., qualified for a Common App fee waiver).

Overall, we found that students who met both factors had somewhat lower enrollment rates than those who exhibit only one of the two factors. Students who were both first-generation *and* low-income had a fall 2021 enrollment rate of 72%, while students who only exhibited one of these factors had a 73% enrollment rate (a -1 percentage point difference).

Ideally, we would have matched students in our 2020 sample to demographically similar students in the 2021 sample, allowing us to estimate the group differentials and adjust our impact estimates directly. Unfortunately, this was not possible due to data sharing agreements. However, we were able to reduce the 2021 sample to students who attended high schools in our final matched 2020 sample. We found a similar enrollment differential in fall 2021 of around -1 percentage point.

Based on the differentials of both these data sources, we conclude that -- absent any intervention -- students who are first-generation *and* low-income have lower college-going rates than those who only hold one of those identities. Therefore, we reason that absent the intervention we would expect that students in the outreach group would have lower college enrollment rates than the comparison group, with differences on the order of -1 to -5 percentage points. These differences suggest that to find a positive effect, the impact of the outreach would have had to be larger than these differentials. Therefore, the modest negative enrollment for outreach students we estimate for the entire sample (-2 percentage points) likely reflect these differentials -- and not -- detrimental effects of the chatbot outreach.

## Looking Ahead

NEED TO TALK ABOUT IN FUTURE WORK THAT I PLAN TO DO SENSITIVITY ANALYSIS

The outreach undertaken by Common App and its partners cast a wide net in hopes of helping students through the typically challenging transition to college during the uncertainty of the COVID-19 pandemic. As part of our evaluation, we conducted a quantitative impact analysis to investigate whether the outreach improved students' college-going outcomes and also a qualitative text analysis to explore students' engagement with the chatbot and CAC advisers. As discussed in detail throughout this report, the core analytic approach to our impact analysis relied on comparisons between observationally similar students who nevertheless differed in a critical way. Those in the treatment group were both first-generation *and* from low-income households. Those in the comparison group had one both not both of these characteristics.

Historical evidence (see page 10) suggests that low family income and status as a first-generation college student serve as complementary risk factors for not enrolling in college. On average, high school seniors who are first-generation *and* low-income are less likely to enroll in college than those who are either first-generation *or* low-income but not both.

While the design choice to provide outreach to all students in the graduating class of 2020 directed services to the subgroup of students that were likely most at risk in the early stages of the pandemic, it also adds an additional layer of complexity to the task of evaluating the effect of this outreach. Since students with two risk factors all received outreach, any contemporaneous comparison group will consist of students facing fewer barriers on average to college enrollment.

In the absence of outreach, students in any comparison group can be expected to be more likely to enroll in college than those selected for the intervention. That is, a typical "treatment" vs. "control" group comparison likely leads to bias and an underestimate of the effects of the outreach on college enrollment. The large size of the intervention and the availability of Common App data for students who were not offered outreach enabled us to estimate the difference in enrollment rates for "treatment" vs. "control" groups with standard errors well less than 1 percentage point. Still, large sample sizes cannot compensate on their own for underlying differences in these two groups of students.

We used propensity score matching to create an analysis sample of students who received outreach that is best suited for comparison to a similar group of students who were not offered outreach. More specifically, our matching procedure identified pairs of students who did and did not receive outreach where each pair attended the same high school and are broadly similar in terms of six demographic variables and four academic indicators. To the degree that first-generation *and* low-income students face greater barriers than first-generation *or* low-income students do, using these variables in the matching procedure should help to reduce the underlying difference in college enrollment rates for treatment and control group students expected in the absence of the intervention. Nevertheless, we still anticipated that within each matched pair, the student who received outreach would face greater barriers to college enrollment and that a comparison of enrollment rates for those in matched pairs would still underestimate the effect of advising.

Despite these challenging issues, it was still quite plausible that our evaluation would find statistically significant evidence that virtual advising increased college enrollment in 2020. One recent multi-district study estimated a 5.2 percentage point increase in four-year college enrollment as the result of text-based FAFSA outreach and support from students' own high school counselors.[^03-commonapp_over-5] An effect of that size could well have been enough to provide a positive and significant result in this study. On the other hand, recent studies of similar outreach and support implemented at scale have found much smaller effects of virtual advising than in-person advising designed to deliver the same kind of support.[^03-commonapp_over-6][^03-commonapp_over-7] Overall and based on our empirical results, we conclude that the chatbot campaign had no impact on the targeted class of 2020 students submitting college applications or enrolling in college.

[^03-commonapp_over-5]: James, G., Witten, D., Hastie, T., & Tibshirani, R. (2017). An introduction to statistical learning with applications in R. Springer.

[^03-commonapp_over-6]: Avery, C., Castleman, B.L., Hurwitz, M., Long, B.T, and Page, L.C. (2021) "Digital messaging to improve college enrollment and success," Economics of Education Review, Elsevier, vol. 84(C).

[^03-commonapp_over-7]: Phillips, M., and Reber, S. (2022). "Does Virtual Advising Increase College Enrollment? Evidence from a Random-Assignment College Access Field Experiment." American Economic Journal: Economic Policy, 14 (3): 198-234.

The limited engagement of students with the bot likely explains these results. While some students were asking the bot important questions about how to enroll in college, decipher financial aid letters, and choose a major, most students did not message Oli in substantively meaningful ways. On average, students sent 8 text messages throughout the 38 week intervention, while 2.5% of students messaged a CAC adviser. A sizable portion of the study population--16%--opted out of the intervention and thus did not receive all of Oli's guidance. However, there was evidence to suggest that the chatbot had a positive impact on college enrollment for students who were highly engaged with the bot during the intervention. This group of highly engaged students may warrant further investigation, as they could shed light on what types of conversations with the bot were most helpful in supporting their postsecondary transition. However, with an intervention that cast such a wide net and that provided relatively general guidance related to college-going processes, it is perhaps unsurprising that the overall effects of the outreach were null.

Taken together, we reasoned that this outreach faced an uphill battle to improve outcomes beyond the differentials that exist absent the intervention. Even though we cannot know what the true causal impact of the outreach was, the evidence we gathered leads us to conclude that the outreach likely neither helped nor harmed students in applying to and enrolling in college.

\newpage

## Tables and Figures

\newpage

|                                                   |           |         | **Before Matching** |           |         | **After Matching** |
|-----------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
|                                                   | Treatment | Control |    Std.Mean.Diff    | Treatment | Control |   Std.Mean.Diff    |
|                                                   |           |         |                     |           |         |                    |
| Age                                               |  17.082   | 17.023  |        0.118        |   17.05   | 17.025  |       0.050        |
| Male                                              |   0.363   |  0.376  |        0.029        |   0.369   |  0.357  |       0.024        |
| English spoken at home                            |   0.562   |  0.558  |        0.009        |   0.569   |  0.573  |       0.009        |
| Has dependent                                     |   0.011   |  0.007  |        0.042        |   0.009   |  0.009  |       0.008        |
| Attended \> 1 High School                         |   0.181   |  0.161  |        0.041        |   0.164   |  0.166  |       0.003        |
| GPA rank                                          |   0.643   |  0.701  |        0.150        |   0.698   |  0.705  |       0.018        |
| Missing GPA rank                                  |   0.238   |  0.207  |        0.071        |   0.178   |  0.178  |       0.000        |
| SAT score                                         |   671.3   | 794.868 |        0.230        |  712.224  | 712.861 |       0.001        |
| Missing SAT Score                                 |   0.376   |  0.317  |        0.122        |   0.347   |  0.347  |       0.000        |
| College credit exams                              |   0.708   |  1.135  |        0.222        |   0.806   |  0.848  |       0.022        |
| TOEFL                                             |   0.000   |  0.001  |        0.093        |   0.000   |  0.000  |       0.009        |
| Sibling attended college                          |   0.337   |  0.387  |        0.105        |   0.353   |  0.345  |       0.017        |
| Submitted one college application before outreach |   0.777   |  0.796  |        0.044        |   0.853   |  0.853  |       0.000        |
| American Indian/Alaskan Native                    |   0.005   |  0.003  |        0.021        |   0.003   |  0.003  |       0.003        |
| Asian                                             |   0.084   |  0.091  |        0.027        |   0.096   |  0.093  |       0.010        |
| Black                                             |   0.28    |  0.185  |        0.213        |   0.287   |  0.307  |       0.045        |
| Latinx                                            |   0.41    |  0.231  |        0.364        |   0.408   |  0.396  |       0.023        |
| Native Hawaiian/Pacific Islander                  |   0.003   |  0.002  |        0.011        |   0.002   |  0.002  |       0.011        |
| White                                             |   0.169   |  0.383  |        0.570        |   0.164   |  0.164  |       0.002        |
| Multi-racial                                      |   0.044   |  0.051  |        0.031        |   0.036   |  0.031  |       0.025        |
| Race unknown                                      |   0.004   |  0.026  |        0.328        |   0.003   |  0.004  |       0.006        |
| Non-resident                                      |   0.000   |  0.028  |        4.261        |   0.000   |  0.000  |       0.042        |
| URM                                               |   0.826   |  0.563  |        0.694        |   0.833   |  0.833  |       0.000        |
|                                                   |           |         |                     |           |         |                    |
| Num.Obs                                           |  142,827  | 263,299 |                     |  99,593   | 61,553  |                    |

: Balance of student characteristics {#tbl-balance }

\newpage

|                          |   Overall    | Before outreach | After outreach |
|--------------------------|:------------:|:---------------:|:--------------:|
| Application Differential | -0.015\*\*\* |      0.004      |  -0.013\*\*\*  |
|                          |   (0.003)    |     (0.003)     |    (0.002)     |
|                          |              |                 |                |
| Control Mean             |    0.861     |      0.848      |     0.036      |
| Num.Obs.                 |    161146    |     161146      |     161146     |
| R2                       |    0.318     |      0.324      |     0.079      |
| FE: school_id            |      X       |        X        |       X        |

: Impacts on submitting at least one college application {#tbl-submit}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement. Baseline covariates include fee waiver indicator, first-generation status, age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator, sibling college indicator, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance. Robust standard errors clustered at the school level, reported in parentheses.

\newpage

|                         |  Fall 2020   | Spring 2021  |  Fall 2021   | Spring 2022  | Gap semester | Gap year  |
|:----------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
| Enrollment Differential | -0.020\*\*\* | -0.031\*\*\* | -0.033\*\*\* | -0.038\*\*\* |    0.003     | 0.004\*\* |
|                         |   (0.004)    |   (0.005)    |   (0.005)    |   (0.005)    |   (0.002)    |  (0.002)  |
|                         |              |              |              |              |              |           |
| Control Mean            |    0.787     |    0.724     |    0.701     |    0.648     |    0.024     |   0.023   |
| Num.Obs.                |    161146    |    161146    |    161146    |    161146    |    161146    |  161146   |
| R2                      |    0.155     |    0.177     |    0.179     |    0.194     |    0.073     |   0.079   |
| FE: school_id           |      X       |      X       |      X       |      X       |      X       |     X     |

: Impacts on college enrollment outcomes, by term {#tbl-overenroll}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement.Baseline covariates include fee waiver indicator, first-generation status,age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator,sibling college indicator, submitted at least one CommonApp application before start of intervention, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance.The gap semester outcome is a binary indicator coded as 1 if a student did not enroll in Fall 2020 but did enroll in Spring 2021. The gap year outcome is also a binary indicator coded as 1 if a student did not enroll in Fall 2020 or Spring 2021, but did enroll in Fall 2022. Robust standard errors clustered at the school level, reported in parentheses.

\newpage

|                         | Fall 2020 | Spring 2021 | Fall 2021 | Spring 2022 | Gap semester | Gap year |
|:----------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
| Enrollment Differential |   0.008   |    0.030    |  -0.007   |    0.016    |    0.001     |  0.000   |
|                         |  (0.016)  |   (0.016)   |  (0.016)  |   (0.016)   |   (0.006)    | (0.006)  |
|                         |           |             |           |             |              |          |
| Control Mean            |   0.683   |    0.605    |   0.593   |    0.528    |    0.036     |  0.041   |
| Num.Obs.                |   14842   |    14842    |   14842   |    14842    |    14842     |  14842   |
| R2                      |   0.311   |    0.338    |   0.337   |    0.344    |    0.224     |  0.248   |
| FE: school_id           |     X     |      X      |     X     |      X      |      X       |    X     |

: Impacts on college enrollment outcomes, by term - Submitted no application prior to intervention {#tbl-overenroll_submit}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement.Baseline covariates include fee waiver indicator, first-generation status,age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator,sibling college indicator, submitted at least one CommonApp application before start of intervention, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance.The gap semester outcome is a binary indicator coded as 1 if a student did not enroll in Fall 2020 but did enroll in Spring 2021. The gap year outcome is also a binary indicator coded as 1 if a student did not enroll in Fall 2020 or Spring 2021, but did enroll in Fall 2022. Robust standard errors clustered at the school level, reported in parentheses.

\

\newpage

|                         |  Fall 2020   |    4-year    | 4-year public | 4-year private |   2-year    |  Full-time   |
|:----------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
| Enrollment Differential | -0.020\*\*\* | -0.040\*\*\* |    -0.007     |  -0.033\*\*\*  | 0.019\*\*\* | -0.029\*\*\* |
|                         |   (0.004)    |   (0.005)    |    (0.005)    |    (0.004)     |   (0.003)   |   (0.005)    |
|                         |              |              |               |                |             |              |
| Control Mean            |    0.787     |    0.671     |     0.457     |     0.217      |    0.121    |    0.668     |
| Num.Obs.                |    161146    |    161146    |    161146     |     161146     |   161146    |    161146    |
| R2                      |    0.155     |    0.207     |     0.173     |     0.141      |    0.145    |    0.175     |
| FE: school_id           |      X       |      X       |       X       |       X        |      X      |      X       |

: Impacts on Fall 2020 college enrollment outcomes {#tbl-20enroll}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement. Baseline covariates include fee waiver indicator, first-generation status, age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator, sibling college indicator, submitted at least one CommonApp application before start of intervention, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance. Robust standard errors clustered at the school level, reported in parentheses.

\newpage

|                         |  Fall 2020   |    4-year    | 4-year public | 4-year private |   2-year    |  Full-time   |
|:----------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
| Enrollment Differential | -0.021\*\*\* | -0.044\*\*\* |    -0.007     |  -0.037\*\*\*  | 0.022\*\*\* | -0.034\*\*\* |
|                         |   (0.005)    |   (0.006)    |    (0.007)    |    (0.006)     |   (0.004)   |   (0.006)    |
|                         |              |              |               |                |             |              |
| Control Mean            |    0.784     |    0.661     |     0.452     |     0.212      |    0.129    |    0.661     |
| Num.Obs.                |    97537     |    97537     |     97537     |     97537      |    97537    |    97537     |
| R2                      |    0.160     |    0.213     |     0.181     |     0.144      |    0.152    |    0.180     |
| FE: school_id           |      X       |      X       |       X       |       X        |      X      |      X       |

: Impacts on Fall 2020 college enrollment outcomes - Racially marginalized students {#tbl-20enrollurm}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement. Baseline covariates include fee waiver indicator, first-generation status, age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator, sibling college indicator, submitted at least one CommonApp application before start of intervention, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance. Robust standard errors clustered at the school level, reported in parentheses.

\newpage

|                         | Fall 2020 | Spring 2021 | Fall 2021 | Spring 2022 | Gap semester | Gap year |
|:----------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
| Enrollment Differential |  -0.017   |   -0.016    |  -0.011   |   -0.011    |    -0.001    |  0.006   |
|                         |  (0.010)  |   (0.011)   |  (0.011)  |   (0.011)   |   (0.004)    | (0.003)  |
|                         |           |             |           |             |              |          |
| Control Mean            |   0.787   |    0.724    |   0.701   |    0.651    |    0.025     |  0.022   |
| Num.Obs.                |   20527   |    20527    |   20527   |    20527    |    20527     |  20527   |
| R2                      |   0.282   |    0.295    |   0.303   |    0.315    |    0.212     |  0.222   |
| FE: school_id           |     X     |      X      |     X     |      X      |      X       |    X     |

: Impacts on college enrollment outcomes - Opt out {#tbl-20enrollopt}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: Opt-out was constructed by flagging students who explicitly messaged "STOP" to the bot at any point during the intervention and/or received a "goodbye" message from Oli, indicating that they had requested to opt out. Due to data quality issues we only have text message data for 70% of all treated students. Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement.Baseline covariates include fee waiver indicator, first-generation status,age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator,sibling college indicator, submitted at least one CommonApp application before start of intervention, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance.The gap semester outcome is a binary indicator coded as 1 if a student did not enroll in Fall 2020 but did enroll in Spring 2021. The gap year outcome is also a binary indicator coded as 1 if a student did not enroll in Fall 2020 or Spring 2021, but did enroll in Fall 2022. Robust standard errors clustered at the school level, reported in parentheses.

\newpage

|                         | Fall 2020 | 4-year  | 4-year public | 4-year private |   2-year    | Full-time |
|:----------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
| Enrollment Differential | 0.026\*\* |  0.002  |   0.032\*\*   |  -0.032\*\*\*  | 0.024\*\*\* |   0.005   |
|                         |  (0.009)  | (0.010) |    (0.011)    |    (0.009)     |   (0.007)   |  (0.010)  |
|                         |           |         |               |                |             |           |
| Control Mean            |   0.793   |  0.674  |     0.461     |     0.215      |    0.123    |   0.676   |
| Num.Obs.                |   26281   |  26281  |     26281     |     26281      |    26281    |   26281   |
| R2                      |   0.256   |  0.304  |     0.279     |     0.253      |    0.251    |   0.272   |
| FE: school_id           |     X     |    X    |       X       |       X        |      X      |     X     |

: Impacts of text message engagement on Fall 2020 college enrollment - High text engagement {#tbl-20enrolleng}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: High engagement indicator is coded as 1 for students who are in the top 25th percentile (sent more than 9 text messages) of total number of text messages students sent throughout the intervention. Due to data quality issues we only have text message data for 70% of all treated students. Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement.Baseline covariates include fee waiver indicator, first-generation status,age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator,sibling college indicator, submitted at least one CommonApp application before start of intervention, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance.Robust standard errors clustered at the school level, reported in parentheses.

\newpage

![Propensity score distribution](figures/ps_dist.png){#fig-psdist alt="Propensity score distribution" width="340"}
