# Paper 2: Navigating college enrollment during the COVID-19 pandemic: examining the effectiveness of an artificially intelligent chatbot in helping students navigate the road to college

## Introduction

In spring of 2020, schools and colleges across the US began to shut down and pivot to remote learning to protect students and their communities from the rapid spread of the COVID-19 virus. While all the while, high school seniors began to receive acceptance letters from the colleges to which they had applied before the pandemic. At the time, the effect of the pandemic on college-going was uncertain. On the one hand, the COVID-19 crisis hampered business-as-usual operating procedures for colleges. Because of this, students -- particularly those planning to live on campus -- may have been more likely to delay college entry. On the other hand, higher education is a counter-cyclical industry such that college enrollment rates tend to be higher during economic downturns. For recent high school graduates, in particular, the impact of the pandemic on service and retail, among other sectors, may severely limit job opportunities. This, in turn, may make higher education an attractive option for young adults who might otherwise enter the labor market after high school.

In response to this uncertainty, the Common Application (Common App), in partnership with Mainstay and the College Advising Corps (CAC), acted quickly to provide students with proactive outreach and guidance on college-going tasks via an innovative large-scale artificially intelligent (AI) chatbot campaign. This outreach targeted nearly 174,000 US high school students who were the first in their families to go to college *and* came from underresourced families, most of whom were racially marginalized (83%). Due to an array of systemic barriers, this subgroup of students likely faced reduced access to quality health care, living, and work conditions during the pandemic (CITE) and, therefore, would likely benefit from additional support during their precarious college transition.

Over 38 weeks, a Mainstay AI chatbot named *Oli* sent scripted messages to students on various topics related to the college search, application, and matriculation processes. To better target the information, Oli solicited information directly from students about the types of resources they needed and pressing questions they had. The chatbot "learned" from these students' interactions and developed a knowledge base to provide more targeted and individualized real-time responses. Student questions that Oli could not answer were forwarded to college advisers, who would follow up directly with individual students.

The present study aligns with prior research utilizing AI chatbots in the postsecondary sector. Recent literature has leveraged behavioral economics insights to evaluate the effectiveness of AI chatbots in helping students transition to and through college (CITE). These studies have demonstrated that AI chatbots deployed within a local university context are effective at helping students complete pre-matriculation tasks and ultimately enroll in college (Page..Gelbach..Aizat..2017;2020;2021).

However, questions remain about the scalability of these AI chatbot interventions. Prior research has highlighted that similar nudge interventions -- not employing chatbots -- fail to maintain efficacy when scaled to a national level and target hundreds of thousands of students (CITE). Therefore, my second dissertation study aims to add to the literature on the scalability of AI chatbot campaigns.

Toward this aim, my second dissertation study evaluates the effectiveness of a large-scale AI chatbot campaign on first-generation, low-income students' college-going outcomes. Specifically, I focus on whether the outreach campaign resulted in more students applying to and enrolling in college. In addition, I explore heterogeneity in outreach impact on college enrollment by students' application behavior, racial/ethnic subgroups, and level of engagement with the chatbot.

In the following sections, I review the relevant literature for this study, followed by a description of the data and the analytical strategy that I employ. I end by presenting preliminary results of the evaluation.

## Literature Review

In this section, I review the literature on college access, both in general and during the COVID-19 pandemic. In addition, I draw on literature addressing barriers to college access for high school students through behavioral nudge chatbot interventions.

However, before reviewing the literature, I want to define how I conceptualize first-generation students. I take an anti-deficit approach to my grounding of first-generation students -- many of whom also tend to be economically and racially marginalized (CITE). Anti-deficit practices explore how marginalized students persist in the face of social and structural barriers (CITE, Harper). The goal is to reframe students' experiences from struggling to ones of resiliency. Since most deficit approaches focus on what students lack, anti-deficit strategies focus on what students bring that leads to their resiliency and persistence. Although I take an anti-deficit stance in my review of the literature, I acknowledge that most existing literature around college access for first-generation students is written from a deficit perspective (CITE).

### College access and the returns to a college education

Despite increasing access to higher education in recent decades, there remain disparities in college attainment by socioeconomic status and parental education (CITE). Students whose parents received a college degree enroll in college at higher rates than those parents who did not complete high school (CITE). Additionally, students from higher-income backgrounds are more likely to enroll and complete postsecondary education than students from less-resourced families (CITE). For example, around 13% of students from the lowest income brackets earn a bachelor's degree by age 24, compared to 64% of students from the highest income brackets (CITE). Yet, much of the quantitative literature fails to acknowledge systemic barriers and the role of US policies in creating these disparities.

Lower-income and particularly racially marginalized people face structural challenges in accessing quality education, housing, and working conditions (CITE). For example, decades of federal redlining in housing and the policy of using local taxes to fund schools creates raced and classed outcomes in educational opportunities and family wealth (CITE). Therefore, disparities in college access and completion by parental education -- along with social and structural barriers -- are vital contributors to the growing income inequality in the US.

Despite the rising cost of college (CITE), the non-pecuniary and long-term economic returns to a college education remain positive. Adults who earn a bachelor's degree tend to earn and accumulate more wealth than those who have not completed college (CITE). On average, individuals with a college degree have lower unemployment rates, higher civic involvement, and better health outcomes (CITE). However, researchers note that the rate of economic return is substantially higher for continuing education students as opposed to first-generation students (CITE).Given that more than half of all college students identify as first-generation (56%), there has been a substantial policy focus on supporting college-intending first-generation students enroll in college (CITE). However, the COVID-19 pandemic has added additional complexity to the college enrollment process.

### College access during the COVID-19 pandemic

The COVID-19 pandemic increased the uncertainty for high school seniors who planed to enroll in college in the fall of 2020. In the early stages of the pandemic, one in six recent high school graduates reported rethinking their decision to enroll in college, and nearly two-thirds expressed concern about attending their first choice college (CITE). The most cited reason for not enrolling in their first-choice school was concern about the ability of their families to afford college (CITE).

Now, nearly three years into the pandemic, we have some evidence of the detrimental effects of the pandemic on college-going. Enrollment for first-time undergraduates in fall 2020 fell to unprecedented levels, dropping nearly ten percentage points compared to fall 2019, with higher rates for students from low-income high schools (CITE). Furthermore, students from low-minority schools were more likely to immediately enroll in college than those from high schools with more diverse student bodies, 64% versus 52%, respectively (CITE).

A recent report from the College Board provides some additional details on the detrimental effects of the pandemic on college enrollment. Drawing on data from nearly 10 million US students, the College Board calculated high-level descriptives and regression-adjusted models identifying the portion of fall 2020 enrollment rates attributable to COVID-19. They find that the most significant change in student enrollment occurred within community colleges, where enrollment rates declined by nearly 12% due to the pandemic. Most worrying is that enrollment rates fell mainly for first-generation low-income students (CITE).

A likely reason for the drops in college enrollment rates for first-generation students is the pandemics' disproportionate effect on racially and economically marginalized communities. During the pandemic, these communities experienced reduced access to high-quality healthcare and working conditions and lower expected wages (CITE). Before the pandemic, the typical Black and Latinx household had a net wealth of approximately $17,000 compared to $171,000 held by the typical white household. Therefore, Black and Latinx households may have found it more difficult to escape the economic shock caused by the pandemic. These households had to grapple with less liquidity and wealth to cover unexpected expenses or loss in wages (CITE, center for american progress). This, in turn, may have caused first-generation students to prioritize immediate health and economic stressors, such as supporting their families financially or dealing with lost wages, instead of prioritizing college enrollment. Data from the College Board may support this potential mechanism. Students from higher-income households were far more likely to enroll in college immediately after graduation (65%) than those from low-income high schools (49%). Additionally, first-generation students were nearly twice as likely to be concerned about paying for education expenses in fall 2020 than continuing education students (CITE). In sum, the literature highlights the unprecedented shift in college going caused by the COVID-19 pandemic, particularly for first-generation, economically, and racially marginalized students. Next, I review the literature on well documented challenges to college access.

### Challenges in college access 

Considerable research attention has been paid to barriers in college application, financing, and enrollment processes, especially for students who are the first in their family to attend college (CITE). To apply for college, students must complete a series of complex tasks such as developing a list of colleges, deciding on which colleges to apply to, and applying for financial aid through a cumbersome financial aid application (the Free Application for Federal Financial Aid - FAFSA). Students typically access federal, state, and grant aid -- aid that *does not* have to be repaid -- by completing the FAFSA form. The FAFSA has more than 100 questions that ask students about their and their families' financial assets.

Many scholars have stressed the importance of supporting students with completing and submitting the FAFSA, given a sizable portion of first-generation students come from less-resourced families and, therefore, must rely on financial support to finance their postsecondary aspirations (CITE). Scholars have argued that due to federal and state divestment from funding postsecondary institutions, the focus has changed from need-based grants -- money that a student does not have to pay back -- to merit-based aid and loans (CITE). This shift has negatively impacted first-generation and low-income students (Long & Riley, 2007).

In a series of studies Page and collegeus (CITE) have shown how supporting students with submitting the FAFSA has lead to positive college enrollment outcomes. For example, Page and colleagues (2020) conducted a randomized trial that provided students with timely, personalized reminders about the importance of the FAFSA in the summer before college. They find that this support improved students' successful completion of the FAFSA (CITE). However, even after a student completes these steps they still have a lot more complexities after the accep admission.

After completing these steps, students must weigh competing admission offers, including complex financial aid award letters (CITE). Finally, after a student accepts an admission offer, students must navigate the equally cumbersome college enrollment process. Students must complete a series of pre-matriculation administrative tasks, including submitting high school transcripts, deciding which and how much aid to accept, taking placement exams, and successfully enrolling in classes (CITE). Scholars have noted that the summer before a student enrolls in college is an exceptionally cumbersome time when students need additional support to complete these pre-matriculation tasks (CITE). If a student fails to complete any of these steps, they may ultimately fail to enroll in college.

### Unique challenges in college access among first-generation students

However, for first-generation students, there is an added layer of complexity in navigating these tasks with little to no access to social and cultural capital around the college application process. For instance, scholars have emphasized how first-generation students tend to reside in schools with less access to college prep or Advanced Placement (AP) courses, which have been positively associated with college enrollment (CITE). In addition, first-generation students attend schools with little to no access to college counselors, which could support students in completing college applications and navigating the complex financial aid process (CITE). However, even when students have access to a college counselor, these counselors are burdened with high caseloads and cannot provide individualized support to students (CITE).

Yet, first-generation students of color must also endure social and systemic barriers that traditionally left them with less access to high-quality primary and secondary education. For example, college opportunities are influenced by education policy that track racially funnel students in to underserved school, providing these student with limited access to college prep and andvanced placement coursework, and reduced access to quality college counseling (Oakes, 2005; Oakes, Rogers, Lipton, & Morell, 2002; Solorzano & Ornelas, 2002, 2004)through tracking policies that funnel them to courses that don't qualify as prerequisites to apply to college (CITE). In addition, first-generation students of color have historically had less access to intergenerational wealth to finance college and must rely on financial aid to fund their postsecondary education (CITE).

### Behavioral nudge interventions in college access

The rich body of literature has proposed a few mechanisms that may explain why students may ultimately fail to enroll in college, the predominant one being information asymmetries around college going. Students may fail to enroll because of minimal information and guidance in navigating pre-matriculation tasks. This is especially salient for first-generation students that may not have family members that could help them with the college transition. Scholars note that students tend to falter in certain steps. These include missing critical deadlines for financial aid or failing to submit transcripts or other documentation. Therefore, an array of researchers have developed interventions that examine ways to support students in the key steps and deadlines to enroll in college by providing high-quality college primarily, but not exclusively, through behavioral nudge interventions.

One favored approach to remedying students' information asymmetries in the college-going process is access to high-quality college advising that can support students' transition to college. However, many first-generation students reside in schools with little to no access to college counselors, likely due to systemic barriers such as federal redlining policies (CITE). Even when these students have access to college counselors, those counselors are overworked due to large caseloads (CITE).

The liteature points to the positve effects of college advisos is associated with enrolling in college at higher rates, and postively associated with persistance trhough college. For example Castleman et al., 2012 randomized students in secon Rhode Island high schools to received high quality one on one college counseling. This "high touch" intervention resultted in a increate in students imediate college enrollment by 14 percentage points. Although this intervention occured in a relatively diverse schools, all school sin the sample where choice schools and therefore the results may not generalize to public schools. However, in 20 CAstlman and Page look at similar efforts with a nationally representative sample of boston public school students.

However, in the backdrop of globabl pandmic access to qualify advicing was heavily impacts. Naughton (2021) describes the chanlledges to college advising during covid-19 as craks turning into craters. Naughton find that college advisors in their study felt inefective at providing virtual support and guidance once high school transitioned to virtual. They experienced limited to little reach to the students. Naughton reasons that advisers in high schools with higher rates of college going wer less impact than advisrs who were woring in schol with the highest need, as advisers were unable to get inforation to students during the postseocndry transition. The longstandingn "cracks" om the transtion to college were made into unpassable "craters" primarily for first-generaiton, lo-income, racially marginalized students.

Lack of access to these support may make students put off important college going task or simply be unaware of deadlines, such as completing finacil aid applicaitons to scucess matriculating into college.

Therefore appraoches that bring college advising directly to students who need it most is crucial. Once such low-cost way that has ben prosposed has been low-cost nudge interventions based on beahvioral economisc (CITE). These intervention distill complex information and deadlines into short concise megaing delived to students via a mode of communication studenets are well away of, text messages. In the domain of college access these studies have primarily focused on helping student vaigtate well-defined but complex task such as compleitng finalcial aid applicaiton, submitting trasncipt, accepting loand and paying tuition.

A number of randomized trial in higher educaiton have evaluated the use of thsese sort of nudge interventions to remedy summer melt. Such as givign students access to a highly trained college counselor (CITE). these effors improved student on-tine colleg enrollment.Canstleman and Page have undetakern several large randomized controll studies to support college-intending high school student with college going tak and comple the FAFSA

TALK about study

Castleman & Page, 2015, 2017; Castleman, Page & Schooley, 2014) and to file or refile the FAFSA (Castleman & Page, 2016; Page, Castleman & Meyer, 2019).

Hoewever, the research is mixed in the effectivenss of these interventions when they are scaled up. Oreopeoplou et al. find no effect of text message outreach on college studenet grades or completion or when the entity dong the messaging is unfamiliar to the student. for exmaple they find that FAFSA submision are null.

Although these intervention find positive effect, the scalability and const prove to be a problem. On average these cousenlor and text based interventions costt... In additional one would need a large amount of counselors to provide personliaze 1:1 support.

## Scaling of these intervention Bird et al.

### AI Chat-bots in college access

A recent push in th enudge literature has been to levelrea artifial intelligence (AI) to provide personalized support to students navigating college applicaiton and enrollment. These AI bots have primarily been teasting using AI chatbots. These chatbots are no different that the boths that many business such as cell phone companies use to provide supoprt to customers tryign to access their account, or buy a cellphone. These AI bots are trained by human superision to provide suport to students. Overtime the AI "learns" how to handle increasinlgy complex questions such as XXX. Once sucessfull trained and deployed to student the AI can help student andwer quesitons around important deadlines to when they should submit their transcripts. These chatbots are able to provide sutdent real-time responses based on prior students bot interactions and information provided by the institions.

A series of recent studies conducted by Page, Gelhbach and collegeues have exmployed AI chatbots to help student enroll and navigate college (CITE). In the first studdy Page and Gelbach (2017) used in AI chatbot named "Pounce" located in Georgia Stata Universtiy (GSU) helped freshmanstudents who were accpeted to join GSU in fall 2017 with on time matricutiona and pre-matriculation task at GSU. The use of the chatbot lead to a 3.3 percentage point increase in timenly enrollment at GSU as well as well as hlped students with pre-matriculation class like signing up for orientation. Note this study was situatiod at the instituion where students were planning to enroll. This centralized nature of the intervention is important even among regular nudge interventions. This on is on adminsitrative processes

Nushatayeva and collegesfurther exten the work of Page and gelbach 2017 by testing if AI chatbots are parttucala hlepuf for cetain student subgroup (e.g. ). This study took place at EAst Carolina Univesicy ECU with an AI chatbot names PeeDee. The authors found no overall effect of the chatbot on college enrollment, but did find an effect on loan acceptance rates. The authors point to realtive advanataged student body and ECU to explain their null effect on college enrollmentcompared to overall positive effect on college enrollment found in the GSU study. However, authors did find a postive treatment effects for first-generation students. PeeDee lead to an increace of enrolling at ECU by 3 percentage points for first-generation students, and an increase of completing pre-matriculation task such as accpeintg a loan (8 percentage point incres), and registerign fro class (3 percentage point increase).

Finally in a follow up study by Page and colleges (20202) shifted the focus of the study from helping students enroll at GSU to helpoing students navigate the administrative process after student arrive at GSU. This study specifically, focused on acadmeic support, social and carrer support and administrative processes. This study was employed as a randomized control trisl Through a RCT Page and colelges foudn taht outreach was most effective when the chatbot was focused on adminsitrative process, particualry those that were time sensityve. However, when oureach that focused on suplmente academic, social and carrerl-related suport, such as meeting with an advisor was not found to be effective. The authors conclude that the AI-bot outreach was most effective and changing student bahvior when the bot was focused on discrete, well-defined administion tasks in which a lack of action lead to high consequences, such as resolving registration holds.

Taken together these studies iluminate that AI bots can be an effective tool for helping students transtion into and trhough college, specifically when dealing with adinistratie tasks. However, chat-bot based nudges are not uniformaly effective across various context. These interventions were most effective when providing support to first-generaiton students. An important aspect of these studies to highligh is that these chatbots were housed within a receiving insittions like GSU *not* at a wider level. Little is know about the scalability of these efforts outside universities and at system or national levels.

Page, L. C. & Gehlbach, H. (2017). How an Artificially Intelligent Virtual Assistant Helps Students Navigate the Road to College. AERA Open, 3(4), 2332858417749220. https://doi.org/10.1177/2332858417749220

Page, L., Lee, J. & Gehlbach, H. (2020). Conditions under which college students can be responsive to nudging. EdWorkingPaper.

The barriers mentioned above to college access exist due in part to US postsecondary institutions -- and society at large -- reflecting the norms of the dominant, white, middle-class. These norms assume that students have access to knowledge in navigating the complex higher education system (Perna, 2006, CITE, Tinto). This lack of understanding of the college-going process may cause a student to face a "cultural clash" when navigating into and through college as they must learn the complex process of navigating the college access pipeline from individuals outside their families or immediate social groups (CITE).

## Methods

It is important to note that the outreach team did not randomly assign the outreach for this campaign. Randomly assigning the outreach would ensure that students who received the outreach and those who did not would be balanced on both observable *and* unobservable characteristics. Given the lack of randomization, a simple comparison of outcomes between students who received the outreach and those who did not may -- incorrectly -- conclude that the outreach affected the outcome. To guard against this to the fullest extent possible, we matched students in the outreach group to demographically similar students in the same high school who were not treated. These matched students are then observationally similar to students in the outreach group and, therefore, a reasonable comparison group. Analytically, we do this matching via a procedure termed propensity score matching.

### Matching

Propensity score matching is used in non-experimental studies (i.e., studies in which students were not randomized to outreach or control) to balance the observed characteristics between treated and untreated students. It involves estimating a *propensity score*, which is the probability that a student would have been exposed to outreach, conditional on a set of observable characteristics. This propensity score is then used to match treated students to control students with similar estimated propensity scores, resulting in a control group that is demographically similar to the outreach group. Propensity score matching allows us to more accurately evaluate the impact of the outreach than a simple comparison of outcomes between those students who received the outreach and those who did not.

We estimated the propensity score as a function of student-level characteristics available from the Common App data, including: include race/ethnicity, age, gender, English spoken at home, dependent status, number of high schools attended, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator, whether they had a sibling in college, and whether they submitted at least one Common App application before the start of outreach.

To increase balance on our covariates, we placed additional restrictions on our matching algorithm to exact match on specific variables, such as students' high school. Exact matching forced the matching algorithm to *only* find potential matches for treated students in the same high school. This allowed us to control for the fact that students across different high schools have varied school experiences; therefore, matching within schools provides more reasonable matched control students.

Given the algorithm we used, it is possible for one control student to be a perfect match for more than one treatment student. Therefore, we allowed the matching algorithm to match an outreach student to multiple control students. Where this occurred, the control student was up-weighted in our analysis to essentially be "counted" as a comparison for numerous treated students. After concluding our matching, we had a final outreach group of 99,593 students matched to 61,553 control students. @tbl-balance presents balance on student characteristics before and after our matching procedure. After matching, we found no difference above 0.1 standardized mean difference, a commonly accepted benchmark for achieving balance. For a technical explanation of our matching approach, please refer to Appendix A.

## Data

Common App and partners selected for outreach all students from the high school class of 2020 who met two specific criteria: 1) they would be first-generation college students, and 2) they had a low family income, as indicated by qualifying for a Common App fee waiver. In total, the outreach targeted nearly 174,000 students in the class of 2020.

We received student-level data from the Common App for the entire cohort of students who had created a Common App account and had intended to apply to college -- hoping to enroll in fall 2020 -- including students who were selected for the outreach $(n = 173,776)$ and those who were not $(n = 1,229,232)$, for a total of nearly 1.5 million student records.

To evaluate the effectiveness of the outreach campaign, our analysis relied on matching students selected to receive outreach to similar students in the same high school who were not selected for outreach. In particular, some Common App students met one but not both criteria for inclusion in the outreach (i.e., first-generation college student *or* qualified for a fee-waiver). We use these students as our key source of comparison in the analyses. Therefore, our outreach ("treatment") group includes students who met both inclusion criteria (i.e., first-generation college student *and* qualified for a fee-waiver) while our control students met one but not *both* criteria.

Once we cleaned and subset the data to match the outreach inclusion criteria, we arrived at a final analytic sample of 142,837 students who were targeted for the outreach and 263,399 students who were not treated and therefore qualified as potential control students.

## Analytic strategy

In this appendix, we provide a technical explanation of our analytic approach to constructing our matched control group using propensity score matching. Propensity score matching allows us to balance observed characteristics between treated and untreated students, leading to a more accurate estimate of the impact of the outreach.

We begin by estimating a propensity score, defined as the probability that a student would have been exposed to outreach, conditional on a set of observable characteristics. Our propensity score model took the following general form @eq-int-ps:

$$
Outreach_{is} = \beta_{0} + \beta_{1}X + \lambda_{is}
$$ {#eq-int-ps}

Where $outreach_{i.s}$ represents a binary indicator coded as 1 for students who received the outreach and 0 if a student did not. $X$ is a vector of student level characteristics. These include age, gender, English spoken at home, dependent flag, number of high schools attended, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator, sibling college indicator, submitted at least one Common App application before start of outreach, and race/ethnicity indicators.

We also included $(\lambda)$, which represents a vector of covariates for which we exact matched students on. These include students' high school, underrepresented minority indicator, an indicator for submitting at least one Common App application before the start of outreach, and missing indicators for high school GPA and SAT/ACT performance. Exact matching on these variables allows us to match outreach students to control students who *exactly* match them on the set of variables.

It is possible for one control student to be a perfect match for more than one treatment student. Therefore, we allow the matching algorithm to match an outreach student to multiple control students (i.e., matching with replacement). Additionally, we restrict matches to within .50 standard deviations of a propensity score. This allows us to guard against having treatment students matched with control students with a large difference in propensity scores.

To examine how well our matching procedure was able to balance student characteristics, we look at the overall distribution of the propensity score among treatment and untreated students as well as covariate balance before and after matching. @fig-psdist presents the propensity score distribution between treatment and control students before and after matching. Overall, our matching procedure did an excellent job matching treated students to control students, as indicated by the overlap in the distributions on the right panel.

In addition to analyzing the distributional differences in the propensity score, we also looked at baseline characteristics between treatment and control before and after matching. @tbl-balance compares balance of student characteristics before and after matching. Overall, control group students (who are either "low-income" or "first generation" but not both) had higher GPA class rank (70.1% vs. 64.3%) and SAT scores (794.9 vs. 671.3) than treatment group students (who are both "low-income" and "first generation"). The treatment group also included a substantially larger percentage of minority students (28% Black and 41% Latinx) than the control group (19% Black and 23% Latinx). We achieve excellent balance on all covariates included in our matching approach, with no standard mean difference below 0.1. Our final matched sample includes a treatment group consisting of 99,593 students and a matched control of 61,553.

To estimate the impact of outreach on student outcomes, we ran a series of regression models. Specifically, we regressed each outcome on an outreach indicator and student-level characteristics. We also included weights to account for our matching approach.[^03-commonapp_over-1] Our regressions took the following general form[^03-commonapp_over-2] @eq-out:

[^03-commonapp_over-1]: Matching weights were constructed to account for our matching approach that allowed multiple control students to be matched to treated students. Each treated unit gets a weight of 1, while each control student is weighted as the sum of the inverse of the number of control units matched to the same treated unit across its matches.

[^03-commonapp_over-2]: We used robust standard errors clustered at the high school level.

$$
Y_{is} = \beta_{0} + \beta_{1}Outreach_{is} + X\theta + \lambda_s + \epsilon_{is}
$$ {#eq-out}

Where $Y_{is}$ represents our outcome of interest for student $i$ in school $s$. $Outreach$ is a binary indicator coded as 1 for students who received the outreach and 0 if a student did not. $X$ is a vector of student level characteristics such as age and gender. In addition we included high school fixed effects $(\lambda)$ which soak up any remaining variation in the outcome due to differences across high schools. $\beta_{1}$ is our coefficient of interest and represents the mean controlled difference in the outcome between those who received the outreach and those who did not.

Note that although we took a robust matching approach in constructing our control group, there could still be differences between our outreach and control group on student-level characteristics. Therefore, by including the vector of student level covariates $(X)$ in our regressions, we can account for any remaining imbalance between student characteristics that we observe and increase the precision of our impact estimates.[^03-commonapp_over-3]

[^03-commonapp_over-3]: This is commonly referred to as a "doubly robust" approach.

### Limitations

While this matching procedure was successful, we want to highlight the limitations in our ability to attribute a causal relationship between the outreach and the effects we estimate. Given that students were not randomly assigned to the outreach, we cannot make causal claims about the effectiveness of the outreach. However, our robust matching approach allows us to reduce some -- but not all -- the bias that could exist in explaining the relationship between receiving outreach and outcomes. Note that we are only matching on *observable* characteristics. There could be *unobservable* characteristics that the matching procedure cannot consider that could influence outcomes absent the intervention.

Furthermore, important observable differences do remain between the outreach group and the comparison group, even after matching. Students selected for outreach were first-generation *and* low-income, while control students were either first-generation *or* low-income. The outreach team selected students in this manner to reach as many students as possible, specifically students who would likely benefit from this sort of outreach. Although this was a wise approach for targeting students in need of support, it created some analytic obstacles for our impact analysis that can't be remedied through matching. In particular, in any matched pair of low-income students, the treatment group student is first-generation and the control group student is not, and similarly, in any matched pair of first-generation students, the treatment group student is low-income and the control group student is not.

Students in the outreach group have two factors that -- due to an array of systemic issues -- may disadvantage them in terms of college enrollment. Therefore, we may expect that -- even absent the intervention -- students in the outreach group likely have lower college enrollment rates than those who did not receive the outreach. Given how the outreach groups were defined, we cannot observe college enrollment outcomes for control students who were first-generation *and* low-income, which would be the natural comparison for our outreach group. This is because, for the class of 2020, the intervention was targeted to *all* students who met these two criteria. After presenting our findings, we discuss two different strategies we used to investigate what the differences between these two groups might have been absent the chatbot outreach. Overall, we find that students who met both factors had somewhat lower enrollment rates than those who exhibit only one of the two factors

## Preliminary Results

In the following section, we present key findings from our impact analysis. We primarily focus on college application submission and enrollment in the fall term after students graduate high school. Additionally, we analyzed whether effects varied according to student characteristics and engagement in the chatbot communication. We specifically look at impacts for racially marginalized students, students who opted out of the chatbot outreach, and students who had a high level of engagement with the chatbot.

### College Application Submission

> *What is the impact of being targeted for outreach on college application submission?*

@tbl-submit shows the impact of the outreach on submitting at least one college application via the Common App. The *Differential* row refers to the difference in the mean outcome between those students who were targeted for the outreach and their demographically matched controls. For example,the first column shows the impact of the intervention on overall college application submission. Students targeted for outreach had somewhat lower application submission rates than their matched controls. 86.1% of the control group (see *Control Mean* row) submitted at least one college application, while 84.7% of outreach students did so. This differential is equal to around -1.5 percentage points.

Next, we analyzed whether impacts varied based on whether a student submitted at least one college application before or after the start of the intervention. The outreach had no impact on submitting at least one application prior to the start of the intervention, as expected. However, outreach students were less likely to submit an application after the start of the intervention (-1.3 percentage point difference).

### Overall College Enrollment

> *What is the impact of the outreach on college enrollment?*

Next, we present the effects on college enrollment outcomes (@tbl-overenroll). Students in the outreach group had somewhat lower fall 2020 enrollment rates compared to the control group. 76.7% of students in the outreach group enrolled in college in fall 2020, compared to 78.7% of control students, a 2 percentage point differential. We observed effects of a similar magnitude across all other terms.

Additionally, we explored if the outreach impacted whether a student took a gap semester or year. The outreach did not seem to impact whether students took a gap semester. However, we observed a slight increase in outreach students taking a gap year. Olli and the CAC advisors provided no information or advice about taking time off before enrolling in college, so it seems unlikely that the outreach influenced these choices given the underlying uncontrolled differences between treatment and control groups.

We further unpacked enrollment impacts by subsetting our sample to only students who had submitted no college applications prior to the start of the intervention. Given the timing of the outreach in late spring, these students missed most of the college application deadlines and therefore could benefit from this kind of outreach. However, the outreach had no impact across all enrollment outcomes (@tbl-overenroll_submit).

### Fall 2020 College Enrollment

The Common App and its partners were primarily interested in immediate college enrollment after students completed high school. Therefore, we analyzed the impact of the outreach on fall 2020 enrollment by 4-year versus 2-year institutions, private versus public, and full-time enrollment (@tbl-20enroll). The somewhat lower enrollment we observed for outreach students compared to students who did not receive the outreach was likely driven by students forgoing enrolling in 4-year institutions -- specifically 4-year privates (-3.3 percentage point difference) -- and enrolling in 2-year institutions (1.9 percentage point difference).The CAC advisors sometimes suggested two-year colleges to students who expressed concerns about college affordability but did not otherwise take any position on choosing between a two-year and a four-year college.

Additionally, outreach students had lower full-time enrollment rates -- 3 percentage points -- than control students (66.8%).

### Racially Marginalized Students

Next, we examined whether the outreach had a differential impact on students who belonged to a racially marginalized group. We defined a student as racially marginalized if a student self-identified as non-white or bi/multi-racial.

In @tbl-20enrollurm we present the impact of the outreach on fall 2020 college enrollment outcomes. We observed impacts of a similar scale as the entire sample. Racially marginalized students in the outreach group had relatively lower fall 2020 enrollment. There was a modest negative difference in enrollment in fall 2020 of around 2.1 percentage points, specifically driven by outreach students not enrolling at 4-year institutions by around -4.4 percentage points, but instead enrolling at 2-year institutions (2.2 percentage point difference).

### Outreach Participation

Using data from the qualitative analysis team about student engagement with the bot, including whether students decided to opt-out of receiving outreach from Oli. We constructed high engagement and opt out measures. Specifically, we defined high engagers as a student in the top 25th percentile of the total number of text messages sent throughout the outreach. We created the opt-out indicator by flagging students who explicitly messaged "STOP" to Oli at any point during the outreach or received a "goodbye" message from Oli, indicating that the student had requested to opt-out .[^03-commonapp_over-4]

[^03-commonapp_over-4]: Due to data quality issues, we received text message data for 70% of the outreach group. Furthermore, engagement across the intervention was relatively low. On average, students sent around 8 messages throughout the 38 weeks of the outreach. For a thorough explanation of the text analysis, please refer to Part II of this report.

### Opt-Out

Throughout the outreach campaign, students had the option to opt-out of receiving outreach by directly messaging Oli. Around 16% of students requested to opt out. In @tbl-20enrollopt we examine to what extent impacts varied among students who opted out. Across all outcomes, we see no effect on college enrollment. This finding is not surprising, given that most of the students who opted out did so in the first few weeks of the intervention and therefore received little to no communication from the chatbot.

### Oli Engagement

A final question we explored was whether impacts varied for those who engaged highly with Oli, i.e., those in the top 25th percentile of total messages sent throughout the outreach. Due to the low engagement throughout the intervention, a student who sent more than 9 messages was flagged as a "high" engager, which equaled around 17% of all outreach students.

@tbl-20enrolleng presents results for students with high engagement throughout the outreach. As opposed to our overall modest negative impacts for the whole sample, here we found a slightly positive impact on fall 2020 enrollment for highly engaged students, around a 3 percentage point improvement over the matched controls. Students who engaged with Oli at a high rate were, on average, more likely female, came from a racially marginalized group, and had slightly lower SAT/ACT performance than students who didn't have high engagement with the chatbot. Furthermore, these students also had higher rates of college application submission than non-highly engaged students.

### Enrollment Differentials in Context

We want to underscore that important, observable differences remain between the outreach group and the comparison group -- even after matching. As mentioned previously, the students selected for outreach were first-generation *and* low-income, while the control students were first-generation *or* low-income. This presented analytic obstacles in estimating outreach impacts.

In this section, we outline two different strategies we took to investigate what differences between these two groups might have been absent the chatbot outreach.

#### High School Longitudinal Study of 2009

First, we used the High School Longitudinal Study of 2009 (HSLS-09), a nationally representative sample of 9th graders in 2009 who were observed through 2016. The HSLS-09 dataset includes information on college-going. We subsetted the HSLS-09 data to a sample of students who had indicated college interest in 9th grade and had submitted at least one college application. This allowed us to mimic -- although imperfectly -- our 2020 Common App cohort of students who were college intending.

Next, we calculated enrollment differentials for first-generation *and* low-income students and first-generation *or* low-income students. In our 2020 cohort, low-income was defined as qualifying for a Common App fee waiver. Given the information available in the HSLS-09, we defined low-income as a student's family income falling below 130% of the poverty line (i.e., qualified for free or reduced lunch).

Overall, we found that students who were first-generation and low-income had lower college enrollment rates than students with only one of those characteristics. Students who were first-generation *and* low-income had a college enrollment rate of 79.1%. In comparison, students who exhibited only one of these factors had an enrollment rate of around 84.2% (a -5.1 percentage point difference).

Drawbacks of relying on the HSLS-09 as a source for informing this differential include its age (it observes a cohort of students who completed high school several years prior to 2020) and differences in how we define the subsample of students, relative to the focal groups of interest in our analysis of the Common App data.Therefore, we turn to another data source to generate an additional comparison.

#### Common App 2021 Cohort

During the 2021 school year, Common App conducted an unrelated randomized controlled study (RCT) with a cohort of students similar to our 2020 sample. We were able to capitalize on the sample employed in this RCT to further investigate enrollment differentials. We worked with Common App to receive high-level descriptives for the cohort of students who did not receive the 2021 intervention. Specifically, we examined fall 2021 enrollment overall and disaggregated by 4-year versus 2-year institutions for students in the 2021 study control group. These differentials allowed us to observe enrollment outcomes for first-generation *and* low-income students and first-generation *or* low-income students from the class of 2021. Unlike the HSLS-09 dataset, the 2021 Common App dataset allows us to define low-income similarly to our 2020 cohort (i.e., qualified for a Common App fee waiver).

Overall, we found that students who met both factors had somewhat lower enrollment rates than those who exhibit only one of the two factors. Students who were both first-generation *and* low-income had a fall 2021 enrollment rate of 72%, while students who only exhibited one of these factors had a 73% enrollment rate (a -1 percentage point difference).

Ideally, we would have matched students in our 2020 sample to demographically similar students in the 2021 sample, allowing us to estimate the group differentials and adjust our impact estimates directly. Unfortunately, this was not possible due to data sharing agreements. However, we were able to reduce the 2021 sample to students who attended high schools in our final matched 2020 sample. We found a similar enrollment differential in fall 2021 of around -1 percentage point.

Based on the differentials of both these data sources, we conclude that -- absent any intervention -- students who are first-generation *and* low-income have lower college-going rates than those who only hold one of those identities. Therefore, we reason that absent the intervention we would expect that students in the outreach group would have lower college enrollment rates than the comparison group, with differences on the order of -1 to -5 percentage points. These differences suggest that to find a positive effect, the impact of the outreach would have had to be larger than these differentials. Therefore, the modest negative enrollment for outreach students we estimate for the entire sample (-2 percentage points) likely reflect these differentials -- and not -- detrimental effects of the chatbot outreach.

## Looking Ahead

NEED TO TALK ABOUT IN FUTURE WORK THAT I PLAN TO DO SENSITIVITY ANALYSIS

The outreach undertaken by Common App and its partners cast a wide net in hopes of helping students through the typically challenging transition to college during the uncertainty of the COVID-19 pandemic. As part of our evaluation, we conducted a quantitative impact analysis to investigate whether the outreach improved students' college-going outcomes and also a qualitative text analysis to explore students' engagement with the chatbot and CAC advisers. As discussed in detail throughout this report, the core analytic approach to our impact analysis relied on comparisons between observationally similar students who nevertheless differed in a critical way. Those in the treatment group were both first-generation *and* from low-income households. Those in the comparison group had one both not both of these characteristics.

Historical evidence (see page 10) suggests that low family income and status as a first-generation college student serve as complementary risk factors for not enrolling in college. On average, high school seniors who are first-generation *and* low-income are less likely to enroll in college than those who are either first-generation *or* low-income but not both.

While the design choice to provide outreach to all students in the graduating class of 2020 directed services to the subgroup of students that were likely most at risk in the early stages of the pandemic, it also adds an additional layer of complexity to the task of evaluating the effect of this outreach. Since students with two risk factors all received outreach, any contemporaneous comparison group will consist of students facing fewer barriers on average to college enrollment.

In the absence of outreach, students in any comparison group can be expected to be more likely to enroll in college than those selected for the intervention. That is, a typical "treatment" vs. "control" group comparison likely leads to bias and an underestimate of the effects of the outreach on college enrollment. The large size of the intervention and the availability of Common App data for students who were not offered outreach enabled us to estimate the difference in enrollment rates for "treatment" vs. "control" groups with standard errors well less than 1 percentage point. Still, large sample sizes cannot compensate on their own for underlying differences in these two groups of students.

We used propensity score matching to create an analysis sample of students who received outreach that is best suited for comparison to a similar group of students who were not offered outreach. More specifically, our matching procedure identified pairs of students who did and did not receive outreach where each pair attended the same high school and are broadly similar in terms of six demographic variables and four academic indicators. To the degree that first-generation *and* low-income students face greater barriers than first-generation *or* low-income students do, using these variables in the matching procedure should help to reduce the underlying difference in college enrollment rates for treatment and control group students expected in the absence of the intervention. Nevertheless, we still anticipated that within each matched pair, the student who received outreach would face greater barriers to college enrollment and that a comparison of enrollment rates for those in matched pairs would still underestimate the effect of advising.

Despite these challenging issues, it was still quite plausible that our evaluation would find statistically significant evidence that virtual advising increased college enrollment in 2020. One recent multi-district study estimated a 5.2 percentage point increase in four-year college enrollment as the result of text-based FAFSA outreach and support from students' own high school counselors.[^03-commonapp_over-5] An effect of that size could well have been enough to provide a positive and significant result in this study. On the other hand, recent studies of similar outreach and support implemented at scale have found much smaller effects of virtual advising than in-person advising designed to deliver the same kind of support.[^03-commonapp_over-6][^03-commonapp_over-7] Overall and based on our empirical results, we conclude that the chatbot campaign had no impact on the targeted class of 2020 students submitting college applications or enrolling in college.

[^03-commonapp_over-5]: James, G., Witten, D., Hastie, T., & Tibshirani, R. (2017). An introduction to statistical learning with applications in R. Springer.

[^03-commonapp_over-6]: Avery, C., Castleman, B.L., Hurwitz, M., Long, B.T, and Page, L.C. (2021) "Digital messaging to improve college enrollment and success," Economics of Education Review, Elsevier, vol. 84(C).

[^03-commonapp_over-7]: Phillips, M., and Reber, S. (2022). "Does Virtual Advising Increase College Enrollment? Evidence from a Random-Assignment College Access Field Experiment." American Economic Journal: Economic Policy, 14 (3): 198-234.

The limited engagement of students with the bot likely explains these results. While some students were asking the bot important questions about how to enroll in college, decipher financial aid letters, and choose a major, most students did not message Oli in substantively meaningful ways. On average, students sent 8 text messages throughout the 38 week intervention, while 2.5% of students messaged a CAC adviser. A sizable portion of the study population--16%--opted out of the intervention and thus did not receive all of Oli's guidance. However, there was evidence to suggest that the chatbot had a positive impact on college enrollment for students who were highly engaged with the bot during the intervention. This group of highly engaged students may warrant further investigation, as they could shed light on what types of conversations with the bot were most helpful in supporting their postsecondary transition. However, with an intervention that cast such a wide net and that provided relatively general guidance related to college-going processes, it is perhaps unsurprising that the overall effects of the outreach were null.

Taken together, we reasoned that this outreach faced an uphill battle to improve outcomes beyond the differentials that exist absent the intervention. Even though we cannot know what the true causal impact of the outreach was, the evidence we gathered leads us to conclude that the outreach likely neither helped nor harmed students in applying to and enrolling in college.

\newpage

## Tables and Figures

\newpage

|                                                   |           |         | **Before Matching** |           |         | **After Matching** |
|-----------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
|                                                   | Treatment | Control |    Std.Mean.Diff    | Treatment | Control |   Std.Mean.Diff    |
|                                                   |           |         |                     |           |         |                    |
| Age                                               |  17.082   | 17.023  |        0.118        |   17.05   | 17.025  |       0.050        |
| Male                                              |   0.363   |  0.376  |        0.029        |   0.369   |  0.357  |       0.024        |
| English spoken at home                            |   0.562   |  0.558  |        0.009        |   0.569   |  0.573  |       0.009        |
| Has dependent                                     |   0.011   |  0.007  |        0.042        |   0.009   |  0.009  |       0.008        |
| Attended \> 1 High School                         |   0.181   |  0.161  |        0.041        |   0.164   |  0.166  |       0.003        |
| GPA rank                                          |   0.643   |  0.701  |        0.150        |   0.698   |  0.705  |       0.018        |
| Missing GPA rank                                  |   0.238   |  0.207  |        0.071        |   0.178   |  0.178  |       0.000        |
| SAT score                                         |   671.3   | 794.868 |        0.230        |  712.224  | 712.861 |       0.001        |
| Missing SAT Score                                 |   0.376   |  0.317  |        0.122        |   0.347   |  0.347  |       0.000        |
| College credit exams                              |   0.708   |  1.135  |        0.222        |   0.806   |  0.848  |       0.022        |
| TOEFL                                             |   0.000   |  0.001  |        0.093        |   0.000   |  0.000  |       0.009        |
| Sibling attended college                          |   0.337   |  0.387  |        0.105        |   0.353   |  0.345  |       0.017        |
| Submitted one college application before outreach |   0.777   |  0.796  |        0.044        |   0.853   |  0.853  |       0.000        |
| American Indian/Alaskan Native                    |   0.005   |  0.003  |        0.021        |   0.003   |  0.003  |       0.003        |
| Asian                                             |   0.084   |  0.091  |        0.027        |   0.096   |  0.093  |       0.010        |
| Black                                             |   0.28    |  0.185  |        0.213        |   0.287   |  0.307  |       0.045        |
| Latinx                                            |   0.41    |  0.231  |        0.364        |   0.408   |  0.396  |       0.023        |
| Native Hawaiian/Pacific Islander                  |   0.003   |  0.002  |        0.011        |   0.002   |  0.002  |       0.011        |
| White                                             |   0.169   |  0.383  |        0.570        |   0.164   |  0.164  |       0.002        |
| Multi-racial                                      |   0.044   |  0.051  |        0.031        |   0.036   |  0.031  |       0.025        |
| Race unknown                                      |   0.004   |  0.026  |        0.328        |   0.003   |  0.004  |       0.006        |
| Non-resident                                      |   0.000   |  0.028  |        4.261        |   0.000   |  0.000  |       0.042        |
| URM                                               |   0.826   |  0.563  |        0.694        |   0.833   |  0.833  |       0.000        |
|                                                   |           |         |                     |           |         |                    |
| Num.Obs                                           |  142,827  | 263,299 |                     |  99,593   | 61,553  |                    |

: Balance of student characteristics {#tbl-balance }

\newpage

|                          |   Overall    | Before outreach | After outreach |
|--------------------------|:------------:|:---------------:|:--------------:|
| Application Differential | -0.015\*\*\* |      0.004      |  -0.013\*\*\*  |
|                          |   (0.003)    |     (0.003)     |    (0.002)     |
|                          |              |                 |                |
| Control Mean             |    0.861     |      0.848      |     0.036      |
| Num.Obs.                 |    161146    |     161146      |     161146     |
| R2                       |    0.318     |      0.324      |     0.079      |
| FE: school_id            |      X       |        X        |       X        |

: Impacts on submitting at least one college application {#tbl-submit}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement. Baseline covariates include fee waiver indicator, first-generation status, age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator, sibling college indicator, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance. Robust standard errors clustered at the school level, reported in parentheses.

\newpage

|                         |  Fall 2020   | Spring 2021  |  Fall 2021   | Spring 2022  | Gap semester | Gap year  |
|:----------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
| Enrollment Differential | -0.020\*\*\* | -0.031\*\*\* | -0.033\*\*\* | -0.038\*\*\* |    0.003     | 0.004\*\* |
|                         |   (0.004)    |   (0.005)    |   (0.005)    |   (0.005)    |   (0.002)    |  (0.002)  |
|                         |              |              |              |              |              |           |
| Control Mean            |    0.787     |    0.724     |    0.701     |    0.648     |    0.024     |   0.023   |
| Num.Obs.                |    161146    |    161146    |    161146    |    161146    |    161146    |  161146   |
| R2                      |    0.155     |    0.177     |    0.179     |    0.194     |    0.073     |   0.079   |
| FE: school_id           |      X       |      X       |      X       |      X       |      X       |     X     |

: Impacts on college enrollment outcomes, by term {#tbl-overenroll}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement.Baseline covariates include fee waiver indicator, first-generation status,age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator,sibling college indicator, submitted at least one CommonApp application before start of intervention, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance.The gap semester outcome is a binary indicator coded as 1 if a student did not enroll in Fall 2020 but did enroll in Spring 2021. The gap year outcome is also a binary indicator coded as 1 if a student did not enroll in Fall 2020 or Spring 2021, but did enroll in Fall 2022. Robust standard errors clustered at the school level, reported in parentheses.

\newpage

|                         | Fall 2020 | Spring 2021 | Fall 2021 | Spring 2022 | Gap semester | Gap year |
|:----------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
| Enrollment Differential |   0.008   |    0.030    |  -0.007   |    0.016    |    0.001     |  0.000   |
|                         |  (0.016)  |   (0.016)   |  (0.016)  |   (0.016)   |   (0.006)    | (0.006)  |
|                         |           |             |           |             |              |          |
| Control Mean            |   0.683   |    0.605    |   0.593   |    0.528    |    0.036     |  0.041   |
| Num.Obs.                |   14842   |    14842    |   14842   |    14842    |    14842     |  14842   |
| R2                      |   0.311   |    0.338    |   0.337   |    0.344    |    0.224     |  0.248   |
| FE: school_id           |     X     |      X      |     X     |      X      |      X       |    X     |

: Impacts on college enrollment outcomes, by term - Submitted no application prior to intervention {#tbl-overenroll_submit}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement.Baseline covariates include fee waiver indicator, first-generation status,age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator,sibling college indicator, submitted at least one CommonApp application before start of intervention, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance.The gap semester outcome is a binary indicator coded as 1 if a student did not enroll in Fall 2020 but did enroll in Spring 2021. The gap year outcome is also a binary indicator coded as 1 if a student did not enroll in Fall 2020 or Spring 2021, but did enroll in Fall 2022. Robust standard errors clustered at the school level, reported in parentheses.

\

\newpage

|                         |  Fall 2020   |    4-year    | 4-year public | 4-year private |   2-year    |  Full-time   |
|:----------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
| Enrollment Differential | -0.020\*\*\* | -0.040\*\*\* |    -0.007     |  -0.033\*\*\*  | 0.019\*\*\* | -0.029\*\*\* |
|                         |   (0.004)    |   (0.005)    |    (0.005)    |    (0.004)     |   (0.003)   |   (0.005)    |
|                         |              |              |               |                |             |              |
| Control Mean            |    0.787     |    0.671     |     0.457     |     0.217      |    0.121    |    0.668     |
| Num.Obs.                |    161146    |    161146    |    161146     |     161146     |   161146    |    161146    |
| R2                      |    0.155     |    0.207     |     0.173     |     0.141      |    0.145    |    0.175     |
| FE: school_id           |      X       |      X       |       X       |       X        |      X      |      X       |

: Impacts on Fall 2020 college enrollment outcomes {#tbl-20enroll}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement. Baseline covariates include fee waiver indicator, first-generation status, age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator, sibling college indicator, submitted at least one CommonApp application before start of intervention, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance. Robust standard errors clustered at the school level, reported in parentheses.

\newpage

|                         |  Fall 2020   |    4-year    | 4-year public | 4-year private |   2-year    |  Full-time   |
|:----------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
| Enrollment Differential | -0.021\*\*\* | -0.044\*\*\* |    -0.007     |  -0.037\*\*\*  | 0.022\*\*\* | -0.034\*\*\* |
|                         |   (0.005)    |   (0.006)    |    (0.007)    |    (0.006)     |   (0.004)   |   (0.006)    |
|                         |              |              |               |                |             |              |
| Control Mean            |    0.784     |    0.661     |     0.452     |     0.212      |    0.129    |    0.661     |
| Num.Obs.                |    97537     |    97537     |     97537     |     97537      |    97537    |    97537     |
| R2                      |    0.160     |    0.213     |     0.181     |     0.144      |    0.152    |    0.180     |
| FE: school_id           |      X       |      X       |       X       |       X        |      X      |      X       |

: Impacts on Fall 2020 college enrollment outcomes - Racially marginalized students {#tbl-20enrollurm}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement. Baseline covariates include fee waiver indicator, first-generation status, age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator, sibling college indicator, submitted at least one CommonApp application before start of intervention, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance. Robust standard errors clustered at the school level, reported in parentheses.

\newpage

|                         | Fall 2020 | Spring 2021 | Fall 2021 | Spring 2022 | Gap semester | Gap year |
|:----------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
| Enrollment Differential |  -0.017   |   -0.016    |  -0.011   |   -0.011    |    -0.001    |  0.006   |
|                         |  (0.010)  |   (0.011)   |  (0.011)  |   (0.011)   |   (0.004)    | (0.003)  |
|                         |           |             |           |             |              |          |
| Control Mean            |   0.787   |    0.724    |   0.701   |    0.651    |    0.025     |  0.022   |
| Num.Obs.                |   20527   |    20527    |   20527   |    20527    |    20527     |  20527   |
| R2                      |   0.282   |    0.295    |   0.303   |    0.315    |    0.212     |  0.222   |
| FE: school_id           |     X     |      X      |     X     |      X      |      X       |    X     |

: Impacts on college enrollment outcomes - Opt out {#tbl-20enrollopt}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: Opt-out was constructed by flagging students who explicitly messaged "STOP" to the bot at any point during the intervention and/or received a "goodbye" message from Oli, indicating that they had requested to opt out. Due to data quality issues we only have text message data for 70% of all treated students. Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement.Baseline covariates include fee waiver indicator, first-generation status,age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator,sibling college indicator, submitted at least one CommonApp application before start of intervention, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance.The gap semester outcome is a binary indicator coded as 1 if a student did not enroll in Fall 2020 but did enroll in Spring 2021. The gap year outcome is also a binary indicator coded as 1 if a student did not enroll in Fall 2020 or Spring 2021, but did enroll in Fall 2022. Robust standard errors clustered at the school level, reported in parentheses.

\newpage

|                         | Fall 2020 | 4-year  | 4-year public | 4-year private |   2-year    | Full-time |
|:----------|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
| Enrollment Differential | 0.026\*\* |  0.002  |   0.032\*\*   |  -0.032\*\*\*  | 0.024\*\*\* |   0.005   |
|                         |  (0.009)  | (0.010) |    (0.011)    |    (0.009)     |   (0.007)   |  (0.010)  |
|                         |           |         |               |                |             |           |
| Control Mean            |   0.793   |  0.674  |     0.461     |     0.215      |    0.123    |   0.676   |
| Num.Obs.                |   26281   |  26281  |     26281     |     26281      |    26281    |   26281   |
| R2                      |   0.256   |  0.304  |     0.279     |     0.253      |    0.251    |   0.272   |
| FE: school_id           |     X     |    X    |       X       |       X        |      X      |     X     |

: Impacts of text message engagement on Fall 2020 college enrollment - High text engagement {#tbl-20enrolleng}

\**p* \< 0.05, \*\* *p* \< 0.01, \*\*\* *p* \< 0.001 Notes: High engagement indicator is coded as 1 for students who are in the top 25th percentile (sent more than 9 text messages) of total number of text messages students sent throughout the intervention. Due to data quality issues we only have text message data for 70% of all treated students. Analyses include school-level fixed effects and sampling weights to account for our propensity score matching approach with replacement.Baseline covariates include fee waiver indicator, first-generation status,age, gender, English spoken at home, dependent flag, attended more than one high school, high school GPA, SAT/ACT performance, college credit exams count, TOEFL indicator,sibling college indicator, submitted at least one CommonApp application before start of intervention, and race/ethnicity indicators. Additionally, we included indicators of missingness for high school GPA and SAT/ACT performance.Robust standard errors clustered at the school level, reported in parentheses.

\newpage

![Propensity score distribution](figures/ps_dist.png){#fig-psdist alt="Propensity score distribution" width="340"}
