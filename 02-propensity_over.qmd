# Paper 1: Evaluating modern propensity score estimation methods with high-dimensional data


## Abtract

Randomized Control Trials (RCTs) are still regarded as the gold standard for establishing causal claims. In education and more broadly social science research, RCTs may be difficult to implement or simply unethical (CITE), leaving researchers to rely on observational data for inference. This places researchers in a precarious position of attributing causal conclusion to correlation analysis (CITE). Thankfully, educational researchers have causal inference methods, such as propensity score analysis, that allow us to approximate the causal claims of an RCT with observational data. 
[INSERT TECHNICAL INTRO TO PS ANALYSIS]
As educational research moves towards having access to high dimensional datasets, for example, administrative records. We gain access to a rich set of covariates, making propensity score methods ideal. Propensity score analysis follows a straightforward approach. First, the researchers harnesses a rich set of  covariates to estimate the propensity score. This is typically done via logistic regression, where the conditional probability of being assigned to treatment can be estimated . After each observation has an estimated propensity score, various techniques can be employed, such as matching (Caliendo & Kopeinig, 2008), or weighting the analysis by the propensity score (Hirano & Imbens, 2001). 
Although adanvces have been made in incorporating sophisticated matching techniques [CITE]. Yet we have made little advances in the estimation step [CITE]. Estimating propensity scores via logistic regression may lead to biased estimates (d’Agostino, 1998 [CITE]). Theoretically, any methodological technique that can output a probability can be used for propensity score estimation. Something that machine learning techniques have been honed down to do with great success.

	We have yet to harness the advantages that ML algorithms can provide in classification tasks [CITE]. Furthermore,  ML algorithms  are not confined by parametric model assumptions, and with today's technology, they are computationally fast and efficient to implement. In the educational context, most applications of ML algorithms for classification have focused on high-dimensional, cross-sectional data. In contrast, ML classification techniques that are under the umbrella term “Deep Learning” have received comparatively less attention in applied research. 
In this simulation study, we applied logistic regression and Deep Neural Networks (DNN) to estimate propensity scores. In order to evaluate whether DNN could outperform the standard logistics regression, as well as more robust techniques currently in the literature [CITE]. We generate three simulated cohorts of varying sizes with a binary treatment variable and a series of covariates with induced correlation structure. Treatment assignment was constructed as a function of the covariates. Since treatment assignment is known, we have a "ground truth" of what the true propensity score should be. Different data structure scenarios were constructed to evaluate the performance of both models.  



Q4: What is your biggest worry about the current stage of the writing?

I started this paper a few years ago. The writing needs to be polished up and I need to incorporate all the new literature. Thankfully, I have finished going through the literature and now need to just incorporate it. My biggest worry is that I haven’t written a methods paper in a while, I feel a bit rusty. I also struggle in being 

Q5: What alternatives are you considering for “fixing” it? 

Incorporating the new literature and having an updated simulation plan that is more thought out. I plan for this to be Paper 1 of my dissertation. 

Q6: What would you benefit from having advice on? What do you need to do to resolve the issue?

I want this paper to be accessible to general social science researcher while being technical enough for a more “methods” audience. I don’t know if this paper can do both. I guess I need to be convinced what my audience should be…







## Introduction

Machine Learning (ML) in the age of big-data in education is still in its infancy but has already proved to be a useful analytic tool, especially with supervised ML methods such as decision trees, and neural networks. One advantage of ML algorithms is that they are not confined by parametric model assumptions, and with today's technology, they are computationally fast and efficient to implement. In the educational context, most applications of ML algorithms for classification have focused on high-dimensional, cross-sectional data. In contrast, ML classification techniques that are under the umbrella term “Deep Learning” have received comparatively less attention in applied research. 
In educational research, an important question relates to how to estimate causal estimates from observational data. In comparison to data from randomized experiments, observational data does not allow for randomization into treatment. Individuals self-select into treatment without knowing the underlying mechanism that leads to that selection. To overcome this problem, a researcher can leverage the rich set of covariates that may uncover the underlying mechanism for selection.  If we can balance both treatment and control based on these observable covariates, then we can approach estimating unbiased treatment effects without randomization. One way to achieve this is through propensity score estimation. 
The number of publications in education that use propensity score methods has drastically increased in the past several years (Fan & Nowell, 2011). This is due to the rise in quantitatively rigorous studies in education that rely on robust causal estimates. In education, we continuously deal with observational data in which we hope to extract meaningful causal estimates of an obersational studies in which no intervention was implemented.

 intervention that we implemented. Propensity score methods allow us to use observational data to identify a counterfactual for the estimation of causal effects. Propensity score methods were initially introduced by Rosenbaum and Rubin and are defined as the probability of receiving a treatment conditional on a set of observable covariates (1983). In propensity score matching the estimated propensity score is used to “match” treatment units to control units. The aim of conditioning on the propensity score is to achieve balance in the observed covariates between treated and untreated units and recreate an "as good as random" scenario. 
	As educational research moves towards having high dimensional datasets, for example, administrative records, we have access to a rich set of covariates, making propensity score methods appealing. The standard way a propensity score analysis moves forward is first by using the rich set of covariates at the researcher's disposal to estimate the propensity score. This is typically done by logistic regression, where the conditional probability of being assigned to treatment can be estimated via propensity scores. After each observation has an estimated propensity score various techniques can be employed, such as matching (Caliendo & Kopeinig, 2008), or weighing the analysis using the propensity score (Hirano & Imbens, 2001). Estimating propensity scores via logistic regression may lead to biased estimates (d’Agostino, 1998). Theoretically, any methodological technique that can output a probability can be used for propensity score estimation. Something that machine learning techniques have been honed down to do with great success.

In this simulation study, we applied logistic regression and Deep Neural Networks (DNN) to estimate propensity scores to evaluate whether DNN could outperform the standard logistics regression. We generate three simulated cohorts of varying sizes with a binary treatment variable and a series of covariates with induced correlation structure. Treatment assignment was constructed as a function of the covariates. Since treatment assignment is known, we have a "ground truth" of what the true propensity score should be. Different data structure scenarios were constructed to evaluate the performance of both models.  
Machine Learning-Neural Networks
	Machine learning algorithms have gotten much more exposure in social science, and recent work has shown how educational research can begin to employ these algorithms (Michalski, Carbonell & Mitchell, 2013). Machine Learning is a discipline that is focused on having programs "learn" intricate patterns in the data for prediction. ML is a form of applied statistics with an emphasis on the use of computers to estimate complicated functional forms. These ML algorithms are meant to "learn" from the data we provide, further letting us tackle a very complicated task such as weather prediction or stock predictions (Michalski et al., 2013). These algorithms are particularly suitable at classification problems (Kotsiantis, Zaharaskis & Pintelas, 2007), where the algorithm is asked to specify which of k categories some inputs belong to. To solve this problem, the algorithm produces a function f when y = f(x) the function assigned it to a k class. This is the same problem we are trying to solve with propensity score estimation where we have some function that is trying to find the probability that the unit is assigned to treatment.
These methods were left mainly to massive supercomputers, but with the advancement in computing power, these methods are now readily able to be run on any machine. As educational researchers, we must be aware of new methods and be able to use them to analyze our questions appropriately. With a large amount of data that is now available, these methods will one day become standard practice in social science research. 
	Although ML has been used for propensity score estimation and causal inference work (Lee, Lessler & Stuart, 2010), for example BART for propensity score matching, to my knowledge propensity score methods have not leveraged new Deep-Learning algorithms that have slowly come into prominence amongst the ML community (Deng & Yu, 2014). For this paper, I am focused on a particular family of algorithms in deep learning, called deep neural networks (DNN). To begin the discussion around deep neural networks, we have to understand what an artificial neural network is. 
Neural Networks
Artificial neural networks were initially developed by psychologist Frank Rosenblatt (1958) and his idea of a perceptron, which he thought of as a simplified mathematical model of how the neurons in our brains worked (Figure 1). The perceptron takes in a set of binary inputs (called neurons) and applies a weights (synapse strength) sums across all these weighted inputs and outputs either a 1 or 0 depending on the summed weights. This was meant to mimic how neurons in our brain work, with enough energy a neuron will fire (1) or not (0). Following this straightforward model, work began to expand this concept, and it cumulated with the creation of Artificial Neural Networks (ANN).
A diagram of a simple ANN is shown in Figure 2. The network is made up of a series of interconnected "neurons," that are organized in layers (Schalkoff, 1997). There is an input layer; these can be regarded as the covariates in a data frame. A series of hidden layers make up the middle portion of the diagram; the last layer is the output layer. In a classification problem, this layer outputs a probability of assignment to k class, can be regarded as a propensity score. The input layer does not do any computations; the hidden layers take in data from the input layers an apply an "activation function" such as a sigmoid function (Schalkoff, 1997) this gets fed forward to the next hidden layer where yet another activation function is applied. The presence of hidden layers allows the ANN to learn very complex non-linear relationships associated with the input data. These algorithms have been in extensive use within the machine learning community, but as computation speed increases, Deep Neural Networks are being used (DNN).
DNN is not entirely different from ANN. Their distinguishing feature is the depth (or several hidden layers). ANN that has more than two hidden layers are considered DNN. DNN can capture even more complex relationships amongst covariates. Each layer of neurons trains on a distinct set of features based on the previous layers activation function output (Silver, Huang, Maddison, Guez, Sifre, Van Den Driessche & Dieleman, 2016). The deeper you go inside the DNN, the more intricate features the model can learn. In the classification case, the last output layer is set to be a SoftMax function that returns a value from [0,1] the same as a sigmoid function in logistic regression. If we can leverage DNN to assign propensity score weights, perhaps we can improve weighting methods in observational studies.





Randomized Control Trials (RCTs) are still regarded as the gold standard for establishing causal claims. In education and more broadly social science, RCTs may be difficult to implement or simply unethical (CITE), leaving researchers to rely on observational data for inference. This places researchers in a precarious position of attributing causal conclusion to correlation analysis (CITE). Thankfully, educational researcher is experiencing a causal reinansance with the acceptance of well used causal inference methods, such as propensity score analysis.
Although, propensity score methods in social science researcher is now widely used (CITE). What has received relatively little attention has been advances in the estimation of the propensity score, which is commonly estimated via a logistic function. Research extending back to the 1970’s have proposed advances in the estimation of the propensity score through the use of machine learning algorithms. More recent, work has been done on using the tree based methods ….  For propensity score estimation. Yet, current litearature is still relying on logistic regression for the estimation step even though logistic models have many setbacks (CITE). 

A good contender for propensity score estimation has been neural networks, many evaluations have looked at the advantages of such algorithms (CITE). Such as non-parametric assumptions, and its flexibility to uncovered complex functional forms of the treatment selection mechanism. As any methodologies neural networks and other methodologies have been updated and left in the past. With the recent improvements in computational speed what has come to the forefront of computations research has been deep learnings, and may be worthy candidate for propensity score estimation. 

In this simulation study, we present the first evalutio of deep neural networks (DNN) for propensity score analysis. We examine the the performance of DNN against a main effects logistic regression, CART, Neural Networks, and Random Forrest. We generate three simulated cohorts of varying sizes (n = 50, n = 500, n = 2,000)  with a binary treatment variable. We further extend the simulation literature by generating a set of data-generating models: a propensity score data-generating model and an outcome-generation model as initially proposed by Keller et al. (2015). We induce non-linearity and non-additivity to both models to examine the effect that confounding has on the performance of our tested algorithms. 

We propose the use of Deep Neural Networks for propensity score analysis 



## Literature Review

### Review of propensity scores


Before diving in to propensity score analysis, we introduce the potential outcomes framework of the Rubin causal model (RCM). As it is the seminal work that lead to many of the developments in quasi experimental methods. For a dichotomous treatment variable (say students who received a math curriculum versus those who did not) , each student I has a potential outcome Yi(1), which we would observe for the students who received the math curriculum (I.e. Treatment) (Zi =1), and a potential outcome Yi(0), which we would observe if the same student I did not receive the math curriculum (I.e. control condition) (Zi = 0). Given this the difference in the two potential outcomes, Yi(1) - Yi(0), represents the individual causal effect. If we take the difference between the expectation of these potential outcomes in the entire sample, we estimated what is called the average treatment effect (ATE), ATE (gama) = E[Yi(1)] - E[Yi(0)]. As should be aparent, in the real world we do not observe both potential outcomes for one student simultaniously, that is to say we do not observe student I both when they received the math curriculum and when they didn’t. This is why the fundemental problem of causal inference is a missing data problem (CITE).
Although we cant directly observe the expectations of these potential outcome under certain assumptions (mainly independent and the stable unit tereamtne value assumptions (SUTVA). we can identify the ATE (CITE). The beauty of RCT is that students are randomized into these conditions which would satisfy the independence condition, meaning that the potential outcomes, (Yi(0), Yi(1)) were independent of treatment assignment (Z), making estimating the ATE straightforward since we can express the ATE using observed outcomes [FORMULA]. Randomization also gives us another advantage of making SUTVA plausible. Since treatment assignment is randomized the potneailt outcomes of a student is not dependent on their treatment assignment or that of other students. Finally, there is only one unique treatment and control group (CITE). 
In educational research, we rarely have the ability to implement high-fidelity RCTs, therefore we must rely on observational data and quasi experimental methods. In comparison to data from randomized experiments, observational data does not allow for randomization into treatment. Individuals self-select into treatment without knowing the underlying mechanism that leads to that selection, (I.e. The indepndentandce assumption is violated). Therefore, using a non-randomized version of our math curriculum example, student in the math curium group (Z = 1) may differ on baseline characteristics compared to students not given the curriculum (Z = 0). Perhaps students who were selected into the math curriculum were already high achieving in math, therefore looking at a naive estimate of the treatment effect one may wrongly conclude that the math curriculum has a positive effect on students. To overcome this violation, a researcher can leverage the rich set of covariates that may uncover the underlying selection mechanism as well as any confounders that effect the outcome of interest. If we can balance both treatment and control groups based on these observable covariates, then we can approach estimating unbiased treatment effects without randomization through propensity score analysis (CITE). 

In propensity score matching the estimated propensity score is used to “match” treatment units to control units. The aim of conditioning on the propensity score is to achieve balance in the observed covariates between treated and untreated units and 





Machine Learning (ML) in the age of big-data in education is still in its infancy but has already proved to be a useful analytic tool, especially with supervised ML methods such as decision trees, and neural networks. One advantage of ML algorithms is that they are not confined by parametric model assumptions, and with today's technology, they are computationally fast and efficient to implement. In the educational context, most applications of ML algorithms for classification have focused on high-dimensional, cross-sectional data. In contrast, ML classification techniques that are under the umbrella term “Deep Learning” have received comparatively less attention in applied research. 
The number of publications in education that use propensity score methods has drastically increased in the past several years (Fan & Nowell, 2011). This is due to the rise in quantitatively rigorous studies in education that rely on robust causal estimates. In education, we continuously deal with observational data in which we hope to extract meaningful causal estimates of an obersational studies in which no intervention was implemented.

 intervention that we implemented.	As educational research moves towards having high dimensional datasets, for example, administrative records, we have access to a rich set of covariates, making propensity score methods appealing. The standard way a propensity score analysis moves forward is first by using the rich set of covariates at the researcher's disposal to estimate the propensity score. This is typically done by logistic regression, where the conditional probability of being assigned to treatment can be estimated via propensity scores. After each observation has an estimated propensity score various techniques can be employed, such as matching (Caliendo & Kopeinig, 2008), or weighting the analysis using the propensity score (Hirano & Imbens, 2001). Estimating propensity scores via logistic regression may lead to biased estimates (d’Agostino, 1998). Theoretically, any methodological technique that can output a probability can be used for propensity score estimation. Something that machine learning techniques have been honed down to do with great success.



### Machine Learning and propensity scores

	Machine learning algorithms have gotten much more exposure in social science, and recent work has shown how educational research can begin to employ these algorithms (Michalski, Carbonell & Mitchell, 2013). Machine Learning is a discipline that is focused on having programs "learn" intricate patterns in the data for prediction. ML is a form of applied statistics with an emphasis on the use of computers to estimate complicated functional forms. These ML algorithms are meant to "learn" from the data we provide, further letting us tackle a very complicated task such as weather prediction or stock predictions (Michalski et al., 2013). These algorithms are particularly suitable at classification problems (Kotsiantis, Zaharaskis & Pintelas, 2007), where the algorithm is asked to specify which of k categories some inputs belong to. To solve this problem, the algorithm produces a function f when y = f(x) the function assigned it to a k class. This is the same problem we are trying to solve with propensity score estimation where we have some function that is trying to find the probability that the unit is assigned to treatment.
These methods were left mainly to massive supercomputers, but with the advancement in computing power, these methods are now readily able to be run on any machine. As educational researchers, we must be aware of new methods and be able to use them to analyze our questions appropriately. With a large amount of data that is now available, these methods will one day become standard practice in social science research. 

	Although ML has been used for propensity score estimation and causal inference work (Lee, Lessler & Stuart, 2010), for example BART for propensity score matching, to my knowledge propensity score methods have not leveraged new Deep-Learning algorithms that have slowly come into prominence amongst the ML community (Deng & Yu, 2014). For this paper, I am focused on a particular family of algorithms in deep learning, called deep neural networks (DNN). To begin the discussion around deep neural networks, we have to understand what an artificial neural network is. 

### Neural networks

Artificial neural networks were initially developed by psychologist Frank Rosenblatt (1958) and his idea of a perceptron, which he thought of as a simplified mathematical model of how the neurons in our brains worked (Figure 1). The perceptron takes in a set of binary inputs (called neurons) and applies a weights (synapse strength) sums across all these weighted inputs and outputs either a 1 or 0 depending on the summed weights. This was meant to mimic how neurons in our brain work, with enough energy a neuron will fire (1) or not (0). Following this straightforward model, work began to expand this concept, and it cumulated with the creation of Artificial Neural Networks (ANN).
A diagram of a simple ANN is shown in Figure 2. The network is made up of a series of interconnected "neurons," that are organized in layers (Schalkoff, 1997). There is an input layer; these can be regarded as the covariates in a data frame. A series of hidden layers make up the middle portion of the diagram; the last layer is the output layer. In a classification problem, this layer outputs a probability of assignment to k class, can be regarded as a propensity score. The input layer does not do any computations; the hidden layers take in data from the input layers an apply an "activation function" such as a sigmoid function (Schalkoff, 1997) this gets fed forward to the next hidden layer where yet another activation function is applied. The presence of hidden layers allows the ANN to learn very complex non-linear relationships associated with the input data. These algorithms have been in extensive use within the machine learning community, but as computation speed increases, Deep Neural Networks are being used (DNN).

DNN is not entirely different from ANN. Their distinguishing feature is the depth (or several hidden layers). ANN that has more than two hidden layers are considered DNN. DNN can capture even more complex relationships amongst covariates. Each layer of neurons trains on a distinct set of features based on the previous layers activation function output (Silver, Huang, Maddison, Guez, Sifre, Van Den Driessche & Dieleman, 2016). The deeper you go inside the DNN, the more intricate features the model can learn. In the classification case, the last output layer is set to be a SoftMax function that returns a value from [0,1] the same as a sigmoid function in logistic regression. If we can leverage DNN to assign propensity score weights, perhaps we can improve weighting methods in observational studies.



## Methods

To answer the research question, we generated three simulated cohorts of varying sample size (n = 100, 1000, 10000) with a binary treatment variable Tx and ten continuous covariates (W_i = 1…10), three dichotomized covariates (D_i = 1,2,3) that were independently associated with Tx. Another 83 covariates were generated that were not associated with Tx (E_i = 1…87). This produced a dataset with 100 covariates and a binary Tx variable. 
Data were generated by first constructing seven covariates generated as independent standard normal random variables with mean zero and unit variance (W_i = 1…7). Another 3 (W_i= 8, 9, 10) standard normal random variables were created with induced correlations of .1, .5, and .8. An additional 3 dichotomized covariates were created (D_i = 1,2,3). Finally, 87 covariates were generated as independent draws from a standard normal random distribution.
The treatment dummy variable (Tx) was generated using linear regression as a function of W_i. The formula to generate the true propensity score was generated with the following structure:
〖(1)  Pr〗⁡[Tx=1│W_i ]
=(1+exp⁡{-(β_0+β_1 W_1+β_2 W_2+β_3 W_3+β_4 W_4+β_5 W_5+β_6 W_6+β_7 W_7+β_8 D_1+β_9 D_2+β_10 D_3 )} )^(-1)
To generate the treatment variable, we generated a random number from [0,1] from a uniform distribution. Tx was set to equal one if the random number was less than the true propensity score estimated with formula (1). The covariance matrix for the constructed variables is shown in Appendix A. 
Scenarios to be Tested
To compare the performance of logistic regression and DNN for propensity score estimates, we created realistic data structure scenarios that educational researchers could find themselves in. We varied the sample size and number of covariates to develop 6 unique scenarios. Table 1 shows the scenarios that we tested.
Different sample sizes were used from 100, 1000, to 10000 as well as the number of covariates from 13 covariates (Low-Dimensional Data) to 100 covariates (High Dimensional Data). This should capture scenarios in which researchers are using very small educational datasets with a few covariates, to a large amount of administrative data that could yield hundreds of covariates. 
Since we theoretically know the membership assignment to tx for the entire sample we were able to employ cross-validation using the holdout method, in which we can split our dataset into two parts a Training dataset for which we train our model on and a Testing dataset to test our model specifications that were “learned” in the training step (Kohavi, 1995). We used standard holdout, with 75% of the dataset being used to train and 25% being "held out" to test. This avoids overfitting our models.
For each scenario, we compared both logistic and DNN in estimating propensity scores. I accessed the performance of each model using an accuracy score given by:

Accuracy=(True Positive+True Negative)/(n )

Simulated data and data analysis was conducted on Python 3 on a Mac platform.


To answer the research question, we generated three simulated cohorts of varying sample size (n = 100, 1000, 10000) with a binary treatment variable Tx and ten continuous covariates ( = 1…10), three dichotomized covariates ( = 1,2,3) that were independently associated with Tx. Another 83 covariates were generated that were not associated with Tx ( = 1…87). This produced a dataset with 100 covariates and a binary Tx variable. 
Data were generated by first constructing seven covariates generated as independent standard normal random variables with mean zero and unit variance ( = 1…7). Another 3 (= 8, 9, 10) standard normal random variables were created with induced correlations of .1, .5, and .8. An additional 3 dichotomized covariates were created ( = 1,2,3). Finally, 87 covariates were generated as independent draws from a standard normal random distribution.
The treatment dummy variable (Tx) was generated using linear regression as a function of . The formula to generate the true propensity score was generated with the following structure:


To generate the treatment variable, we generated a random number from [0,1] from a uniform distribution. Tx was set to equal one if the random number was less than the true propensity score estimated with formula (1). The covariance matrix for the constructed variables is shown in Appendix A. 
Scenarios to be Tested
To compare the performance of logistic regression and DNN for propensity score estimates, we created realistic data structure scenarios that educational researchers could find themselves in. We varied the sample size and number of covariates to develop 6 unique scenarios. Table 1 shows the scenarios that we tested.
Different sample sizes were used from 100, 1000, to 10000 as well as the number of covariates from 13 covariates (Low-Dimensional Data) to 100 covariates (High Dimensional Data). This should capture scenarios in which researchers are using very small educational datasets with a few covariates, to a large amount of administrative data that could yield hundreds of covariates. 
Since we theoretically know the membership assignment to tx for the entire sample we were able to employ cross-validation using the holdout method, in which we can split our dataset into two parts a Training dataset for which we train our model on and a Testing dataset to test our model specifications that were “learned” in the training step (Kohavi, 1995). We used standard holdout, with 75% of the dataset being used to train and 25% being "held out" to test. This avoids overfitting our models.
For each scenario, we compared both logistic and DNN in estimating propensity scores. I accessed the performance of each model using an accuracy score given by:



Simulated data and data analysis was conducted on Python 3 on a Mac platform. 


### Data

### Simulation

## Preliminary Results

	Table 1 shows the results of the Low Dimension scenario. In general, as the sample size increased both DNN and Logistic Regression began to underperform, this is a surprising result given that ML algorithms tend to perform very well in large datasets (Rauber, Merki & Dittenbach, 2002). With regards to our measures of interest, both algorithms performed equally well. 
Next, we looked at the High Dimensional setting. Table 3 shows the results of the High Dimension scenario. As opposed to the Low Dimensional scenario as sample size increased, both models performed equally well. With regards to our measures of interest, both models had relatively good accuracy.  
	These results are surprising, being that a non-parametric (DNN) model can have as good of accuracy as a parametric model (Logistic).  Given that DNN models require a bit of programming knowledge it may not be feasible for an applied researcher to use this method as opposed to a large number of packages available that can quickly run propensity score estimation (Leuven & Sianesi, 2018). 


## Looking Ahead

As educational research moves towards the age of Big Data, researchers will have access to tremendous amounts of covariates. With such a large number of covariates at our disposal, researchers can leverage causal methods like propensity score estimation. The traditional approach has been to use logistic regression, but as computational power has increased more sophisticated Deep Learning methods have been developed that are well situated to solve classification problems such as propensity score estimation. Results showed that DNN and Logistic regression performed equally well across conditions and sample sizes. In the final iteration of the paper, we will focus on creating a Monte Carlo simulation study to analyze these effects. We will further consider varying the linear relationship the covariates had with the outcome to more complex non-linear patterns, perhaps in that context DNN model would outperform Logistic models.

## Tables and Figures
