# Paper 1: Evaluating modern propensity score estimation methods with high-dimensional data

## Introduction

Questions of causality have traditionally been addressed using Randomized Control Trials (RCT), whereby individuals are randomly assigned to either a treatment group that receives some intervention or policy and a control group that receives no such treatment (Murnane & Willett, 2010). The act of randomization assures, on average, that both groups will be balanced on all observable *and* unobservable characteristics, ruling out that an intervention impact is due to the differences in group characteristics (Rosenbaum, 2010, 2020). However, in education and more broadly social science research, ethical, cost, or practical barriers can preclude a researcher from conducting a randomized experiment. Instead, researchers may rely on non-experimental/observational studies in which students either self-select or are placed into an intervention without randomization. This complicates our ability to attribute causal treatment effects because without randomization imbalance in the distribution of student covaraites between groups could lead us to incorectly attribute a change in outcome to the intervention or policy, instead of differences in group composition. A rich body of methodological literature has focused on developing quasi-experimental methods that attempt to remedy the internal validity issues with observational data by balancing characteristics between those who received a treatment and a pool of potential comparison units. One such popular approach is propensity score analysis (CITE).

In education research propensity score analysis is one of the most widely used quasi-experimental methods (Fan & Nowell, 2011; Powell et al., 2019; Rosenbaum, 2020; Stuart, 2007; Thoemmes & Kim, 2011). Propensity score analysis is a quasi-experimental techniques used to balance *observed* characteristics between treated and un-treated students, just as randomization to treatment and control conditions creates balance on observable variables. The propensity score represents the probability that a student *would have* been exposed to treatment, conditional on observable characteristics (Rosenbaum & Rubin, 1983, 1984). Propensity scores can be used in a variety of ways to balance pre-treatment characteristics, includind propensity score matching (PSM), in which treated students are matched to un-treated students who have similar propensity scores, and propensity score weighting (PSW), in which the treatment groups are re-weighted in such a way that observed characteristics are balanced between both groups.

However, in order to estimate unbiased treatement effects depends on the researcher correctly specifying the propensity score model(Rosenbaum & Rubin, 1981, 1984), such that:

1.  The *propensity score model* captures *all* covariates related to the selection into treatment, leaving out no potential confounders (Rosenbaum, 2010, 2020).

2.  The *propensity score model* correctly models the association between covariates and treatment selection, meaning that all proper interactions and non-linear terms are specified (Rosenbaum, 2010, 2020).

The literature suggests a "kitchen sink" approach for variable selection to guard against the first condition since the penalties are high if a potential confounder is *not* included in the *propensity score model*, leading to biased treatment effect estimates (Pirracchio et al., 2015; Shortreed & Ertefaie, 2017). For the second condition, the literature suggests iterating through various model specifications, with each iteration adding interactions or non-linear terms until a balance is achieved amongst the observed characteristics between treated and un-treated students (Murnane & Willett, 2010; Rosenbaum). This iterative process becomes increasingly cumbersome as the number of covariates increases (CITE).

With ever increasing acess to rich administrative data (i.e., big data, high-dimensional data), typically used in education research, correctly capturing and modeling *all* covariates related to treatment becomes complex (CITE). The consequences are steep if these conditiona are not met, as it will lead to biased treatment effect estimates (CITE). Furthermore, much of the applied literature fails to follow the iterative approach in modeling the propensity score (CITE).

The most widely used model for estimating the propensity score in education -- and broadly social science research -- is logistic regressio. Although it is the most widely used, the logistic model mayb be inadqeuate in estimating the propensity score, leasding to biased treatment effects when the model is incorectly specified (Lee et al., 2010; Pan & Bai, 2015; Setoguchi et al., 2008; Westreich et al., 2010). In the context of high-dimensional data logistic models tend to degrade in performance leading to a model that overfits the data and generates propenisty score out of bounds (CITE).

With the rapid emergence of increasingly sophisticated quantitative methods and access to an ever-increasing amount of high-dimensional administrative data (Daniel, 2019; Gibson & Ifenthaler, 2016; Williamson, 2017) their are increasing posibilities for the deveopment and application of new analytic methods. Particularly, their have been exciting developmets using machine learning algorithms for propensity score estimation, such as tree-based methods and neural network approaches (CITE). These machine learnign methods allows automatic, data drien way of capturing nonlinearities and nonaddvitivity in the propensity score estimation model, which may lead to better covariate balance without the laborious iteration process. However, to date no work has looked at the performance of these machine learning methods when applied to high-dimentioal data. As much of the previous propensity score simulation work has focused on simulated low-dimensional datasets with less than 20 covariates.

In addition, recent advances in computer science has generated a wealth of research on a potential method that could be used to estiamte the propensity score: Deep Neural Networks (DNN) (Deng & Yu, 2014; Hernández-Blanco et al., 2019; LeCun et al., 2015; Perrotta & Selwyn, 2019). DNN's are algorithms based on our brain's neural architecture. They are commonly used in industry to model complex prediction and classification tasks and have already proven to excel in modeling high-dimensional data (LeCun et al., 2015). DNN's are highly flexible and capable of capturing complex interactions and non-linearities, and allow for automatic variable section (Henandex-Blanco et al., 2010; LeCu et al., 2015; Zou et al., 2019).

DNN's represent an essential tool for researchers in estimating the correct *propensity score model*, in general, and mainly when dealing with large datasets that include hundreds of variables on students, and when those variables have complex associations.

Thefore, my first disseration study aims to assess the performance of the traditional logistic model and other machine based methods, specifcally XXXXX in the context of high-dimensional data using propensity score weighting. In addtion, I overcome the limitaiton of existing methods by proposing a novel DNN-based appraoch to propensity score estimation. I illustrate a novel new framework for generation multi-type, correlated high-dimenional data that replicates the high-dmensaon administrative data in eduation and social sicne research.

The remainder of this document is structured as follows: First, I review the relevant literature for this study. Then, I describe my proposed data and simulation approach. Finally, I conclude with a discussion of remaining questions for my analysis.

## Literature Review

In this section, I review the literature on causal inference and propensity score analysis with a particular focus on machine learning approaches to estimating the propensity score. In addition, I draw on literature on neural network approaches, specifically deep neural networks to motivate the development of my method.

### Neyman-Rubin-Holland Model for Causal Inference

My approach for causal inference is based on the Neyman-Rubin-Holland model, here after refereed to as the Rubin model (Holland 1986; Rubin 2006, 1974; Rosenbaum 2002). A core tenet of the Rubin model states that a treatment effect is the difference in two potential outcomes for an individual. For a dichotomous treatment variable ($Z$), let $Z_i = 1$ if the $i$'th student was assigned to the treatment group -- say a job training program - and $Z_i = 0$ if they received no such training. Let $Y_{ij}$ be a potential outcome for an individual student depending on treatment assignment $Z_i$, such that each student has two potential outcomes $Y_{i1}$ is the potential outcome had a student received the job training program and $Y_{i0}$ as the potential outcome had that same student not received the program. Therefore the treatment effect of the job training program for student $i$ would be defined by their individual treatment effect as:

$$
\tau_i = Y_{i1} - Y_{i0}
$$ {#eq-treat}

However, in the real world we do not observe both potential outcomes for the same student simultaneously. That is to say we do not observe student $i$'s potential outcome both under receiving the job training program and not. This is why the fundamental problem of causal inference is a missing data problem (CITE). Although we cant directly observe these potential outcomes under certain assumptions we can estimate the average treatment effect (ATE) across our sample, defined as:

$$
ATE= \mathbb{E}(Y_{i1} -Y_{i0})=\mathbb{E}(Y_{i1})-\mathbb{E}(Y_{i0})
$$ {#eq-ate}

Where $\mathbb{E}(Y_{i1}$ is the expected value of all students in the treatment group and $\mathbb{E}(Y_{i0}$ is the expected value of all students in the control group. In the context of a true experiment/randomized control studies (RCT), in which students are *randomly* assigned to treatment conditions we can estimate the ATE in a straightforward manner since with a large enough sample size both treated and control students will be balanced on all observable and unobservable characteristics. Stated differently a randomized experiment assures that $Y_1$ and $Y_0$ are independent of treatment assignment ($Z$), therefore the treatment effect can be regarded as causal.

We may be also interested in estimating the average treatment effect for those who were *actually* treated. Given, that even in an experiment perfect compliance is not always possible. Some students in the treated group may decide not to take-up the job training program while students in the control group may find a way to receive the treatment. In this case we may be interested in the average treatment effect on the treated (ATT) (CITE), which is defined as:

$$
ATT= \mathbb{E}(Y_{i1} -Y_{i0}|T_i=1)=\mathbb{E}(Y_{i1}|Z_i=1)-\mathbb{E}(Y_{i0}|Z_i=1)
$$ {#eq-att}

Where ATT is the difference in potential outcomes *only* for those students who were actually treated. In practice, the ATT is the most policy relevent estimand since its the treatment effect on students who actually took-up treatment (CITE).

### Observational Studies

In behavioral and social science research, ethical, cost, or practical barriers can preclude a researcher from conducting a randomized experiment and instead must rely on non-experimental/observational data (Bai, 20XX). As opposed to RCT's, students in observational studies either self-select or are placed into an intervention group without randomization. This complicates the ability to attribute causal claims to a treatment or outreach because without randomization, their may be differences in students observable and unobservable characteristics between those who received the treatment and those who did not.

For example, say than in our job-training example students were not randomized into treatment or control, instead students self-select into treatment. In this scenario the underlying treatment assignment is unknown. It could be that students who self-select into the job-training program are more motivated to find employment than those who did not sign up for the program. Given this imbalance in group characteristics we may incorrectly attribute a change in outcomes to the treatment instead of differences in characteristics between those students who received the treatment and those who did not. In other words, observational studies *cannot* ensure that a students outcome is independent of treatment ($Z$).

Various statistical methods have been proposed that focus on estimating un-biased treatment effects in observational studies. These include, instrumental variable approaches (CITE), regression discontinuity (CITE), syntethic control (CITE), and propensity score approaches (CITE). For my dissertation I focus on propensity score approaches that leverage student-level covariates to create a balancing score that can account for *observed* group differences.

### Propensity Scores

Propensity score analysis was originally proposed by Rosenbaum and Rubin in 1983 as a means to increase the accuracy of captuing unbiased treatment effects in observational studies (CITE). Propensity score analysis is one of the most popular quasi-experimental method among social and behavioral researchers, particularly among applied educational researchers (CITE).

Rosenbaum and Rubin define a propensity score ($e$) as the conditional probability of assignment to treatment conditional on observed pre-treatment covariates. Defined as:

$$
e(x) = Pr(Z=1|X)
$$ {#eq-ps}

They are typically estimated as predicted probabilitions from a logistic regresison in which a binary outcome varible indicating wether a unit received treatment predicted by observed characteristics ($X$). They range from 0 to 1, with values closer to 1 indicating that an individual has a higher probability of being assigned to treatment. Rosenbaum and Rubin (1983, 1984) demonstrate that if the propensity score is balanced across treatment and control groups then the distribution of observed covariates between treatment and control is also balanced (CITE). Therefore, two students with similar propensity score, theoretically should have simular distribution of observed covariates.

Researchers can condition on the propensity score using a variety of techniques including matching, which pairs treatateed student to control students based on their proximity of their propensity score; stratification, which groups students into unique strata based on their propensity score; and weightin, which reweights students in the sample so that the distribution of the propensity score of the control group is similar to that of the treated group. All these methods have the similar aim of using the propensity sore to adjust for the inbalance in covariate distribution between treated and cotnrol units. Therefore, by conditioning on the PS via matching, stratification, or weighting we can eliminate bias in our treatment effect, and approximate the true causal effect. However, for this to be true certain assumptions must hold.

#### Assumptions

Several assumptions must hold in order to correctly estimate an unbiased treatment effect using propensity score analysis. These include the strong ignorabiliyt assumption, stable unit value assumption, and region of common support.

The strong ignorability assumption states that the potential outcomes of an individual is conditionaly independent of treatment assignment, if we successfully conditioned on all observed covariates, $Y_1, Y_0 \perp Z|X$. In experimental studies this assumption is satisfied by the act of randomization, since randomization ensures that treatment assignment is independent of an individuals outcome. However, in observational studies this assumption holds only if *all* covariates that relate to treatment assignment are included in the PS model.

The second assumption, the stable unit treatment value assumption (SUTVA) states that the potential outcomes for an individual is unaffected by the treatment assignment of any other individual. Said another way, the outcome of a student is independent of whether another fkstudent is assigned to the treatment or control group.

Finally the third assumptio is the region of common support. This assumption states that their needs to be sufficient overlap in the distribution of the estimated propensity scores (PS) between the treatment and control group. With suffecient support we can assure that we are able to find reasonable matches to control students.

If these assumptions hold we are able to estimate causal effects from observational data. Unfortunately many of these assumptions are untestable. For example, in observational data the majority of the time we are blind to the underlying treatment assignment mechanism and therefore we can't be certain that *all* covariates related to treatment assignment are included in our PS model. In practice, propensity scores should be viewed as a means to reduce some -- but no all -- of the bias associated between treatment and outcome in observational studies.

#### Covariate Selection

The first step in propensity score analysis is selecting covariates that are theoretical sound and relate both to treatment assingemnt and the outcome. Since in order to estimate unbiased treatment effects correctly using propensity scores we have to capture *all* covariates related to the selection into treatment, leaving out no potential confounders (Rosenbaum, 2010, 2020). However, in practice it may be imposibble to know which covariates our confounders, therefore the literature suggests a "kitchen sink" approach for variable selection to guard against the the penalties of excluding potential founders (Pirracchio et al., 2015; Shortreed & Ertefaie, 2017). The exclusion of potential confounders lead to biased treament effect estimates (Rosenbaum et al., 1983; Shadish et al., 2010; Steiner, et al., 2011).

In order to estimate unbiased treatement effects using propenisty score analysis depends on the researcher selecting variables that are theoretical backed and potential confounders(Rosenbaum & Rubin, 1981, 1984). Such that the *propensity score model* captures *all* covariates related to the selection into treatment (Rosenbaum, 2010, 2020) and the functional form of their relationship is correly modelled, meaning that all proper interactions and non-linear terms are specified (Rosenbaum, 2010, 2020).

The literature suggests a "kitchen sink" approach for variable selection to guard against the first condition since the penalties are high if a potential confounder is *not* included in the *propensity score model*, leading to biased treatment effect estimates (Pirracchio et al., 2015; Shortreed & Ertefaie, 2017). For the second condition, the literature suggests iterating through various model specifications, with each iteration adding interactions or non-linear terms until a balance is achieved amongst the observed characteristics between treated and un-treated students (Murnane & Willett, 2010; Rosenbaum). This iterative process becomes increasingly cumbersome as the number of covariates increases (CITE).

### Estimating the Propensity Score

The next step is estimating the propensity score ($e$) based on some parametric or non-parametric model. Any parametric or non-parametric model that can be used to generate propensity scores as long as it output a bounded probability between 0 and 1. The most common parametric model used for estimating the propensity score is logistic regresison (CITE).

#### Logistic Regression

Logistic regression is the most common model used to generate propensity scores in the social science literature, due to its ease of interpretation and familiarity to many applied researchers (Keller et al., 2013). Additonally the logistic model can accomodate both continous and categorical covariates. The logistic model is defined as:

$$
e(X) = ln\left( \frac{Pr(Z=1|X)}{1-Pr(Z=1|X)} \right) = \beta_o + \beta_1X_1 + \cdots + \beta_pX_p
$$

Where $X$ is a vector of observed covariates being regressed on $Z$ a binary indicator equal to 1 if a student is in the treatment group and 0 otherwise. The predicted probabilities estimated by the natural log of the log odds of the output produce continous probabilities bounded between 0 and 1, that are used as the propensity score.

Although logistic regression is the most widely used model to estimate propensity scores it may not be the most valid for inference. Various simulation studies suggests that using the logisti regression for modeling propenisty scores is inadequate and lead to biased treatment effects when incorrectly specified, given parametric models require asusmptions regardig the functional form, and varible distributions (Lee et al., 2010; Pan & Bai, 2015; Setoguchi et al., 2008; Westreich et al., 2010). This is especially true when estimating propensity score with a large amount of covariates. In a high-dimensional context the logistic model tend to degrade and estimate propenisty score that fall outside the desired 0 to 1 bounds. With the even increasing amounts of "big" administrative data at the disposal of applied beahvioral and social sicnece research the degradation of logistic regression is worrisome. However, outside the social sicnece research has suggested alterantive to the logistic regression with the use of non-parametric models using machine learing algorithms.

#### Machine Learning Approaches

Machine learning algorithms have recently gained populairty in the causal inference and propensity score litreature (CITE Athey), due to these algortihgm being highly flexible and able to model complex functional forms in an iterative manner without explicit manipulation from the reserch. With genrally simple modificaiton these alroghthm are able to output bounder probabilities that can be used as propenisty scores. Broadly, the machine leanring algorithms used in proepnisty score estimation can be classifed under tree-based methods, ensemble methods, and the focus of this disseration neural network based apporaches.

#### Tree Based Methods

One such popular machine learning approach to estimating propensity score is classificaiton and regression trees (CART) (Friedman, Olshen & Stone in 1984). This algoriths split data recursively into subset based on indviduals covariates in order to predict a given individuals probability of membership assingment. For use in propenisty score the predicted outcome is simply a binary indicator of treatment assigment ($Z$). This algorithm splits covariates by level of importance, with the first split being the covariate that can produce the most distintive split. For exmaple, if a students avobe 15 years of age is influential in wether they received tretment or not the first split would be those student with $age > 15$ and with $age < 15$. The algorithm processed to the next most influential covariate split creating a treaa like structure (FIGURE). Splitting stops whenthe data into subsets which minmize the prediction error in which an additional split would not continue to minize the prediction error. The final "branches" of the model repsent invidual grouping of students with similar propenisty score. Like logistic regression the CART algorithm can handle both continous and categorical covarites. However, unlike logistic regression CART is insensitive to outliers, and can model interactions and non-linearities automatically (CITE). 

A well documented issue with CART is that the single tree may likley overfit the data, therefore a modified verision of CART that grows a tree and then "prunes" back branches that don't lead to a reduction in prediction error. This leads to trees that dont overfit the data. Although pruning can alleviate some of the changes of overfitting the data, a major drawback of CART is that it relies on a single tree which may be weak at predicting the propensity score. 

#### Ensemble Methods

Ensemble methods are a family of algirthms that generate multiple trees predicting treatment assingment. By exploiding differences among each tress ensemble methods can significant improve prediction, since many weak trees together can generate better prediction than a single CART. A popular ensemble method is boostrapped aggregated CART, reffered to as Bagged CART (CITE). Bagged CART involves fitting multiple CART on boothstrapped samples of the data. By "growing" trees on boostrapped samples we can reduce teh chances of overfitting our data. Each individual tree will generate a proability of an indivudal being assigned to treatment, with a final probability of membership based on aggregating the probability of assingment across all trees.  Finally, a more robust ensemble method is random forest (CITE). In comparsion to CART and Bagging random forest algortihm grows trees by randomly selecting covariates and individuals to grow individual trees, with each tree the model can learn the correct combination of covariates to maximize prediction. With each new tree the algorithm "learns" the best combination of covariates to generation a final probabilistic prediction that is based on the "forest" it has generated. These random forest techniques have shown significant promise in predicting propensity score in the presense of complex data associations (CITE). With the recent advances in computing power, non-tree based methods, such as neural networks, have increased in popularity. 

#### Neural Network

Artificial neural networks were initially developed by psychologist Frank Rosenblatt (1958) and his idea of a perceptron, which he thought of as a simplified mathematical model of how the neurons in our brains worked (Figure 1). The perceptron takes in a set of binary inputs (called neurons) and applies a weights (synapse strength) sums across all these weighted inputs and outputs either a 1 or 0 depending on the summed weights. This was meant to mimic how neurons in our brain work, with enough energy a neuron will fire (1) or not (0). Following this straightforward model, work began to expand this concept, and it cumulated with the creation of Artificial Neural Networks (ANN). A diagram of a simple ANN is shown in Figure 2. The network is made up of a series of interconnected "neurons," that are organized in layers (Schalkoff, 1997). There is an input layer; these can be regarded as the covariates in a data frame. 



A series of hidden layers make up the middle portion of the diagram; the last layer is the output layer. In a classification problem, this layer outputs a probability of assignment to k class, can be regarded as a propensity score. The input layer does not do any computations; the hidden layers take in data from the input layers an apply an "activation function" such as a sigmoid function (Schalkoff, 1997) this gets fed forward to the next hidden layer where yet another activation function is applied, these neural networks are sometimes reffered to as "feed-forward neural networks". The presence of hidden layers allows the ANN to learn very complex non-linear relationships associated with the input data. These algorithms have been in extensive use within the machine learning community, but as computation speed increases, Deep Neural Networks are being used (DNN).

#### Deep Neural Networks

DNN is not entirely different from ANN. Their distinguishing feature is the depth (or several hidden layers). ANN that has more than two hidden layers are considered DNN. DNN can capture even more complex relationships amongst covariates. Each layer of neurons trains on a distinct set of features based on the previous layers activation function output (Silver, Huang, Maddison, Guez, Sifre, Van Den Driessche & Dieleman, 2016). The deeper you go inside the DNN, the more intricate features the model can learn. In the classification case, the last output layer is set to be a SoftMax function that returns a value from \[0,1\] the same as a sigmoid function in logistic regression. If we can leverage DNN to assign propensity score weights, perhaps we can improve weighting methods in observational studies.

For the second condition, the literature suggests iterating through various model specifications, with each iteration adding interactions or non-linear terms until a balance is achieved amongst the observed characteristics between treated and un-treated students (Murnane & Willett, 2010). This iterative process becomes increasingly complex as the number of covariates increases and additionally their is no gurantee that the covariate balance will be achieved after these iterations (Diamon & Sekhon, 2013). The consequences are steep if these assumptions are not met, as it will lead to biased treatment effect estimates.

These conditions may be difficult to satisfy in the social science literature, especially education, since behavioral data has many more intricacies than data used in the propensity score simulation literature (Thoemmes & Kim, 2011). Furthemore, much of the applied literature fails to follow the iterative approach in modeling the propensity score. Addtionally, the most widely used model for estimating the propensity score in education are logistic regression models, which are inadequate and lead to biased treatment effects when incorrectly specified (Lee et al., 2010; Pan & Bai, 2015; Setoguchi et al., 2008; Westreich et al., 2010). For example, logistic models tend to degrade when modeling high-dimensional data.

### Application of Propensity Scores

The two most popular way to apply propensity scores to an analysis is through propensity score matching (PSM) or weighting (PSW). These applications attempt to balanced observed covariates between treatment and control groups leading to a less biased estimate of the treatment effect. PSM relies on creating matched sets of treated units with similar PS to control units. In the most simplest case, each treated unit is matched to only one control unit with similar propensity score forming a mached set. For example, if a student who received the job-training program has an estimated propensity score of .70, that student would be matched with a student who did not receive the training program with a similar propensity score, and is expected to be a demographically similar control student and therefore a suitable control. Some limitations do exist with this approach. If the pool of control students is small in comparison to the treatment pool, their may not be suitable matches for students. In addition, sample size and statistical power are a concern when the treatment group is relatively small.

PSW on the other hand uses the propensity score to reweight observastion so that covariates are balanced between treateed and control. Although various, weighting schemes can be used the most popular one being inverse probability weihgtin (IPW) (CITE). Where treated units get assigned a weight of $1/e$, while control units get assigned a weight of $1/(1-e)$. This weighting approach reweights our analytic sample to have relative balance on the propensity score. PSW is the favored approach for propenisty score analysis given it makes use of the entire sample size wihtout dropping observation as with PSM. However, it is not without fault for exmaple, for observation that have relatively large or ...

### 

#### 

### High Dimensional Data

To cndcut Ps correclty you need to balance covariates on sevel because with a low amount , realted to seletion and outcome. with few varaible is is unlikley that we will correctl apture the selection process (steiner, cook, sadihs, & clark, 2010)

### Neural Networks

### Deep Neural Networks

### 

### Assesing quality of PS

\

Machine Learning (ML) in the age of big-data in education is still in its infancy but has already proved to be a useful analytic tool, especially with supervised ML methods such as decision trees, and neural networks. One advantage of ML algorithms is that they are not confined by parametric model assumptions, and with today's technology, they are computationally fast and efficient to implement. In the educational context, most applications of ML algorithms for classification have focused on high-dimensional, cross-sectional data. In contrast, ML classification techniques that are under the umbrella term "Deep Learning" have received comparatively less attention in applied research. In educational research, an important question relates to how to estimate causal estimates from observational data. In comparison to data from randomized experiments, observational data does not allow for randomization into treatment. Individuals self-select into treatment without knowing the underlying mechanism that leads to that selection. To overcome this problem, a researcher can leverage the rich set of covariates that may uncover the underlying mechanism for selection. If we can balance both treatment and control based on these observable covariates, then we can approach estimating unbiased treatment effects without randomization. One way to achieve this is through propensity score estimation. The number of publications in education that use propensity score methods has drastically increased in the past several years (Fan & Nowell, 2011). This is due to the rise in quantitatively rigorous studies in education that rely on robust causal estimates. In education, we continuously deal with observational data in which we hope to extract meaningful causal estimates of an obersational studies in which no intervention was implemented.

intervention that we implemented. Propensity score methods allow us to use observational data to identify a counterfactual for the estimation of causal effects. Propensity score methods were initially introduced by Rosenbaum and Rubin and are defined as the probability of receiving a treatment conditional on a set of observable covariates (1983). In propensity score matching the estimated propensity score is used to "match" treatment units to control units. The aim of conditioning on the propensity score is to achieve balance in the observed covariates between treated and untreated units and recreate an "as good as random" scenario. As educational research moves towards having high dimensional datasets, for example, administrative records, we have access to a rich set of covariates, making propensity score methods appealing. The standard way a propensity score analysis moves forward is first by using the rich set of covariates at the researcher's disposal to estimate the propensity score. This is typically done by logistic regression, where the conditional probability of being assigned to treatment can be estimated via propensity scores. After each observation has an estimated propensity score various techniques can be employed, such as matching (Caliendo & Kopeinig, 2008), or weighing the analysis using the propensity score (Hirano & Imbens, 2001). Estimating propensity scores via logistic regression may lead to biased estimates (d'Agostino, 1998). Theoretically, any methodological technique that can output a probability can be used for propensity score estimation. Something that machine learning techniques have been honed down to do with great success.

In my first study, I aim to overcome the limitation of existing methods by proposing a novel DNN-based approach to propensity score estimations. A precursor to my method -- single-layer feed-forward neural network (NN) -- was initially proposed by Setoguchi et al. (2008) and Keller et al. (2015) for propensity score estimation. Their simulation work showed that a NN could outperform the traditional logistic model, leading to less bias in the estimated treatment effect and improved the covariate balance. NN's only allow for the modeling of relatively simple relationships from the data unlike DNN's, which due to its architecture, can calculate exponentially complicated relationships between variables, making it suitable for high-dimensional data (LeCun et al., 2015).

DNN's are based on how our brains process information through the flow of data through interconnected neurons. DNN's can uncover complex relationships in high-dimensional data with better precision than traditional statistical methods and machine learning methods, which were novel just a decade ago (Hernández-Blanco et al., 2019; LeCun et al., 2015; Schmidhuber, 2014; Zou et al., 2019). A DNNs ability to identify the intricate relationships withing large-volume, high-dimensional data makes these algorithms applicable to causal methods, like propensity score analysis.

The architecture of DNN's comprises of multiple layers of processing units (i.e., neurons) that apply mathematical transformations to inputs fed-forward through various layers (See Figure 1) (Hernández-Blanco et al., 2019; Zou et al., 2019). In each layer, additional transformations are applied based on the previous layers (i.e., learning), creating a network that can learn increasingly complex associations between characteristics. The last layer of the DNN is an output layer, which is adaptable for several uses, like prediction and classification (LeCun et al., 2015). The "deep" in deep neural networks comes from the number of layers between the input and output layers; the deeper the DNN, the more complex relationship the network can estimate (Hernández-Blanco et al., 2019).

DNN's have gained significant attention in the industry and academic research, making considerable advances in image recognition and natural language processing (Krittanawong et al., 2019; LeCun et al., 2015; Zou et al., 2019). Recent investments by Google in software development now makes it easy to develop DNN models using user-friendly programming languages (Doleck et al., 2020; Pang et al., 2019). Though to date, DNN has not been applied to the causal inference.

At first glance, high-dimensional data should be an asset to propensity score analysis since conventional practices stress the importance of including all available variables in the propensity score model, given that it increases the likelihood of capturing all relevant confounders. However, this is not the case. The traditional logistic model for propensity score estimation tends to degrade in performance as more variables are included in the estimation step (Hill et al., 2011). This can manifest in one of several ways when dealing with high-dimensional data, for example: (1) The addition of a large number of variables may lead the logistic model to simply fail to run and not produce the required propensity score. (2) If the model does run successfully, the model will likely overfit the data, meaning that the logistic model begins to model the random error in the data rather than the relationship between variables. This leads to propensity scores that have little to no variability, and are therefore not suitable for propensity score analysis. (3) Lastly, in a high-dimensional setting, it becomes increasingly difficult to iteratively model all of the suitable interactions and non-linearities with a logistic model (D'Amour et al., 2020; Dorie et al., 2019; Hill et al., 2011).

Propensity score estimation is a crucial step in a propensity score analysis. Given that if the propensity score model is misspecified, there may be inadequate overlap between the treatment and control groups' estimated propensity scores. The insufficient overlap makes it challenging to balance the groups based on their pre-treatment characteristics, leading to biased treatment effects (D'Amour et al., 2020; Murnane & Willett, 2010; Rosenbaum, 2010, 2020). In this simulation study, I aim to introduce DNNs into the field of education research and show that their unique properties, such as flexibility and lax model assumptions, are suitable and appropriate for propensity score estimation.

Contributions

In this first study, I aim to make the following methodological contributions.

First, I will incorporate a novel framework for simulating educational data modeled on a large-scale college access intervention study focused on providing outreach on a college-going task to nearly 170k high school students. This data set includes any information the students provided on their college application via the CommonApp. This database is indicative of the administrative data that is becoming more common in education research. Therefore, basing my simulated data on the CommonApp data, I can capture representative relationships and data types, which educational researchers experience in their studies. I will use this data to create simulated data, where I varied the sample size and number of variables and induce associations and treatment effects to test my method's performance in extracting unbiased treatment impacts. Using this framework, I will add to the propensity score simulation literature by simulating real educational data, which moves away from conventional and current propensity score simulation studies in epidemiology that simulate relatively small datasets (Cannas & Arpino, 2019; Karim et al., 2018; Lee et al., 2010; Pirracchio et al., 2015; Setoguchi et al., 2008).

Second, I will extend the simulation literature by testing complexities in both the propensity score and outcome models. This is relatively novel in propensity score literature (Cannas & Arpino, 2019), but more realistic for the education context where complicated relationships between variables, treatment assignment, and the outcome could exist. DNNs have been shown to work well in modeling these complexities by creating a parsimonious model that estimates and evaluates all possible interactions and non-linearities among all variables (Hernández-Blanco et al., 2019; Perrotta & Selwyn, 2019).

Third, I will provide the first evaluation of DNN's applied to propensity score analysis. After simulating the data, I will test the DNN's performance against the logistic model and other well-documented estimation techniques used in propensity score literature (e.g., SVM, CART, BART). I evaluate the methods' performance when implemented in both propensity score-based weighting and propensity score-based matching, by assessing how well they achieve a covariate balance, and the bias in treatment effect estimates will be evaluated. Additionally, I will compare how robust these methods are to an unobserved confounder.

I conducted Monte Carlo simulations to assess this new analytical method's performance, following the guidance of recently published best-practices for simulation studies (Morris et al., 2019). My preliminary simulation results show that deep neural networks outperform the traditional logistic model, and a range of currently available machine learning approaches when interactions and non-linearities are introduced in any data generation models (e.g., propensity score model, outcome model).

## Abtract

Randomized Control Trials (RCTs) are still regarded as the gold standard for establishing causal claims. In education and more broadly social science research, RCTs may be difficult to implement or simply unethical (CITE), leaving researchers to rely on observational data for inference. This places researchers in a precarious position of attributing causal conclusion to correlation analysis (CITE). Thankfully, educational researchers have causal inference methods, such as propensity score analysis, that allow us to approximate the causal claims of an RCT with observational data. \[INSERT TECHNICAL INTRO TO PS ANALYSIS\] As educational research moves towards having access to high dimensional datasets, for example, administrative records. We gain access to a rich set of covariates, making propensity score methods ideal. Propensity score analysis follows a straightforward approach. First, the researchers harnesses a rich set of covariates to estimate the propensity score. This is typically done via logistic regression, where the conditional probability of being assigned to treatment can be estimated . After each observation has an estimated propensity score, various techniques can be employed, such as matching (Caliendo & Kopeinig, 2008), or weighting the analysis by the propensity score (Hirano & Imbens, 2001). Although adanvces have been made in incorporating sophisticated matching techniques \[CITE\]. Yet we have made little advances in the estimation step \[CITE\]. Estimating propensity scores via logistic regression may lead to biased estimates (d'Agostino, 1998 \[CITE\]). Theoretically, any methodological technique that can output a probability can be used for propensity score estimation. Something that machine learning techniques have been honed down to do with great success.

    We have yet to harness the advantages that ML algorithms can provide in classification tasks [CITE]. Furthermore,  ML algorithms  are not confined by parametric model assumptions, and with today's technology, they are computationally fast and efficient to implement. In the educational context, most applications of ML algorithms for classification have focused on high-dimensional, cross-sectional data. In contrast, ML classification techniques that are under the umbrella term “Deep Learning” have received comparatively less attention in applied research. 

In this simulation study, we applied logistic regression and Deep Neural Networks (DNN) to estimate propensity scores. In order to evaluate whether DNN could outperform the standard logistics regression, as well as more robust techniques currently in the literature \[CITE\]. We generate three simulated cohorts of varying sizes with a binary treatment variable and a series of covariates with induced correlation structure. Treatment assignment was constructed as a function of the covariates. Since treatment assignment is known, we have a "ground truth" of what the true propensity score should be. Different data structure scenarios were constructed to evaluate the performance of both models.

## Introduction

In this simulation study, we applied logistic regression and Deep Neural Networks (DNN) to estimate propensity scores to evaluate whether DNN could outperform the standard logistics regression. We generate three simulated cohorts of varying sizes with a binary treatment variable and a series of covariates with induced correlation structure. Treatment assignment was constructed as a function of the covariates. Since treatment assignment is known, we have a "ground truth" of what the true propensity score should be. Different data structure scenarios were constructed to evaluate the performance of both models.\
Machine Learning-Neural Networks Machine learning algorithms have gotten much more exposure in social science, and recent work has shown how educational research can begin to employ these algorithms (Michalski, Carbonell & Mitchell, 2013). Machine Learning is a discipline that is focused on having programs "learn" intricate patterns in the data for prediction. ML is a form of applied statistics with an emphasis on the use of computers to estimate complicated functional forms. These ML algorithms are meant to "learn" from the data we provide, further letting us tackle a very complicated task such as weather prediction or stock predictions (Michalski et al., 2013). These algorithms are particularly suitable at classification problems (Kotsiantis, Zaharaskis & Pintelas, 2007), where the algorithm is asked to specify which of k categories some inputs belong to. To solve this problem, the algorithm produces a function f when y = f(x) the function assigned it to a k class. This is the same problem we are trying to solve with propensity score estimation where we have some function that is trying to find the probability that the unit is assigned to treatment. These methods were left mainly to massive supercomputers, but with the advancement in computing power, these methods are now readily able to be run on any machine. As educational researchers, we must be aware of new methods and be able to use them to analyze our questions appropriately. With a large amount of data that is now available, these methods will one day become standard practice in social science research. Although ML has been used for propensity score estimation and causal inference work (Lee, Lessler & Stuart, 2010), for example BART for propensity score matching, to my knowledge propensity score methods have not leveraged new Deep-Learning algorithms that have slowly come into prominence amongst the ML community (Deng & Yu, 2014). For this paper, I am focused on a particular family of algorithms in deep learning, called deep neural networks (DNN). To begin the discussion around deep neural networks, we have to understand what an artificial neural network is. Neural Networks Artificial neural networks were initially developed by psychologist Frank Rosenblatt (1958) and his idea of a perceptron, which he thought of as a simplified mathematical model of how the neurons in our brains worked (Figure 1). The perceptron takes in a set of binary inputs (called neurons) and applies a weights (synapse strength) sums across all these weighted inputs and outputs either a 1 or 0 depending on the summed weights. This was meant to mimic how neurons in our brain work, with enough energy a neuron will fire (1) or not (0). Following this straightforward model, work began to expand this concept, and it cumulated with the creation of Artificial Neural Networks (ANN). A diagram of a simple ANN is shown in Figure 2. The network is made up of a series of interconnected "neurons," that are organized in layers (Schalkoff, 1997). There is an input layer; these can be regarded as the covariates in a data frame. A series of hidden layers make up the middle portion of the diagram; the last layer is the output layer. In a classification problem, this layer outputs a probability of assignment to k class, can be regarded as a propensity score. The input layer does not do any computations; the hidden layers take in data from the input layers an apply an "activation function" such as a sigmoid function (Schalkoff, 1997) this gets fed forward to the next hidden layer where yet another activation function is applied. The presence of hidden layers allows the ANN to learn very complex non-linear relationships associated with the input data. These algorithms have been in extensive use within the machine learning community, but as computation speed increases, Deep Neural Networks are being used (DNN). DNN is not entirely different from ANN. Their distinguishing feature is the depth (or several hidden layers). ANN that has more than two hidden layers are considered DNN. DNN can capture even more complex relationships amongst covariates. Each layer of neurons trains on a distinct set of features based on the previous layers activation function output (Silver, Huang, Maddison, Guez, Sifre, Van Den Driessche & Dieleman, 2016). The deeper you go inside the DNN, the more intricate features the model can learn. In the classification case, the last output layer is set to be a SoftMax function that returns a value from \[0,1\] the same as a sigmoid function in logistic regression. If we can leverage DNN to assign propensity score weights, perhaps we can improve weighting methods in observational studies.

Randomized Control Trials (RCTs) are still regarded as the gold standard for establishing causal claims. In education and more broadly social science, RCTs may be difficult to implement or simply unethical (CITE), leaving researchers to rely on observational data for inference. This places researchers in a precarious position of attributing causal conclusion to correlation analysis (CITE). Thankfully, educational researcher is experiencing a causal reinansance with the acceptance of well used causal inference methods, such as propensity score analysis. Although, propensity score methods in social science researcher is now widely used (CITE). What has received relatively little attention has been advances in the estimation of the propensity score, which is commonly estimated via a logistic function. Research extending back to the 1970's have proposed advances in the estimation of the propensity score through the use of machine learning algorithms. More recent, work has been done on using the tree based methods .... For propensity score estimation. Yet, current litearature is still relying on logistic regression for the estimation step even though logistic models have many setbacks (CITE).

A good contender for propensity score estimation has been neural networks, many evaluations have looked at the advantages of such algorithms (CITE). Such as non-parametric assumptions, and its flexibility to uncovered complex functional forms of the treatment selection mechanism. As any methodologies neural networks and other methodologies have been updated and left in the past. With the recent improvements in computational speed what has come to the forefront of computations research has been deep learnings, and may be worthy candidate for propensity score estimation.

In this simulation study, we present the first evalutio of deep neural networks (DNN) for propensity score analysis. We examine the the performance of DNN against a main effects logistic regression, CART, Neural Networks, and Random Forrest. We generate three simulated cohorts of varying sizes (n = 50, n = 500, n = 2,000) with a binary treatment variable. We further extend the simulation literature by generating a set of data-generating models: a propensity score data-generating model and an outcome-generation model as initially proposed by Keller et al. (2015). We induce non-linearity and non-additivity to both models to examine the effect that confounding has on the performance of our tested algorithms.

We propose the use of Deep Neural Networks for propensity score analysis

## Literature Review

### Review of propensity scores

In propensity score matching the estimated propensity score is used to "match" treatment units to control units. The aim of conditioning on the propensity score is to achieve balance in the observed covariates between treated and untreated units and

Machine Learning (ML) in the age of big-data in education is still in its infancy but has already proved to be a useful analytic tool, especially with supervised ML methods such as decision trees, and neural networks. One advantage of ML algorithms is that they are not confined by parametric model assumptions, and with today's technology, they are computationally fast and efficient to implement. In the educational context, most applications of ML algorithms for classification have focused on high-dimensional, cross-sectional data. In contrast, ML classification techniques that are under the umbrella term "Deep Learning" have received comparatively less attention in applied research. The number of publications in education that use propensity score methods has drastically increased in the past several years (Fan & Nowell, 2011). This is due to the rise in quantitatively rigorous studies in education that rely on robust causal estimates. In education, we continuously deal with observational data in which we hope to extract meaningful causal estimates of an obersational studies in which no intervention was implemented.

intervention that we implemented. As educational research moves towards having high dimensional datasets, for example, administrative records, we have access to a rich set of covariates, making propensity score methods appealing. The standard way a propensity score analysis moves forward is first by using the rich set of covariates at the researcher's disposal to estimate the propensity score. This is typically done by logistic regression, where the conditional probability of being assigned to treatment can be estimated via propensity scores. After each observation has an estimated propensity score various techniques can be employed, such as matching (Caliendo & Kopeinig, 2008), or weighting the analysis using the propensity score (Hirano & Imbens, 2001). Estimating propensity scores via logistic regression may lead to biased estimates (d'Agostino, 1998). Theoretically, any methodological technique that can output a probability can be used for propensity score estimation. Something that machine learning techniques have been honed down to do with great success.

### Machine Learning and propensity scores

    Machine learning algorithms have gotten much more exposure in social science, and recent work has shown how educational research can begin to employ these algorithms (Michalski, Carbonell & Mitchell, 2013). Machine Learning is a discipline that is focused on having programs "learn" intricate patterns in the data for prediction. ML is a form of applied statistics with an emphasis on the use of computers to estimate complicated functional forms. These ML algorithms are meant to "learn" from the data we provide, further letting us tackle a very complicated task such as weather prediction or stock predictions (Michalski et al., 2013). These algorithms are particularly suitable at classification problems (Kotsiantis, Zaharaskis & Pintelas, 2007), where the algorithm is asked to specify which of k categories some inputs belong to. To solve this problem, the algorithm produces a function f when y = f(x) the function assigned it to a k class. This is the same problem we are trying to solve with propensity score estimation where we have some function that is trying to find the probability that the unit is assigned to treatment.

These methods were left mainly to massive supercomputers, but with the advancement in computing power, these methods are now readily able to be run on any machine. As educational researchers, we must be aware of new methods and be able to use them to analyze our questions appropriately. With a large amount of data that is now available, these methods will one day become standard practice in social science research.

    Although ML has been used for propensity score estimation and causal inference work (Lee, Lessler & Stuart, 2010), for example BART for propensity score matching, to my knowledge propensity score methods have not leveraged new Deep-Learning algorithms that have slowly come into prominence amongst the ML community (Deng & Yu, 2014). For this paper, I am focused on a particular family of algorithms in deep learning, called deep neural networks (DNN). To begin the discussion around deep neural networks, we have to understand what an artificial neural network is. 

### Neural networks

## Simulation

Using Monte Carlo simulations, we compare our deep neural network approach to the traditional logistic model as well existing machine learning approach including X, X, for propenisty score estimation under high dimensional data. In this section I describe the proposed simulation design and analytic procedures.

### Data Generation

We generated three covariate sets of varying sizes (p = 20, 100, 200) of mixed-data types (including..). Our starting condition of p = 20 was chosen given this is the average number of covariates used in applied propenisty score analysis papers in social science research (CITE). Our novel data geenration approach is simulated in order to look like high dimensional "messy data" applied researcher may encounter.

We took a programatic approach to generating the covariates by which we half of covariates (p) where dranf from standard normal distributions, a quarter were drawn from a unifrom distribution and the last quarter from a binomial distribution. Correlation among covariates was induced by taking a quarter of covaraites (p) and inducing a random correlation pull from a beta distribution, which is bounded between 0 and 1. Next, we took a sample of 80% of our covariates, and to 20% of that sample

To answer the research question, we generated three simulated cohorts of varying sample size (n = 100, 1000, 10000) with a binary treatment variable Tx and ten continuous covariates (W_i = 1...10), three dichotomized covariates (D_i = 1,2,3) that were independently associated with Tx. Another 83 covariates were generated that were not associated with Tx (E_i = 1...87). This produced a dataset with 100 covariates and a binary Tx variable. Data were generated by first constructing seven covariates generated as independent standard normal random variables with mean zero and unit variance (W_i = 1...7). Another 3 (W_i= 8, 9, 10) standard normal random variables were created with induced correlations of .1, .5, and .8. An additional 3 dichotomized covariates were created (D_i = 1,2,3). Finally, 87 covariates were generated as independent draws from a standard normal random distribution. The treatment dummy variable (Tx) was generated using linear regression as a function of W_i. The formula to generate the true propensity score was generated with the following structure: 〖(1) Pr〗⁡\[Tx=1│W_i \] =(1+exp⁡{-(β_0+β_1 W_1+β_2 W_2+β_3 W_3+β_4 W_4+β_5 W_5+β_6 W_6+β_7 W_7+β_8 D_1+β_9 D_2+β_10 D_3 )} )\^(-1) To generate the treatment variable, we generated a random number from \[0,1\] from a uniform distribution. Tx was set to equal one if the random number was less than the true propensity score estimated with formula (1). The covariance matrix for the constructed variables is shown in Appendix A. Scenarios to be Tested To compare the performance of logistic regression and DNN for propensity score estimates, we created realistic data structure scenarios that educational researchers could find themselves in. We varied the sample size and number of covariates to develop 6 unique scenarios. Table 1 shows the scenarios that we tested. Different sample sizes were used from 100, 1000, to 10000 as well as the number of covariates from 13 covariates (Low-Dimensional Data) to 100 covariates (High Dimensional Data). This should capture scenarios in which researchers are using very small educational datasets with a few covariates, to a large amount of administrative data that could yield hundreds of covariates. Since we theoretically know the membership assignment to tx for the entire sample we were able to employ cross-validation using the holdout method, in which we can split our dataset into two parts a Training dataset for which we train our model on and a Testing dataset to test our model specifications that were "learned" in the training step (Kohavi, 1995). We used standard holdout, with 75% of the dataset being used to train and 25% being "held out" to test. This avoids overfitting our models. For each scenario, we compared both logistic and DNN in estimating propensity scores. I accessed the performance of each model using an accuracy score given by:

Accuracy=(True Positive+True Negative)/(n )

Simulated data and data analysis was conducted on Python 3 on a Mac platform.

To answer the research question, we generated three simulated cohorts of varying sample size (n = 100, 1000, 10000) with a binary treatment variable Tx and ten continuous covariates ( = 1...10), three dichotomized covariates ( = 1,2,3) that were independently associated with Tx. Another 83 covariates were generated that were not associated with Tx ( = 1...87). This produced a dataset with 100 covariates and a binary Tx variable. Data were generated by first constructing seven covariates generated as independent standard normal random variables with mean zero and unit variance ( = 1...7). Another 3 (= 8, 9, 10) standard normal random variables were created with induced correlations of .1, .5, and .8. An additional 3 dichotomized covariates were created ( = 1,2,3). Finally, 87 covariates were generated as independent draws from a standard normal random distribution. The treatment dummy variable (Tx) was generated using linear regression as a function of . The formula to generate the true propensity score was generated with the following structure:

To generate the treatment variable, we generated a random number from \[0,1\] from a uniform distribution. Tx was set to equal one if the random number was less than the true propensity score estimated with formula (1). The covariance matrix for the constructed variables is shown in Appendix A. Scenarios to be Tested To compare the performance of logistic regression and DNN for propensity score estimates, we created realistic data structure scenarios that educational researchers could find themselves in. We varied the sample size and number of covariates to develop 6 unique scenarios. Table 1 shows the scenarios that we tested. Different sample sizes were used from 100, 1000, to 10000 as well as the number of covariates from 13 covariates (Low-Dimensional Data) to 100 covariates (High Dimensional Data). This should capture scenarios in which researchers are using very small educational datasets with a few covariates, to a large amount of administrative data that could yield hundreds of covariates. Since we theoretically know the membership assignment to tx for the entire sample we were able to employ cross-validation using the holdout method, in which we can split our dataset into two parts a Training dataset for which we train our model on and a Testing dataset to test our model specifications that were "learned" in the training step (Kohavi, 1995). We used standard holdout, with 75% of the dataset being used to train and 25% being "held out" to test. This avoids overfitting our models. For each scenario, we compared both logistic and DNN in estimating propensity scores. I accessed the performance of each model using an accuracy score given by:

Simulated data and data analysis was conducted on Python 3 on a Mac platform.

### Data

### Simulation

## Looking Ahead

As educational research moves towards the age of Big Data, researchers will have access to tremendous amounts of covariates. With such a large number of covariates at our disposal, researchers can leverage causal methods like propensity score estimation. The traditional approach has been to use logistic regression, but as computational power has increased more sophisticated Deep Learning methods have been developed that are well situated to solve classification problems such as propensity score estimation. Results showed that DNN and Logistic regression performed equally well across conditions and sample sizes. In the final iteration of the paper, we will focus on creating a Monte Carlo simulation study to analyze these effects. We will further consider varying the linear relationship the covariates had with the outcome to more complex non-linear patterns, perhaps in that context DNN model would outperform Logistic models.

## Tables and Figures
