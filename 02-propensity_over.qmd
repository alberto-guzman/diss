```{r}
#| include: false
packages <- c(
  "tidyverse",
  "gt",
  "ggthemes",
  "gtsummary"
)

lapply(packages, library, character.only = TRUE)

source("~/Projects/inProgress/dissertation/ggplot_theme_pub.R")


df1 <- readRDS("~/Projects/inProgress/2018_propensity_neuralnet_paper/data/sim_results_n10000_r1000_NP_e.rds")



df2 <- readRDS("~/Projects/inProgress/2018_propensity_neuralnet_paper/data/sim_results_n10000_r1000_P_e.rds")



res <- bind_rows(df1, df2)

over_df <-
  res %>%
  group_by(p, method) %>%
  summarise(
    Bias = mean(Bias),
    Abs_Per_Bias = mean(Abs_Per_Bias),
    Abs_Per_Rel_Bias = mean(Abs_Per_Rel_Bias),
    ATE_se = mean(ATE_se),
    MSE = mean(MSE),
    Power = mean(Power),
    PS_Weights = mean(mean_ps_weights),
    ASAM = mean(ASAM),
    coverage_95 = mean(coverage_95),
    .groups = "rowwise"
  ) %>%
  ungroup()


res_sum_df <-
  res %>%
  group_by(p, method, scenarioT, scenarioY) %>%
  summarise(
    Bias = mean(Bias),
    Abs_Per_Bias = mean(Abs_Per_Bias),
    Abs_Per_Rel_Bias = mean(Abs_Per_Rel_Bias),
    ATE_se = mean(ATE_se),
    MSE = mean(MSE),
    Power = mean(Power),
    PS_Weights = mean(mean_ps_weights),
    ASAM = mean(ASAM),
    coverage_95 = mean(coverage_95),
    .groups = "rowwise"
  ) %>%
  ungroup()

```

# Paper 1: Evaluating modern propensity score estimation methods with high-dimensional data

## Introduction

Traditionally, questions of causality have been addressed through the use of randomized control trials (RCT), in which individuals are randomly assigned to either a treatment group that receives an intervention or a control group [@murnane2010methods; @Rosenbaum.2010]. Randomization ensures that, on average, both groups are balanced on all observable and unobservable characteristics, eliminating the possibility that an intervention's effect is due to differences in group characteristics [@Rosenbaum.2010]. However, in education and broader social science research, ethical, financial, or practical obstacles can prevent a researcher from conducting a randomized experiment. Instead, researchers must rely on non-experimental or observational studies in which students self-select or are placed in an intervention without randomization.

Observational data complicates causal inference because, in the absence of randomization, an imbalance in the distribution of student covariates between groups could lead us to incorrectly attribute a change in outcome to the intervention or policy, as opposed to differences in group composition [@Rosenbaum.2010]. A vast body of methodological literature has centered on developing quasi-experimental methods that attempt to address the internal validity issues associated with observational data by balancing the *observed* characteristics of those who received treatment and those who did not. One such popular approach is propensity score analysis [@Stuart.2010; @Lee.2010].

Propensity score analysis is one of the most commonly employed quasi-experimental methods in education research [@Fan.2011; @Powell.2020; @Rosenbaum.2010; @Stuart.2010; @ho2007matching; @Lee.2010]. Propensity score analysis attempts to balance *observed* characteristics between treated and untreated students, just as randomization to treatment and control conditions creates balance on observable characteristics. The propensity score represents the probability that a student *would have* been exposed to treatment, conditional on observed covariates [@Rosenbaum.1981; @Rosenbaum.1984]. Propensity scores can be used in a variety of ways to balance student covariates, including propensity score matching, in which treated and untreated students with similar propensity scores are matched, and propensity score weighting, in which treatment groups are re-weighted so that observed covariates are balanced between both groups, and stratification where the propensity score is used to group students into unique strata with similar propensity scores with the goal of balancing observed covariates within each stratum [@Pan.2018qcl].

However, to estimate unbiased treatment effects, the researcher must correctly specify the propensity score model such that:

1.  The propensity score model captures *all* covariates associated with treatment assignment, omitting no potential confounding variables [@Rosenbaum.1984; @Rosenbaum.1981].

2.  The propensity score model accurately models the relationship between covariates and treatment selection, meaning that all appropriate interactions and non-linear terms are specified [@Rosenbaum.1987; @Rosenbaum.1984; @Rosenbaum.1981].

The literature suggests a "kitchen sink" approach for variable selection to guard against the first condition because the penalties are high if a potential confounder is *not* included in the propensity score model, resulting in biased treatment effect estimates [@Pirracchio.2015; @Shortreed.2017lu8; @Stuart.2010]. Given some confounders are simply unmeasured, many have proposed the use of sensitivity analysis to understand how the exclusion of an unmeasured confounder could influence treatment effects (CITE). For the second condition, the literature suggests iterating through various model specifications, with each iteration adding interactions or non-linear terms until a balance is achieved among the observed characteristics between treated and untreated students [@murnane2010methods; @Rosenbaum.2010].

At first glance, high-dimensional (i.e., big data) data should be an asset to propensity score analysis, since conventional practices stress the importance of including *all* available variables in the propensity score model, given that it increases the likelihood of capturing all relevant confounders. Nevertheless, this is not the case. The performance of the traditional logistic model for estimating propensity scores tends to degrade as more variables are included in the estimation step [@Hill.2011soh].

This can manifest itself in a variety of ways when dealing with high-dimensional data, such as:

1.  A large number of covariates may prevent the logistic model from converging and produce extreme propensity scores [@Hill.2011soh; @Collier.2022].

2.  If the model runs successfully, it is likely to overfit the data. Meaning it will begin to model random error in the data rather than the relationship between variables [@Hill.2011soh; @ho2007matching; @Stuart.2010; @Lee.2010]. This may result in propensity scores with little to no variability.

3.  In a high-dimensional context, it becomes increasingly difficult to iteratively model all of the appropriate interactions and non-linearities with a logistic model [@d2020underspecification; @Dorie.2019; @Hill.2011soh].

With the rapid emergence of increasingly sophisticated machine learning algorithms, there are increasing possibilities for developing and applying new analytic methods that can flexibly estimate propensity scores with high-dimensional data [@daniel2019big; @Hill.2011soh; @hernandez2019systematic; @williamson2017big]. These machine learning algorithms enable an automatic, data-driven method of capturing non-linearities and non-additivity in the propensity score estimation model, which may result in a more balanced covariate distribution without the need for tedious iteration. To date, however, no research has examined the performance of these machine learning methods when applied to high-dimensional data, as the majority of previous propensity score simulation research has focused on simulated low-dimensional datasets [@Setoguchi.2008; @Stuart.2010; @Lee.2010].

Furthermore, recent advances in computer science have generated a wealth of research on a potential method that could be used to estimate the propensity score, deep neural networks (DNN) [@LeCun.2015; @hernandez2019systematic; @LeCun.2015]. DNNs are algorithms based on the neural architecture of the human brain. They are commonly used in industry to model complex prediction and classification tasks and have already proven to excel in modeling high-dimensional data [@LeCun.2015]. DNNs are highly flexible. They are capable of capturing complex interactions and non-linearities and enable automatic variable selection [@hernandez2019systematic; @LeCun.2015; @schmidhuber2015deep; @Pang.2019].

Therefore, my first dissertation study aims to assess the performance of the traditional logistic model and other machine learning methods with high-dimensional data, using propensity score weighting. In addition, I overcome the limitations of existing methods by proposing a novel DNN-based approach to the estimation of propensity scores and illustrate a new framework for generating multi-type, correlated high-dimensional data. Currenlty avaialble simulation literature is based on genomic data, or simple datasets...

The remainder of this document is structured as follows: First, I review the relevant literature for this study. Then, I describe my proposed data and simulation approach.

\newpage

## Literature Review

In this section, I review the literature on causal inference and propensity score analysis, focusing on machine learning approaches to estimating propensity scores. In addition, I draw on the literature on neural network approaches, specifically deep neural networks, to motivate the development of my DNN-based method for estimating propensity scores.

### Neyman-Holland-Rubin Model for Causal Inference

My approach to causal inference is based on the Neyman-Holland-Rubin model, hereafter referred to as the Rubin model [@rubin1976inference; @rubin2005causal; @holland1986statistics; @neyman1923application]. A core tenet of the Rubin model states that a treatment effect is the difference between two potential outcomes for an individual [@rubin2005causal]. For a dichotomous treatment variable ($Z$), let $Z_i = 1$ if the $i$'th student was in the treatment group -- say a college access program - and $Z_i = 0$ if they were not. Let $Y_{iz}$ be a potential outcome for student $i$ depending on treatment assignment $Z$, such that an individual student has two potential outcomes. $Y_{i1}$ is the potential outcome had student $i$ participated in the college access program and $Y_{i0}$ is the potential outcome had that same student not participated. Therefore, the treatment effect of the college access program for student $i$ would simply be the difference in their potential outcomes:

$$
\tau_i = Y_{i1} - Y_{i0}
$$ {#eq-treat}

However, in the real world, we do not simultaneously observe both potential outcomes for the same student. That is to say, we can only observe student $i$'s potential outcome in the college access program if they participated in the program. This is why the fundamental problem of causal inference is a missing data problem [@holland1986statistics]. Although we cannot directly observe these potential outcomes under certain assumptions, we can estimate the average treatment effect (ATE) across our sample, defined as:

$$
ATE= \mathbb{E}(Y_{i1} -Y_{i0})=\mathbb{E}(Y_{i1})-\mathbb{E}(Y_{i0})
$$ {#eq-ate}

Where $\mathbb{E}(Y_{i1})$ is the expected value of all students in the treatment group and $\mathbb{E}(Y_{i0})$ is the expected value of all students in the control group. In the context of a true experiment in which students are *randomly* assigned to treatment conditions, we can estimate the ATE in a straightforward manner since with a large enough sample size, both treated and control students will be balanced on all observable and unobservable characteristics. Stated differently, a randomized experiment ensures that $Y_1$ and $Y_0$ are independent of treatment assignment ($Z$), and therefore, the treatment effect can be regarded as causal [@Rosenbaum.2010].

In addition, we may also be interested in estimating the average treatment effect among those who actually received the treatment. In this case, we may be interested in the average treatment effect on the treated (ATT) [@Rosenbaum.2010], which is defined as:

$$
ATT= \mathbb{E}(Y_{i1} -Y_{i0}|Z_i=1)=\mathbb{E}(Y_{i1}|Z_i=1)-\mathbb{E}(Y_{i0}|Z_i=1)
$$ {#eq-att}

In practice, the ATT is the most policy-relevant estimand since its the treatment effect on students who took up treatment [@Stuart.2010].

### Observational Studies

In behavioral and social science research, ethical, cost, or practical barriers can preclude a researcher from conducting a randomized experiment and instead must rely on using non-experimental or observational data [@Bai.2011; @Pan.2018qcl]. As opposed to RCT's, students in observational studies either self-select or are placed into an intervention without randomization. This complicates causal inference because without randomization, there may be differences in students observable and unobservable characteristics between those who received the treatment and those who did not that may influence a students outcome [@Austin.2011].

For example, say that in are college-access program example, students were not randomized into treatment or control. Instead, the students self-selected into treatment. In this scenario, the underlying treatment assignment is unknown. It could be that students who self-select into the program are generally more interested in going to college than those who did not sign up. Given this imbalance in college interest, we may incorrectly attribute a increase in college enrollment to the treatment instead of differences in college interest between treated and untreated students. In other words, observational studies *cannot* ensure that a student's outcome is independent of treatment assignment ($Z$) [@Rosenbaum.2010].

Various statistical methods have been proposed that focus on estimating unbiased treatment effects in observational studies. These include instrumental variable approaches, regression discontinuity, synthetic control, and propensity score analysis [@Austin.2011; @Bai.2011; @Stuart.2010; @Rosenbaum.2010]. For my dissertation, I focus on propensity score analysis which leverage student-level covariates to create a balancing score that can account for *observed* group differences.

\newpage

### Propensity Scores

Rosenbaum and Rubin initially proposed the concept of a propensity score as a means to increase the accuracy of capturing unbiased treatment effects in observational studies [@Rosenbaum.1984; @Rosenbaum.1981]. Propensity score analysis is one of the most popular quasi-experimental methods among social and behavioral researchers, particularly among applied educational researchers [@Lee.2010; @ho2007matching; @Stuart.2010].

Rosenbaum and Rubin define a propensity score, $e(x)$ as the probability of assignment to treatment conditional on observed pre-treatment covariates. Defined as:

$$
e(x) = Pr(Z=1|\boldsymbol{X})
$$ {#eq-ps}

Propensity scores are typically estimated as predicted probabilities from a logistic regression in which a binary outcome variable indicating whether a unit received treatment is being predicted by observed individual characteristics ($\boldsymbol{X}$). They range from 0 to 1, with values closer to 1 indicating that an individual is more likely to be assigned to treatment. Rosenbaum and Rubin demonstrate that if the propensity score is balanced between treatment and control groups, then the distribution of observed covariates between treatment and control is also balanced [@Rosenbaum.1981; @Rosenbaum.1984; @Rosenbaum.2022]. Therefore, theoretically, two students with similar propensity scores should have a similar underlying distribution of observed covariates.

Researchers can condition on the propensity score using a variety of techniques, including matching [@ho2007matching], which pairs treated students with control students based on the proximity of their propensity score; stratification, which groups students into unique strata based on their propensity score [@Rosenbaum.2010]; and weighting [@Lee.2010], which reweighs students in the sample so that the distribution of the propensity score of the control group is similar to that of the treated group. All these methods have a similar aim of using the propensity score to adjust for the imbalance in the covariate distribution between treated and control units to approximate true randomization conditions. Therefore, by conditioning on the propensity score through matching, stratification, or weighting, we can eliminate bias in our treatment effect due to observed confounding and approximate the true causal effect [@Pan.2018qcl]. However, for this to be true, certain assumptions must hold.

#### Assumptions

Several assumptions must be met to estimate an unbiased treatment effect using propensity scores. These include the stable unit value assumption (SUTVA), the strong ignorability assumption, and region of common support [@rubin2005causal].

The first assumption, the stable unit treatment value assumption (SUTVA), states that 1) their is only one version of the treatment condtion, and 2) the potential outcome of an individual is unaffected by the treatment assignment of any other individual [@Rosenbaum.2022]. In other words, the outcome of a student is independent of whether another student is assigned to the treatment or control group.

The strong ignorability assumption states that the potential outcome of an individual is conditionally independent of treatment assignment if we successfully conditioned on all covariates related to treatment, $Y_1, Y_0 \perp Z|\boldsymbol{X}$. In experimental studies, this assumption is satisfied by the act of randomization, since randomization ensures that treatment assignment is independent of an individual's outcome [@rubin2005causal; @Rosenbaum.2022]. However, in observational studies, this assumption holds if *all* covariates related to treatment assignment are included in the propensity score model [@Rosenbaum.1981; @Rosenbaum.1984]. In practice, this is unlikely and therefore sensity analysis have been proporsed as necesarry to asses the influence of unmeasured confounding (CITE).

Finally, the third assumption is the region of common support. This assumption states that there must be sufficient overlap in the estimated propensity score distribution between the treatment and control groups [@Rosenbaum.2022]. With sufficient overlap, we can ensure that we can find suitable matches for treated students. We can asses if we have adequate support by plotting the distribution of the propensity scores in the treatment group and control group overlayed and aassesing if the distributions succifiend overlap (CITE).

If these assumptions hold, we can estimate causal effects from observational data. Unfortunately, many of these assumptions are untestable [@Stuart.2010; @ho2007matching; @Lee.2010]. For example, in observational data, the majority of the time, we are blind to the underlying treatment assignment mechanism, and therefore we cannot be sure that *all* covariates related to treatment assignment are included in our propensity score model [@Stuart.2010]. In practice, propensity score analysis should be viewed as a means of reducing some, but not all, of the bias between the treatment and the outcome in observational studies. Since we cannot ensure that all confounding variables are included in our propensity score model. Therefore, special attention should be paid to which covariates are selected to be included in the propensity score model as well as perform sensitity analysis to asses the influence of unmeasured confounding (CITE).

#### Covariate Selection

A reasonable approach to selecting covariates in propensity score analysis is selecting covariates that are theoretically sound and related to treatment assignment and the outcome [@Stuart.2010; @Bai.2011; @Pan.2018qcl]. Since in order to estimate unbiased treatment effects correctly using propensity scores, we have to capture *all* covariates related to the selection into treatment, leaving out no potential confounders [@Rosenbaum.2010]. However, in practice, it may be impossible to know which covariates are confounders; therefore, the literature suggests a "kitchen sink" approach for variable selection to guard against the penalties of excluding potential confounders [@Pirracchio.2015; @Shortreed.2017lu8]. In addition, the propensity score model must capture the correct functional form of the covariates, meaning that all proper interactions and non-linear terms are specified [@Rosenbaum.2010; @Pan.2018qcl]. If the propensity score model is misspecified, it will lead to biased treatment effect estimates [@Rosenbaum.1984; @Rosenbaum.1981; @Guo.2009].

To better capture the relationship among covariates, the literature suggests iterating through various model specifications, with each iteration adding interactions or non-linear terms until a balance is achieved among the observed characteristics between treated and untreated students [@murnane2010methods; @Rosenbaum.2010]. This iterative process becomes increasingly cumbersome as the number of available covariates increases.

\newpage

### Estimating Propensity Scores

The most critical step in propensity score analysis is estimating the propensity score ($e(x)$), since a misspecified propensity score model will yield propensity scores that are not well suited to correctly balanced observed characteristics. Any parametric or non-parametric model can generate propensity scores as long if it outputs a bounded probability between 0 and 1. Logistic regression is the most common parametric model used for estimating the propensity score, especially in applied educational research [@Stuart.2010; @ho2007matching; @Lee.2010; @Collier.2021lp; @Keller.2015q8].

#### Logistic Regression

Logistic regression is the most common model used to generate propensity scores in the social science literature due to its ease of interpretation and familiarity with many applied researchers and its ability to accommodate both continuous and categorical covariates[@Keller.2015q8]. The logistic model is defined as follows:

$$
logit(Z = 1|\boldsymbol{X})=log\left( \frac{Pr(Z=1|\boldsymbol{X})}{1-Pr(Z=1|\boldsymbol{X})} \right) = \beta_o + \beta_1X_1 + \cdots + \beta_pX_p
$$ {#eq-log}

Where $Z$, a binary indicator equal to 1 if a student is in the treatment group and 0 otherwise, regressed on $\boldsymbol{X}$ a vector of observed covariates. The output of the logistic regression is predicted probabilities that are continuous and bounded between 0 and 1 (i.e., propensity scores).

Although logistic regression is the most widely used model for estimating propensity scores, it may not be the most valid for inference. Various simulation studies suggest that using logistic regression for modeling propensity scores is inadequate and leads to biased treatment effects when incorrectly specified, given that parametric models require assumptions regarding the functional form and variable distributions [@Lee.2010; @Pan.2018qcl; @Setoguchi.2008; @Westreich.2010]. This is especially true when estimating propensity scores with hundreds of covariates [@Hill.2011soh]. The logistic model tends to degrade and estimate a propensity score that falls outside the desired 0 to 1 bound in a high-dimensional context [@Hill.2011soh; @Collier.2021lp; @Keller.2015q8]. With the ever-increasing amounts of "big" administrative data at the disposal of applied behavioral and social science research, the degradation of logistic regression is worrisome. However, outside social science, research has suggested alternatives to logistic regression in estimating the propensity score that use non-parametric machine learning algorithms [@Stuart.2010; @ho2007matching; @Lee.2010; @Setoguchi.2008; @Watkins.2013; @Westreich.2010; @Cannas.2019].

#### Machine Learning Approaches

Machine learning algorithms have recently gained popularity in the causal inference and propensity score literature due to these algorithms being highly flexible and able to model complex functional forms iteratively without explicit manipulation from the research [@pearl2019seven; @grimmer2015we; @athey2015machine; @cui2020causal]. With a simple modification, these algorithms can output bounded probabilities that can be used as propensity scores. Given the success of machine learning methods for prediction tasks, it is reasonable to think they are worthy candidates for the propensity score estimation problem, which I view as a prediction problem. Broadly, machine learning algorithms used in propensity score estimation can be classified into tree-based, ensemble methods, and -- the focus of this dissertation -- neural network-based approaches.

#### Tree Based Methods

One popular machine learning approach to estimating propensity score is classification and regression trees (CART) [@li1984classification]. This algorithm recursively divides data into subsets based on individual covariates to predict the probability of membership assignment of a given individual. To generate propensity scores, the outcome is set to a binary indicator of treatment assignment ($Z$). This algorithm splits covariates by level of importance, with the first split being the covariate that can produce the most distinctive split. For example, if a student above 15 years of age is related to treatment assignment, the first split would split the data into students with $age > 15$ and $age < 15$. The algorithm continues by splitting the data by the next most influential covariate split, creating a tree-like structure, see @fig-decision_tree. Splitting stops when the data are binned into unique "branches," which minimizes the prediction error in which an additional split would not improve the prediction of treatment assignment. The final "branches" of the model represent the individual grouping of students with similar propensity scores [@Stuart.2010]. The output of CART is the predicted probabilities which are then used as propensity scores. Like logistic regression, the CART algorithm can handle continuous and categorical covariates . However, unlike logistic regression, CART is insensitive to outliers and can automatically model interactions and higher-order terms [@Lee.2010]. Through a simulation, Lee et al. (2009) found that CART outperformed logistic regression in bias reduction and covariate balance when the data had complex associations.

[![Simple decision tree predicting students GPA based on gender and homework grade](figures/decision_tree.jpg){#fig-decision_tree fig-align="center" width="436"}](https://www.apa.org/science/about/psa/2018/04/classification-regression-trees)

However, a well-documented issue with CART is that the single tree may likely overfit the data [@Stuart.2010; @ho2007matching]. Therefore, a modified version of CART was developed that "prunes" back branches that do not lead to a reduction in prediction error, referred to as pruned-CART [@gelfand1989iterative], which generates a tree that is less likely to overfit the data. Although pruning can alleviate some of the changes in overfitting of the data, a significant drawback of CART is that it relies on a single tree, which may be weak in predicting the propensity score.

#### Ensemble Methods

Ensemble methods are a family of algorithms that generate multiple trees predicting treatment assignment [@Keller.2019; @Stuart.2010; @ho2007matching; @Lee.2010]. Exploiting differences among each tree ensemble method can significantly improve prediction, given that many weak trees together can create better prediction than any single tree [@Lee.2010]. A popular ensemble method is bootstrapped aggregated CART, referred to as Bagged CART. Bagged CART involves fitting multiple CARTs (i.e., trees) on bootstrapped data samples [@Lee.2010]. By "growing" trees on bootstrapped samples, we can reduce the chances of overfitting our data. Each tree will generate a probability that an individual will be assigned to treatment, with a final probability of membership based on aggregating the probability of assignment across all trees. A more robust ensemble method is random forest [@biau2016random]. Compared to CART and Bagging, the random forest algorithm grows trees by randomly selecting covariates and individuals to grow individual trees. With each new tree, the algorithm "learns" the best combination of covariates to generate a final probabilistic prediction based on the "forest" it has created. These random forest techniques have shown significant promise in predicting propensity scores in the presence of complex data associations [@Cannas.2019; @Lee.2010]. However, with the recent advances in computing power, non-tree-based methods, such as neural networks, have increased in popularity [@Collier.2021lp; @Setoguchi.2008; @Keller.2015q8].

\newpage

### Artificial Neural Network Architectures

#### Artificial Neuron

The building block for artificial neural networks was initially developed by psychologist Frank Rosenblatt (1958) with their idea of a perceptron or artificial neuron, see @fig-perceptron. An artificial neuron is a simplified mathematical model of neurons in our brain [@james2013introduction]. Neurons are biological switches that take input signals from other neurons that cause the neuron to fire. Neurons can be thought of as simple processing units. In our brain, these individual neurons are connected into vast networks of billions of neurons, where the outputs of one neuron become inputs of another, allowing for the transmission of complex information [@schmidhuber2015deep; @LeCun.2015].

![Structure of a perceptron - artificial neuron](figures/perceptron.png){#fig-perceptron fig-align="center" width="437"}

An artificial neuron mimics the function of a biological neuron. The artificial neuron takes in a set of inputs of $p$ variables, $X_i=(X_1, X_2, \dots, X_p)$ that have corresponding weights $w_i=(w_1, w_2, \dots ,w_p)$ that represent the different input strength of each variable. Each variable is then multiplied by its corresponding weight. These weighted variables are then summed together, and a bias term is added ($\beta_0$) to create a linear transformation of the input variables. Weight and bias terms are analogous to the slope and intercept of linear regression. The linear transformation is then passed to an activation function $g(z)$. The activation function takes in the linear transformation and performs a computation that generates an output ($\hat{y}$), which outputs either a 1 or 0. This activation function can also be set to be a linear function, such as the sigmoid function, which outputs values between 0 and 1.

We can express this artificial neuron generally as:

$$
\hat{y} = g(z) = g(\beta_0 + \sum_{i=1}^{p}X_iw_i)
$$ {#eq-nn}

This simple neuron can be used to solve a binary classification problem. In our case, we are interested in classifying each student in our sample as belonging to the treatment or control condition conditional on observed covariates. If we set the activation function $g(z)$ to a sigmoid function, we are guaranteed an output value between 0 and 1 (i.e., propensity score). For the neuron to make accurate predictions for each student, it must "learn" the correct weight and bias parameter specifications. This is analogous to finding the right slope and intercept, which produce a line of best fit for bivariate data.

This learning process works by taking each observation in our sample and feeding it into the neuron. In which the first pass has initialized weights and bias set to a random number. The outcome variable is set to the binary indicator of treatment assignment, so we obtain a predicted probability of treatment assignment for each student. Our initial pass of the data will likely generate wrong predictions since the weights and bias terms are initialized to random numbers. In other words, our prediction of treatment assignment ($\hat{y}$) will be far from the actual treatment assignment ($y$), with an associated loss.

The goal of the neuron and most machine learning algorithms is to correctly learn the values of the weights and bias parameters that minimize a loss function, for example, the mean squared error (MSE) [@james2013introduction; @Pang.2019]. The neuron will update the bias and weight parameters iteratively for each subsequent data pass to minimize this loss function. This update is commonly done using a process known as backpropagation [@schmidhuber2015deep]. After a user-defined number of iterations, the neuron will be optimized with bias and weight terms that best predict treatment assignment. A single neuron may be appropriate for simple classification problems with linear covariate associations. As the complexities of associations between covariates increase, more neurons are needed to learn complex functional forms.

#### Single-Layer Neural Network

Following the development of the artificial neuron, work began to model more complex processing units composed of interconnected artificial neurons organized in layers, culminating with the creation of the single-layer feed-forward neural network (NN) (@fig-nn)\[@Schmidhuber.2015; @Collier.2022; @Keller.2015q8\]. NN are referred to as single-layer because of the single middle layer of interconnection artificial neurons. These NN can learn very complex data representations by generating various non-linear representations of the data through multiple artificial neurons [@james2013introduction]. These non-linear outputs are passed into a final layer which pools the outputs of the individual neurons into a non-linear function, $f(x)$, which generates a prediction ($\hat{y}$). By having multiple artificial neurons calculating predictions on various non-linear combinations of the input variables, these networks can learn complex non-linear and non-additive associations between covariates.

![Structure of single-layer neural network](figures/nn.png){#fig-nn fig-align="center" width="493"}

\newpage

The NN comprises a series of artificial neurons organized into three layers[@james2013introduction]. The first layer is the input layer of $p$ variables, $X_i=(X_1, X_2, \dots, X_p)$, our observed covariates. Those input variables are fully connected to the second layer - a hidden layer - made up of $K$ hidden neurons, $A = (A_1, A_2, \dots, A_k)$, which are identical to the artificial neurons mentioned previously, except that the activation function is swapped out to a non-linear function. Each hidden neuron calculates its own prediction based on its unique non-linear combination of variable inputs and associated weight and bias parameters. The last layer is the output layer, which pools the non-linear outputs of the hidden neurons and passes them as inputs to a non-linear output function, $f(x)$, which generates a final prediction ($\hat{y}$). The interconnected neurons allow NN to learn more complex associations between covariates and treatment assignment than a single artificial neuron [@james2013introduction; @Schmidhuber.2015; @hernandez2019systematic].

We can express a NN generally as[^02-propensity_over-1]:

[^02-propensity_over-1]: This notation was adapted from James, Witten, Hastie and Tibshirani, 2021

$$
\hat{y} = f(x) = B_0 + \sum_{k=1}^{K}B_kh_k(X) = B_0 + \sum_{k=1}^{K}B_kg(w_{k0}+\sum_{i=1}^{p}w_{ki}X_i)
$$ {#eq-nn_gen}

Where each hidden unit $A_k, k =1,\dots,K$ is created from a weighted linear combination of input variables, $X_1,X_2, \dots,X_p$ that are applied to an activation function $g(z)$ resulting in an "activation" for each hidden neuron, $A_k$:

$$
A_k = h_k(X) = g(w_{k0}+\sum_{i=1}^{p}w_{ki}X_i)
$$ {#eq-nn_activation}

$w_{k0}$ and $w_{ki}$ are the bias and corresponding weights for each $A_k$ hidden neuron. We can regard these activations as individual predictions of each hidden neuron. These predictions are then pooled and passed to the output function ($f(x)$), which has its bias term, $B_0$, and produces a final prediction ($\hat{y}$).

The NN learning process works in a similar way to a single neuron, but instead of learning only one set of weights and bias terms, it learns multiple simultaneously. Similarly, the ultimate goal of the NN is to learn the correct weights and bias parameters so that the loss function is minimized. In contrast to a single artificial neuron, NN can learn complex non-linear relationships in a computationally efficient manner[@james2013introduction]. Few studies have looked at the overall performance of NN in estimating propensity scores [@Setoguchi.2008; @Cannas.2019; @Westreich.2010; @Keller.2015q8; @Collier.2021lp]. These studies find that the NN are suitable for estimating the propensity score and generally outperform logistic regression in bias reduction and covariate balance [@Setoguchi.2008; @Cannas.2019; @Westreich.2010; @Keller.2015q8; @Collier.2021lp].

Modern approaches use NN with more than one hidden layer and a varying number of hidden neurons. This NN architecture is referred to as deep neural networks (DNN) or multilayer neural network [@LeCun.2015].

#### Deep Neural Networks

The DNN distinguishing feature is the multiple hidden layers that allow it to capture even greater complex relationships among covariates [@Pang.2019; @hernandez2019systematic]. These multiple hidden layers make this neural network architecture "deep." @fig-nn shows a diagram of a DNN with two hidden layers ($L_1, L_2$).

![Structure of deep neural network with two hidden layers](figures/dnn.png){#fig-dnn fig-align="center"}

The first hidden layer functions exactly as the middle layer of a NN @eq-nn_activation. The first layer of this DNN is represented as:

$$
A_k^{(1)}=h_k^{(1)}(X) = g(w_{k0}^{(1)}+\sum_{i=1}^{p}w_{ki}^{(1)}X_i)
$$ {#eq-dnn-1}

Where each hidden unit in the first layer $A_k^{(1)}, k =1,\dots,K$ is created from a weighted linear combination of input variables, $X_1,X_2, \dots,X_p$ that are applied to a non-linear activation function $g(z)$ that results in an output for each hidden neuron in the first layer, $A_k^{(1)}$.

The second hidden layer $L_2$ takes as inputs the outputs of $A_k^{(1)}$ of the first hidden layer and computes a new non-linear transformation, $A_k^{(2)}$. The new activations of the second layer are calculated as follows:

$$
A_l^{(2)}=h_k^{(2)}(X) = g(w_{l0}^{(2)}+\sum_{k=1}^{K_1}w_{lk}^{(2)}A_k^{(1)})
$$ {#eq-dnn-2}

Where each hidden unit in the second layer $A_l^{(2)}, l =1,\dots,K$ is a function of the output of the weighted non-linear combinations of $A_k^{(1)}$, this process can be extended to additional hidden layers. As data move from layer to layer, DNN can approximate increasingly complex functional forms [@LeCun.2015; @schmidhuber2015deep; @james2013introduction]. The final layer of the DNN aggregates the outputs of the second layer and feeds them into an output function that generates a prediction. Although any number of hidden layers can be chosen, it comes with computational costs as the number of hidden layers increases [@LeCun.2015]. Outside of image and speech recognition, only a few hidden layers are needed to capture almost any functional form, with three hidden layers being the "goldilocks" number [@james2013introduction].

#### Deep Neural in Causal Inference

DNN can uncover complex relationships in high-dimensional data with better precision than traditional statistical and machine learning methods, which were novel just a decade ago [@hernandez2019systematic; @LeCun.2015; @Schmidhuber.2015]. A DNN capability to uncovere these intricate relationships within large-volume, high-dimensional data makes these algorithms applicable to causal methods, such as propensity score analysis.

DNN have gained significant attention in industry and academic research, making considerable advances in image recognition and natural language processing [@LeCun.2015; @james2013introduction]. Recent investments by Google in software development now make it easy to develop DNN models using user-friendly programming languages [@Pang.2019].

To date, DNN has not been sufficiently evaluated in the broader literature on causal inference, with most research residing within medical or genomic research [@Kale.2015; @Iglesias.2021; @LeCun.2015]. DNN have been relatively unexplored in social science due to the lack of knowledge on how to run DNN and assess to high-dimensional data [@Farrell.2021]. With the increasing availability of high-dimensional datasets in social science research, DNN will undoubtedly become more popular and develop into an exciting line of research in the coming years. However, relatively little work has incorporated DNN into propensity score analysis.

Only two medical peer-reviewed studies have attempted to use DNN for propensity score estimation [@Whata.2022; @Weberpals.2021]. @Weberpals.2021 use data from around 130,000 cancer patients to construct a simulated data set with 31 baseline covariates in which treatment receives a fictional cancer drug, and the outcome is a binary survival indicator. Using a specific DNN architecture called an "autoencoder" with three hidden layers, they find that DNN reduces confounding bias in treatment effect over the traditional logistic model with manual variable selection. In contrast, @Whata.2022 examine the performance of logistics regression compared to DNN with four hidden layers using a simulation (with 15 covariates) and a real-world application. In their simulation, they varied the level of complexity in the population treatment model but not the outcome model. They found that DNN outperformed logistic regression in terms of covariate balance, classification accuracy, and absolute relative bias.

Both studies speak to the increase in interest in using DNN for causal inference and shed light on the possibilities that DNN are a valid approach to estimating the propensity score. My dissertation builds on these studies by comparing DNN with traditional logistic regression and several machine learning approaches well established in the propensity score simulation literature. In addition, my simulation test complexities in both the outcome *and* treatment models. This study will be the first to test DNN for propensity score estimation outside medical research and with high-dimensional data that mimic high-dimensional administrative datasets encountered by educational researchers.

\newpage

## Method

The focus of my second dissertation study is to evaluate the performance of various modern machine learning techniques, including classification and regression tree (CART), bagged CART, random forest, and a single-layer neural network, against the traditional logistic regression and my proposed DNN-based approach using high-dimensional data. Specifically, I will evaluate the impacts of various degrees of data dimensionality (i.e., low vs high-dimensional context) and misspecifications in both the population treatment and outcome models on the estimate of the treatment effect and covariate balance.

To achieve this, I will use a Monte Carlo simulation design informed by previous propensity score estimation studies conducted by Setoguchi et al. (2008), Lee et al. (2009), and Cannas and Arpino (2019). My simulation code will begin with Cannas and Arpino's publicly available code from their 2019 manuscript and will be guided by best practices in statistical simulation outlined by Morris et al. (2018). In the remainder of this section, I will describe the simulation design for this study, including the data generation mechanisms, estimands of interest, specific specifications of methods evaluated, and performance measures.

### Data-Generation Mechanisms

The data generation mechanism was informed by the evaluation I conducted as part of my first dissertaton study

#### Covariates

I took a programmatic approach to generate correlated covariates, $(X_1,X_2,X_3,\dots,X_p$ . Specifically, I simulated three covariate conditions that varied in the number of covaraites generated ($p = 20, 100, 200$). The 20 covariate condition was considerd given that its the average amount of covariates used in the typical propensity score analysis in educational research (Ansong, Wu, & Chowa, 2015; Cho et al., 2012; Monlezun et al., 2015;Morgan et al., 2015; Sullivan & Field, 2013). The remaining two condtions (p = 100, 200) were chosen because they are representative of the high-dimensional datasets in education (CITE HILL).

Much of the literature on propensity score simulation generates uncorrelated covariates [@Setoguchi.2008; @Stuart.2010; @ho2007matching; @Lee.2010]. However, this is not representative of the correlated data typically found in administrative data.All covaraites were generated from random pulls from a multivariate normal distribution with $\mu=0$ and $\sigma=1$, with a specified correlation matrix with correlations set between -0.3 and 0.3 using the *mvrnorm* function from the *MASS* package in *R* (CITE). This was based on the correlations found in the common app.

In order to keep the correlation structure intact I used the inverse CDF (cumulative distribution function) approach to convert the initial random pull of covariates from the multivariate normal distribution to three distributions; a normal distribution with a mean of 0 and SD of 1, a uniform distribution with a range of 0 to 1, and a Bernoulli distribution with a probability of success set at 0.5. This approach attempts to keep the same correlatios structure used however, some correlations are slighly supressed, but should be equal on average across the simulation (CITE). Covariates were constructed so that half of the covariates affected both treatment and outcome (i.e., confounders), a quarter affected only treatment and the remaining quarter affected the outcome only.

#### Sample Size

I used a fixed sample size of 10,000 for all simulation conditions. Note that most of the current propensity score simulation studies generate much smaller sample sizes[@Setoguchi.2008; @Stuart.2010; @ho2007matching; @Lee.2010]. Given that I am trying to mimic the real-world application of high-dimensional administrative data, this sample size is appropriate.

#### Population Treatment Assignment Models

I generated the population treatment assignment models using standard logistic regression. These population treatment assignment models can also be referred to as population propensity score models since they generate the "true" probability of being assigned to treatment. The following logistic regression model represents the general structure of these models.

$$
Pr(Z = 1) = (1 + exp(-(\beta_0 + \sum_{i=1}^{p} \beta_i X_i)))^{-1}
$$ {#eq-popps}

$X_i$ represents $p$ covariates and their associated coefficients ($\beta_i$). The output, $Pr(Z = 1)$ is the probability of assignment to the treatment condition (i.e., the propensity score, $e(x)$). The values for each intercept $\beta_0$ for the population treamtnet models were based on the evalution of the common application such that the intercept $0.25$, and the remaining $\beta_i$ coefecients were given a random value between -0.4 and 0.4.

I generated two types of population treatment assignment models (Base(T) and Complex(T)) in which I added levels of complexity to the model in the form of interactions (i.e., non-additivity) and higher order terms (i.e., non-linearity) to the covariates related to treatment and overall confounders.

**Base(T)** includes only main effects, which assumes a linear relationship between covariates and treatment assignment. I consider this the base model, which is expressed using the following formula:

$$
PS^{Base}(X) = (1 + exp(-(\beta_0 + \sum_{i=1}^{p} \beta_iX_i)))^{-1}
$$ {#eq-psA}

$X$ represents $p$ covariates and their associated coefficients $\beta$. The output, $Pr(Z = 1)$ is the probability of assignment to the treatment condition (i.e., propensity score).

**Complex(T)** describes the most complex population treatment assignment model. In which I include quadratic terms for a random sample of half of the covariates related to treatment and overall confounders, and interactions with another random sample of half of covariates related to outcome and overall confounders:

$$
PS^{Complex}(X) = PS^{Base}(X) + \beta_iX_i^2 + \beta_iX_iX_{i+1}
$$ {#eq-psD}

Using the propensity scores generated from these population treatment models I generated a binary treatment assignment variable by comparing an individual's true propensity score to a random draw from a uniform distribution between 0 and 1. (CITE) If the propensity score of an individual was higher than the random draw of the uniform distribution, the individual was assigned to treatment such that $Z = 1$, otherwise $Z = 0$. This approach should approximate a probability of assignment to treatment of 0.5, which is the probability of assignment to a randomized experiment with binary treatment.

#### Population Outcome Models

In addition to the population treatment assignment models, I generated a two population outcome models in which I also varied the level of complexities between covariates and potential outcomes by including interactions and higher-order terms on covariates related to outcome and overall confounders. Adding these complexities to the outcome model is relatively novel in the propensity score literature, although it is much more representative of the complex associations found in datasets in educational research @Cannas.2019.

Continuous potential outcomes were generated through regression models. Where similar to the population treatment models the values of $\alpha$ were based on the Common App evalution such that the intercet $\alpha_0$ equaled -0.18 and the remaining $\alpha$ coefecient received a random valued between -0.2 and 0.3.

The general structure of the population outcome models is defined as follows:

$$
Y(Z) = \alpha^Z + \sum_{i=1}^{p} \pi_iX_i; Z = 0,1
$$ {#eq-outcome}

$X_i$ represents $p$ covariates with their associated coefficients ($\pi_i$). The output, $Y$ is the continuous outcome. When $Z=1$, the outcome is the potential outcome of treatment, and when $Z = 0$, the potential outcome of control. $\alpha^Z$ represented the coefficient and intercept for $\alpha^0$ and $\alpha^1$, which are set such that the population treatment effect is equal to 0.3. This is in line with the typical small treatment effects that we observe in education research in which the average effect size is equal to 0.28 SD @richardson2011eta.

**Base(Y)** - the base model - includes only main effects, which assumes a linear relationship between covariates and potential outcomes. I consider this the base model, which is expressed using the following formula:

$$
Y^{Base}(Z) = \alpha_0^{a,Z} + \sum_{i=1}^{p} \alpha_i^{a,Z}X_i; Z = 0,1
$$ {#eq-outcome_a}

Where $X_i$ represents $p$ covariates and their associated coefficients ($\alpha_i^{\alpha,Z}$) which depend on treatment assignment, $Z$. The output, $Y^{a}(Z)$ is set to $Y^{a}(1)$ for the outcome related to treatment and $Y^{a}(0)$ for the control.

**Complex(Y)** describes the complex model which includes both non-linearities and non-additivities which were generated in an identical way to how they were generated in the population treatment outcome models:

$$
Y^{Complex}(Z) =  Y^{Base}(Z) + \alpha_iX_i^2 + \alpha_iX_iX_{i+1}; Z = 0,1
$$ {#eq-outcome_d}

#### Propensity Score Estimation Methods

Usin the simulated data, I estimate propensity scores using the standard logistic regresison and popular machine learning techniques used in the propensity score estimation literature, including classification and regression tree (CART), bagged CART, random forest, and single-layer neural network. For all methods the propensity scores are estimated on $p$ covaraites, which includes all confounders. Note that this is a unique scenario where all confounders are included in estimating the propensty score, in practice this is not likely the case but given I am interested in how well these methods recover the true unbiased treatment effect this approach is well suited. Future research should focus on the performance of these methods when confounders are ommited from the propenisty score estimationg model. Stuart says....

-   **Logistic regression**: standard logistic regression with only main effects

-   **Classification and Regression Trees (CART)**: using the R *rpart* package (Therneau & Atkinson, 2018) with default parameters and recursive partitioning.

-   **Bagged CART**: bootstrapped aggregated trees with default parameters using the R *ipred* package (Peters, Hothorn, & Lausen, 2002).

-   **Random Forest**: parallel tree generation on subsamples based on randomly selected covariates using the R *randomForest* package with default parameters (Liaw & Wiener, 2002).

-   **Single-Layer Neural Network (NN)**: a simple three layer NN with an input layer, one hidden layer, and an output layer. The input layer was equal to $p$ covariates and the hidden layer was made up of hidden neurons equal to 1/3 of the input covariates, as is standard practice in neural network applications and default parameters. Using the R *keras* package (Chollet et al., 2017) which interfaces with Python.

-   **Deep Neural Network (DNN-2)**: a four layer DNN with an input layer, two hidden layers, and an output layer. The input layer was equal to $p$ covariates and the hidden layers were made up of hidden neurons equal to 1/3 of the input covariates with default parameters. Using the R *keras* package (Chollet et al., 2017) which interfaces with Python.

-   **Deep Neural Network (DNN-3)**: a five layer DNN with an input layer, three hidden layers, and an output layer. The input layer was equal to $p$ covariates and the hidden layers were made up of hidden neurons equal to 1/3 of the input covariates with default parameters. Using the R *keras* package (Chollet et al., 2017) which interfaces with Python.

I used default parameters for all models to mimic the real-world application of these methods by applied researchers.

#### Hyperparameters

A common critique around neural network approaches, and widely most machine learning algorithms is the need for the researcher to indicate varies values for parameter. Choosing the wrong parameter may lead to widely different results (CITE). For example, tree based approaches like CART rely on specifying the number of trees to train most researcher using the forest pacakge liely dont change this parameter but defaults values are provided by the package creaters, in this case 100 trees is the default. Common wisdom says you can find the optimal number for these parameters using k-fold cross validation where your sample is split into various training and testing set and various value of the parameter one is trying to tune are tested, to find the optimal number. Although this is well established practice, when these methods are applied may researcher fail to do this appraoch and go back to default parameters (CITE).

Neural networks are no different, for exmple one can control how many hidden layers are include, how many hidden nurons, the learning rate, and weight decay. Various combinations of this would lead to different results. Note that X and X used neural networks to estimate propenisty scores and used a neural network trained on a large dataset to find the optimal parameters and then used those parameters for the simulation. This is a good approach however, it may not generalize outside their specific simulation conditons. A better approach and one that I take is to train within each iteraction to find the optimal learnign rate for the specific data at hand.

In reality this would be incredibly time consuming to use cross validation during each iteration. However, with advances in the deep neural network space with pacakges like Keras they have build in "optimizer" that do the trianing. IN this case its ADAm. Adam is an algorithm optimizer that iteratively lears the proper weight. Somethinga bout in an effecient manner.

Therefore for my simulation I use the keras optimizer to automatically find the otpimal learning rate tha tminimes a loss. To get some gains in computaitons effecienty for the neural network based apporahc i use early stopping techniqey, by which i set the once the delat values of does not change after 5 trainign runs I deem the network trained. Additionally, I use L2 regularization to penalize large weights set to .005. All neural network are trained with hidden layers euqla to 2/3 P and relu activaton fucntions that are computationally effecntio with L2 reularizaito applied to each hidden layer and the output layer. The ouput layer is set to a sigmoid activation fucntion in order to generate bounded probabilities between o and 1. I train on a standard 80/20 split and use 100 epochs, and 32 batch since. The losst function is defineda s the binary crossentropy which is good for binary classificaiton. Table X include the hyperparamters values for each of my neural networks I oncisder.

Additonally I test a 2 hidden layer approach and 3 hidden layer appraoch.

### Estimating Treatment Effects - Propensity Score Weighting

To recapture the treatment effect using the methods detailed above, I used a propensity score weighting approach, specifically the inverse probability of treatment weighting (IPTW) [@Lee.2010], where covariate balance is achieved between treated and untreated units by reweighting observations based on their estimated propensity score. Such that each treated observation is equal to $1/e_i(x)$, and untreated units receive a weight of $1/(1-e_i(x)$, where $e_i(x)$ is the estimated propensity score for unit $i$ this weighting scheme estimates the ATE.

The ATE was calculated using the R survey package where the outcome variable ($Y$) was predicted by a binary treatment assignment variable, with ATT weights included. Both the ATE point estimate were safed, as well as the standard error and the 95 confidence interval.

### Performance Metrics

To gauge the performance of each propensity score estimation method, I used the following metrics, which are widely used to access model performance in the propensity score simulation literature [@Setoguchi.2008; @Cannas.2019; @Stuart.2010; @Lee.2010; @McCaffrey.2004]:

-   **Bias**: the difference between the estimated treatment effect and the population treatment effect of 0.3. Bias values indicate deviations from the estimated ATE and the population treatment effect. This also lets us see if we over or under estimate bias

-   **Absoluate Bias**: the absolute difference between the estimated treatment effect and the population treatment effect of 0.3.

-   **Absolute Relative Bias (Bias)**: the absolute difference between the estimated treatment effect and the population treatment effect, divided by the true population treatment effect of 0.3.

-   **Average Standardized Absolute Mean Distance (ASAM)**: is a measure of of covariate balance. After calculating the ATT, the absolute value of the standardized difference of means between the treated and untreated group was calculated for each covariate and averaged across all covariates. Lower ASAM values indicate better covariate balance.

-   **ATE Standard Error**: is simply the standard error associated with the estimated ATE.

-   **Mean Squared Error (MSE)**: the average squared difference between the estimated treatment effect and the population treatment effect. MSE is a good representation of both bias and variance and a common performance metric in machine learning literature [@athey2015machine; @Mullainathan.2017].

-   **Power**: indicator of wether the p-value associated with the estimated ATE is lower than an alpha leve of 0.05.

-   **Type I error**: indicator of wether the p-value associated with the estimated ATE is lower than an alpha leve of 0.05, when the population treatment effect is set to 0.

-   **95% Confidence Interval Coverage**: indicator of wether the population treatment effect of 0.3 is found within the estimate 95% confidence interval.

-   **Weights**: a known issue with PSW is that extreme weights can result in biased treatment effects. Therefore I examine the distribution of the estimated weights.

### Simulation Scenarios

In total, I tested three covariates conditions (p = 20, 100, 200), with two treatment models (Base(T),Complex(T)) and two outcome models (Base(Y), Complex(Y)) and seven propensity score estimation methos including logit, cart, bag, forest, nn-1, dnn-2, and dnn-3. Sample sized was fixed at 10,000. Fully crossing all these conditions yielded a total of 84 scenaritos tested. Each scenario was simulated 1,000 times.

### Software

Given the complexity of the methods and the high-dimensional data generated in my simulation, I utilized the University of Pittsburgh Center for Research Computing cluster to perform the simulations on high-memory nodes. To carry out the simulations, I used the latest versions of R (4.1.0) and Python (3.7) along with the popular deep learning libraries, Keras and Tensorflow.

\newpage

## Results

This section presents the results of my Monte Carlo simulation study, which compares the performance of my DNN-based approach, traditional logistic regression, and other commonly used machine learning-based approaches, including CART, bagged CART, and random forest, in estimating the average treatment effect (ATE) through propensity score weighting. The simulation aims to evaluate how accurately these methods estimate the population ATE (ATE = 0.3) under different levels of covariate conditions (p = 20, 100, 200) and when complexities are introduced into the treatment and outcome models.

The results are organized into three main parts. First, I present the results on the bias in the ATE estimation. Next, I evaluate the variability of the ATE estimation by examining the standard error (SE), mean squared error (MSE), and 95% confidence interval coverage. Then, I assess covariate balance through the average standardized absolute mean difference (ASMD). Finally, I discuss the mean IPTW weights and statistical power.

### Bias of ATE Estimation

#### Bias

I start by summarizing the bias in the average treatment effect (ATE) across the population treatment and outcome model conditions to understand the average performance of the methods under different conditions. Figure @fig-bias displays the results for the bias of the ATE by method and covariate conditions (p = 20, 100, 200). The "true" ATE is set to 0.3, so a bias of 0 means that the method has accurately estimated the true ATE without any bias. If the bar is above 0, the method overestimates the true ATE, and if it is below 0, it underestimates the ATE.

```{r}
#| label: fig-bias
#| fig-cap: "Bias of the ATE"

over_df %>%
  ggplot(aes(x = method, y = Bias, fill = as.factor(p))) +
  ylab("Bias") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  coord_cartesian(ylim=c(-.05,.05)) +
  theme_Publication() +
  scale_fill_Publication() +
  geom_hline(yintercept = -.05, color = "white", size = 2)
```

Across methods and covariate conditions, there is much variability in terms of bias. In the low-dimensional condition (p = 20), all methods accurately estimate the true ATE with relatively little bias, except for bagged CART, which substantially underestimates the true ATE. As the number of covariates increases to 100, the magnitude of bias also increases, with logistic regression overestimating the ATE, while all other machine learning approaches estimate the ATE with less bias.

The results become more pronounced in the highest covariate condition (p = 200) compared to the low-dimensional condition (p = 20). Logistic regression severely overestimates the true ATE, while the machine learning methods estimate the true ATE with significantly less bias. A notable exception is nn-1, which in the high covariate scenario severely underestimates the ATE, albeit in the opposite direction, with a larger magnitude than logistic regression.

#### Absolute Percent Bias

In addition to the raw bias, I also evaluated the absolute percent bias, which is displayed in Figure @fig-abs-bias. An absolute percent bias of 0 indicates that there is no bias in the estimated ATE. It's worth noting that with absolute percent bias, we are disregarding the direction of the bias for easier visual representation.

```{r}
#| label: fig-abs-bias
#| fig-cap: "Absolute bias (%) of the ATE"

over_df %>%
  ggplot(aes(x = method, y = Abs_Per_Bias, fill = as.factor(p))) +
  ylab("Absolute Bias (%)") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  coord_cartesian(ylim=c(0,10)) +
  theme_Publication() +
  scale_fill_Publication() +
  geom_hline(yintercept = 10, color = "white", size = 2)
  
```

In the low-dimensional condition (p = 20), nearly all methods accurately estimate the ATE with little bias. Logistic regression has the least amount of absolute bias, while the DNN and NN approaches are relatively close, and all other approaches produce less than 2% absolute bias. The exception is bagged CART, which has a large amount of bias (absolute bias = 11.3%). As covariates increase from 20 to 100 and 200, logistic regression becomes substantially more biased than all other methods. All machine learning approaches perform well in the 100 covariate condition, with all producing bias below 2.5%. In this scenario, the lowest bias estimates are produced by my DNN approach, specifically DNN-2 and nn-1 (absolute bias = 0.43% and 0.21%, respectively). In the highest dimensional covariate condition (p = 200), all methods have an increase in bias. However, the machine learning approaches perform exceptionally well compared to logistic regression. Bagged CART produces the least biased estimate (absolute bias = 1.02%), followed by my DNN-2 approach (absolute bias = 1.33%), although the difference in magnitude is relatively small. An interesting finding is that the nn-1 approach produces the most biased results, even more so than logistic regression (23.5%, and 5.18%, respectively).

##### Complexities in the Population Outcome and Treatment Models

Next, I evaluate the absolute bias performance of each method by complexities introduced into the treatment and outcome model. Recall, that the data generating models *Base T* and *Base Y* include only main effects in the treatment and outcome model, while *Complex T* and *Complex Y* include non-addivity and non-linearities in the form quadratic and interaction terms among the covariates.

@fig-abs-bias-cond presents a faceted figure where the upper-left facet represents the condition where the population treatment and outcome models only include main effects (i.e., Base T & Base Y). The lower-right facet represents the condition where the population treatment and outcome models include quadratic and interaction terms (i.e., Complex T & Complex Y). Deviations from the diagonal facets indicate conditions where only one of the models includes quadratic and interaction terms, while the other only includes main effects.

```{r}
#| label: fig-abs-bias-cond
#| fig-cap: "Absolute bias (%) of the ATE by data generation mechanisms"
#| fig-width: 7.5
#| fig-height: 5

res_sum_df %>%
  ggplot(aes(x = method, y = Abs_Per_Bias, fill = as.factor(p))) +
  ylab("Absolute Bias (%)") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  facet_grid(scenarioT ~ scenarioY, scales = "fixed") +
  coord_cartesian(ylim=c(0,10)) +
  theme_Publication() +
  scale_fill_Publication() +
  theme(panel.spacing = unit(.5, "cm")) +
  geom_hline(yintercept = 10, color = "white", size = 2)
```

###### Base - Population Outcome and Treatment Models

When complexities are not included in the treatment or outcome models, nearly all methods perform well, regardless of the covariate conditions, exhibiting an absolute bias of less than 2.5%. The only exception is bagged CART, which produced significant bias with 20 covariates (absolute bias = 18.3%). Logistic regression produces the least biased estimates (absolute bias = 0.01%) when both the population treatment and outcome models include only main effects, which is expected as when estimating the propensity score using the various methods, only main effects are included in the model. Thus, in the Base Y and Base T condition, the population treatment model is equal to the propensity score estimation model. As the number of covariates increases, all methods perform exceptionally well, with my DNN-3 approach producing the least biased estimate (absolute bias = 0.11%).

###### Complex - Population Outcome and Treatment Models

In contrast to the models where only main effects are included in the population treatment and outcome models, the lower-right facet of @fig-abs-bias-cond displays the results when both the population treatment and outcome models include quadratic and interaction terms. When complexities are included in the population models, there is much more variability across methods and covariate conditions. All methods perform well when only 20 covariates are included, with absolute bias below 1.5%, except for bagged CART.

However, as the number of covariates increases, there is a significant increase in absolute bias. With 100 covariates, all methods perform well, with absolute bias below 1.5%, with bagged CART and nn-1 producing the least biased estimates (absolute bias = 0.08% and 0.13%, respectively). The differences become more pronounced in the high covariate condition (p = 200). Specifically, logistic and nn-1 produce the largest biased ATE estimates (absolute bias = 16.6% and 87.9%, respectively). The least biased estimates are produced by my DNN-2 approach (absolute bias = 2.67%), which is far lower than the absolute bias produced by all other machine learning approaches.

###### Deviations - Population Outcome and Treatment Models

In @fig-abs-bias-cond, deviations from the treatment and outcome model are indicated by points off the diagonal in the facets. The lower-left facet displays the results when the treatment model is complex, and the outcome model only has main effects, while the upper-right facet displays the results when the outcome model is complex, but the treatment model only includes main effects. In general, bias performance is sensitive to complexities in the outcome model or when complexities exist in both the outcome and treatment models, specifically in the highest covariate condition.

##### Summary

My findings suggest variability in the bias of the estimated ATE across methods and covariate conditions. In low-dimensional condition (p=20), most methods accurately estimate the true ATE with little bias. As the number of covariates increases, the magnitude of bias also increases, with logistic regression overestimating the ATE, while all other machine learning approaches produce less biased estimates. In the highest covariate condition (p=200), logistic regression severely overestimates the true ATE, while the machine learning methods estimate the true ATE with significantly less bias. Notably, my DNN-2 approach is a robust method for estimating the ATE in the high-dimensional covariate setting, especially when complexities are present in the population treatment and outcome model.

### ATE Estimation: SE, MSE, and 95% CI Coverage

Although bias in the average treatment effect (ATE) is an important metric to evaluate the performance of the methods, it is equally important to consider the sampling variability of the estimated ATE. The optimal method should have low bias in the estimated ATE and exhibit minimal variability through tight standard errors (SE) and mean squared errors (MSE) and high 95% confidence interval coverage. These properties enhance the precision of the estimated ATE. In this section, I summarize the findings of the estimated ATE in terms of standard error (SE), mean squared error (MSE), and 95% confidence interval coverage rate.

@fig-ate-var is a multi-panel figure that compares the performance of each method in terms of SE and MSE of the ATE and 95% confidence interval coverage rate across population models.

```{r}
#| label: fig-ate-var
#| fig-cap: Variability in ATE
#| fig-subcap: 
#|   - "Standard Error (SE)"
#|   - "Mean Squared Error (MSE)"
#|   - "95% CI Coverage"
#| layout-ncol: 2

over_df %>%
  ggplot(aes(x = method, y = ATE_se, fill = as.factor(p))) +
  ylab("Standard Error (SE)") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  coord_cartesian(ylim=c(0,1)) +
  theme_Publication() +
  scale_fill_Publication() +
  geom_hline(yintercept = 1, color = "white", size = 2)

over_df %>%
  ggplot(aes(x = method, y = MSE, fill = as.factor(p))) +
  ylab("Mean Squared Error (MSE)") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  coord_cartesian(ylim=c(0,4)) +
  theme_Publication() +
  scale_fill_Publication() +
  geom_hline(yintercept = 4, color = "white", size = 2)


over_df %>%
  ggplot(aes(x = method, y = coverage_95, fill = as.factor(p))) +
  ylab("95% C.I. Coverage") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  coord_cartesian(ylim=c(0,1)) +
  theme_Publication() +
  scale_fill_Publication() +
  geom_hline(yintercept = .95, linetype = 2, color = "black", size = .5) 
```

#### Standard Error (SE) and Mean Squared Error

The first panel (@fig-ate-var-1) displays the results for standard error of the estimated ATE by covariate condition. All methods produce comparable SE in the low dimensional condition (p = 20). As the number of covariates increases, logistic regression and nn-1 produce very large standard errors, while the other machine learnig approaches smaller SE, with cart and forest having the smallest standard errors (SE = 0.17 and 0.16, respectively).

In addition to the standard error (SE) of the estimated ATE, I also evaluated the mean squared error (MSE) of the ATE (@fig-ate-var-2). MSE measures the average squared difference between the estimated and true values, where a lower MSE indicates a better fit between the two. MSE provides a summary statistic that incorporates both bias and variance of the ATE. All methods perform well in the 20 covariate condition, producing small SE values. However, in the highest covariate condition (p = 200), logistic regression and nn-1 produce large MSE values, while all other machine learning approaches perform well with MSE values below 2. Out of all the machine learning approaches, my DNN-2 approach performs the best, with the lowest MSE value (MSE = 0.87), which is 35 times lower than the MSE for logistic regression (MSE = 29.4).

##### Complexities in the Population Outcome and Treatment Models

As with the previous ATE bias results, next, I evaluate the SE and MSE performance of each method by complexities introduced into the treatment and outcome model.

@fig-ate-var-cond further delves into the performance of each method by evaluating SE, MSE, and 95% CI coverage in the presence of complexities in the treatment and outcome models. The figure is divided into three panels, each representing the SE, MSE, or 95% CI coverage rate. Within each panel, the upper-left facet represents the condition where only main effects are included in the population treatment and outcome models (Base T & Base Y), while the lower-right facet represents the condition where both the population treatment and outcome models include quadratic and interaction terms (Complex T & Complex Y). The deviations from the diagonal facets indicate conditions where only one of the models includes quadratic and interaction terms while the other only includes main effects.

```{r}
#| label: fig-ate-var-cond
#| fig-cap: Variability in ATE by data generating mechanisms
#| fig-subcap: 
#|   - "Standard Error (SE)"
#|   - "Mean Squared Error (MSE)"
#|   - "95% CI Coverage"
#| layout-ncol: 2
#| fig-width: 7.5
#| fig-height: 5

res_sum_df %>%
  ggplot(aes(x = method, y = ATE_se, fill = as.factor(p))) +
  ylab("ATE SE") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  facet_grid(scenarioT ~ scenarioY, scales = "fixed") +
  coord_cartesian(ylim=c(0,1)) +
  theme_Publication() +
  scale_fill_Publication() +
  theme(panel.spacing = unit(.5, "cm")) +
  geom_hline(yintercept = 1, color = "white", size = 2)

res_sum_df %>%
  ggplot(aes(x = method, y = MSE, fill = as.factor(p))) +
  ylab("MSE") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  facet_grid(scenarioT ~ scenarioY, scales = "fixed") +
  coord_cartesian(ylim=c(0,4)) +
  theme_Publication() +
  scale_fill_Publication() +
  theme(panel.spacing = unit(.5, "cm")) +
  geom_hline(yintercept = 4, color = "white", size = 2)

res_sum_df %>%
  ggplot(aes(x = method, y = coverage_95, fill = as.factor(p))) +
  ylab("95% CI Coverage") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  facet_grid(scenarioT ~ scenarioY, scales = "fixed") +
  coord_cartesian(ylim=c(0,1)) +
  theme_Publication() +
  scale_fill_Publication() +
  theme(panel.spacing = unit(.5, "cm")) 
```

###### Base - Population Outcome and Treatment Models

The upper-left panels in @fig-ate-var-cond-1 and @fig-ate-var-cond-2 show the results for the SE and MSE of the ATE, when only main effects are included in the population treatment and outcome models. When only main effects are included all methods, regarless of covariate condition, perform well producing low SE and MSE values.

###### Complex - Population Outcome and Treatment Models

However, when both the treatment and outcome models include quadratic and interaction terms (lower-right panels in @fig-ate-var-cond-1 and @fig-ate-var-cond-2), a drastic increase in SE and MSE values is observed. Focusing on the MSE results, as the number of covariates increases to 100, logistic regression, CART, random forest, and nn-1 exhibit high SE and MSE values. Logistic regression has the highest MSE value (MSE = 1.05). In the highest dimensional case with 200 covariates, logistic regression leads to exceptionally high SE and MSE values (SE = 3.24; MSE = 115.1) compared to all other machine learning approaches, except for nn-1. This contrasts logistic regression's low SE and MSE values in the base condition (SE = 0.013; MSE = 0.00008). On the other hand, DNN-2 and DNN-3 have significantly lower MSE values than all other methods, with MSE values below 3, which is nearly 40 times lower than the MSE value for logistic regression in the high dimensional condition.

###### Deviations - Population Outcome and Treatment Models

As with the ATE bias evaluation results, both SE and MSE appear sensitive to complexities in the outcome model, more so than when complexities are only included in the treatment model. When complexities are only included in the outcome model, bagging, DNN-2, DNN-3, and logistic regression are robust to covariate conditions, with MSE values below 1. In contrast, nn-1 has the highest MSE, followed by bagged cart and random forest.

#### 95% Confidence Interval Coverage

Finally, in the third panel (@fig-ate-var-3), I present the results for the 95% confidence interval (CI) coverage rate of the ATE, indicated by the dashed line which represents the desired coverage rate. The 95% CI coverage rate assesses how well each method captures the true ATE of 0.3 within the estimated 95% confidence interval. A good estimator should have high coverage, meaning that the true population ATE is reliably captured within the interval. The machine learning approaches appear relatively insensitive to the different covariate conditions. However, both logistic regression and nn-1 seem more sensitive, showing a decrease in coverage rate as the number of covariates increases. Overall, all methods exhibit adequate coverage, except for bagged CART in the 20 covariate condition, and CART and random forest which exhibit lower coverage rates across all covariate conditions.

##### Complexities in the Population Outcome and Treatment Models

The third panel of @fig-ate-var-cond shows the 95% CI coverage rate results by model complexities. Overall, coverage rates appear particularly sensitive to complexities in the population treatment model, with all methods displaying lower coverage rates than when only main effects are included in the population treatment model. In addition, we observe a fairly consistent pattern that the neural network-based approach (nn-1, dnn-2, dnn-3) and logistic regression produce lower coverage rates as covariates increase.

##### Summary

All methods produced ATE with low SE and MSE values in the low-dimensional condition. However, as the number of covariates increased, the DNN approaches (dnn-2 and dnn-3) outperformed traditional logistic regression and other methods, producing the lowest MSE values. These findings suggest that the DNN approaches are reliable and consistent for ATE estimation in high-dimensional contexts, while also emphasizing the sensitivity of the estimates to complexities in the population outcome and treatment models. Bagging also performed well, with sound 95% confidence interval coverage and low MSE, making it another robust method in high-dimensional contexts. Logistic regression was effective in low-dimensional contexts, but its effectiveness decreased with an increase in dimensionality. The results emphasize the importance of considering both the bias and variability of estimated ATE when choosing the optimal estimation method.

### Covariate Balance

Up to this point, I have considered performance metrics that are not directly observable in practice. For example, a researcher cannot assess the degree of bias introduced by each propensity score estimation method within their observational dataset. A commonly used and directly observable metric for deciding which method to use is covariate balance. Covariate balance is typically assessed using the average standardized absolute mean difference (ASAM) between treated and untreated units. Maintaining covariate balance reduces the risk of bias in the estimated treatment effect and leads to more accurate results. However, the use of average standardized absolute mean difference as the metric for assessing balance is contested, with some researchers suggesting that other moments besides the mean should be considered. In my simulation, I focused exclusively on assessing balance through ASAM. For simplicity, I took the average ASAM for all covariates in the propensity score estimation model, but in practice, each covariate should be assessed separately. A mean ASAM value of 0 indicates balance in the mean between treated and untreated units across all covariates. I consider two commonly used thresholds for deciding whether adequate covariate balance has been achieved: the more typical 0.2 threshold proposed by (CITE), and the more stringent threshold of 0.1.

The results of the ASAM evaluation averaged across all population outcome models can be seen in Figure @fig-asam, where the thicker dashed line indicates an ASAM value of 0.2, and the thinner dashed line indicates an ASAM value of 0.1. Overall, all methods achieved adequate covariate balance in the low-dimensional condition with ASAM values below 0.2. However, using the more stringent cutoff of 0.1, both CART and random forest failed to achieve covariate balance. The lowest ASAM value achieved in the 20 covariate condition was by logistic regression with an ASAM value of XXX. As the covariate dimensionality increased, logistic regression produced extreme imbalance (ASAM = 74.9), while all other machine learning approaches maintained good covariate balance with ASAM values at or below the most stringent cutoff of 0.1. The lowest levels of covariate balance were achieved by bagged CART and my DNN approaches, with the DNN-2 approach producing the best balance with an ASAM value of 0.06.

```{r}
#| label: fig-asam
#| fig-cap: "Average Standardized Absolute Mean Difference (ASAM)"

over_df %>%
  ggplot(aes(x = method, y = ASAM, fill = as.factor(p))) +
  ylab("ASAM") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  coord_cartesian(ylim=c(0,.5)) +
  theme_Publication() +
  scale_fill_Publication() +
  geom_hline(yintercept = .5, color = "white", size = 2) +
  geom_hline(yintercept = .1, linetype = 2, color = "black", size = .25) +
  geom_hline(yintercept = .2, linetype = 2, color = "black", size = .5) 
```

###### Base - Population Outcome and Treatment Models

When the outcome and treatment models included only main effects, all methods produced good covariate balance using the 0.2 thresholds across all covariate conditions (as shown in the upper-left facet of @fig-asam-cond). With the more stringent 0.1 thresholds, bagging, the neural network-based approaches, and logistic regression produced optimal balance. In the base condition, the machine learning approaches appear insensitive to the number of covariates, while logistic regression and nn-1 seem sensitive to dimensionality.

```{r}
#| label: fig-asam-cond
#| fig-cap: "Average Standardized Absolute Mean Difference (ASAM) by data generating mechanisms"
#| fig-width: 7.5
#| fig-height: 5

res_sum_df %>%
  ggplot(aes(x = method, y = ASAM, fill = as.factor(p))) +
  ylab("ASAM") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  facet_grid(scenarioT ~ scenarioY, scales = "fixed") +
  coord_cartesian(ylim=c(0,.5)) +
  theme(panel.spacing = unit(.5, "cm")) +
  theme_Publication() +
  scale_fill_Publication() +
  geom_hline(yintercept = .5, color = "white", size = 2) +
  geom_hline(yintercept = .1, linetype = 2, color = "black", size = .25) +
  geom_hline(yintercept = .2, linetype = 2, color = "black", size = .5) 
```

###### Complex - Population Outcome and Treatment Models

When complexities are introduced into the treatment and outcome models, a slight increase in imbalance is observed, but nothing significant. In the low-covariate condition, all methods perform well with ASAM values below 0.2. With the more stringent threshold, CART and random forest fail to achieve covariate balance. In line with the overall ASAM results, logistic regression and nn-1 produced extreme imbalances among covariates in the higher covariate condition. For example, with 200 covariates, logistic regression produced extreme imbalances (ASAM = 74.94). In comparison, my DNN-2 approach achieved adequate covariate balance (ASAM = 0.061), and using the most stringent threshold, the DNN-2 and DNN-3 approaches were the only methods to achieve covariate balance.

#### Deviations from the Treatment and Outcome Models

Covariate balance is relatively robust to different specifications of covariate conditions and complexities in the outcome and treatment models. However, when dealing with higher covariates, specifically 100 and 200, logistic regression and nn-1 are sensitive to complexities in the treatment models, producing more imbalance in this context.

##### Summary

Most methods produced acceptable balance using a 0.2 threshold across all covariate levels, except for CART, Random Forest, and Logistic Regression. With the more stringent 0.1 thresholds, bagging, DNN-2, and DNN-3 performed better. In the highest covariate case, logistic regression produced extreme imbalance, while DNN-2 had the best balance with an ASAM value of 0.06. When considering complexities in both the outcome and treatment models, DNN-2 led to the best covariate balance among all methods, especially in high-dimensional cases, while logistic and nn-1 were sensitive to complexities in the treatment models. These results suggest that DNN-2 will lead to the best covariate balance in high-dimensional cases, especially when complexities are present.

### Weights Assessment

I estimated the average treatment effect (ATE) using inverse propensity score weighting (IPTW). To ensure unbiased treatment effects, it is important to avoid extreme IPTW weights, as large weights can result in bias (CITE). To evaluate this, I calculated the mean IPTW weight for each method, with a considerable departure from 1, indicating the presence of extreme weights.

@fig-weights shows the mean IPTW weights across treated and untreated units. All machine learning approaches resulted in acceptable weights below 2.5. All methods performed adequately well in the low-dimensional case with mean weights below 2. However, in the high-dimensional cases, logistic regression and nn-1 produced extreme weights. In the 200 covariate case, logistic regression produced extreme weights (IPTW = 225,868,800), as did nn-1 (IPTW = 130,406).

```{r}
#| label: fig-weights
#| fig-cap: "Mean IPTW weights"

over_df %>%
  ggplot(aes(x = method, y = PS_Weights, fill = as.factor(p))) +
  ylab("IPTW Weights") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  coord_cartesian(ylim=c(0,5)) +
  theme_Publication() +
  scale_fill_Publication() +
  geom_hline(yintercept = 5, color = "white", size = 2) 

```

##### Complexities in the Population Outcome and Treatment Models

When assesing the IPTW weight by the various complexities in the population treatment and outcome model, we find that all methods regardless of condition produce adequate weights below 2 (@fig-weights-cond). The mean values of the IPTW were consistent across all covariate conditions, indicating that the balance between the treatment and control groups was not affected by the presence of covariates. The only deviation from this result is logistic and nn-1, where they produce extreme weights whever complexities are introduced into the population treatment model, we don't see this affect when the base treatment model only includes main effects. 

```{r}
#| label: fig-weights-cond
#| fig-cap: "IPTW Weights by data generating mechanisms"
#| fig-width: 7.5
#| fig-height: 5

res_sum_df %>%
  ggplot(aes(x = method, y = PS_Weights, fill = as.factor(p))) +
  ylab("IPTW Weights") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  facet_grid(scenarioT ~ scenarioY, scales = "fixed") +
  coord_cartesian(ylim=c(0,5)) +
  theme_Publication() +
  scale_fill_Publication() +
  geom_hline(yintercept = 5, color = "white", size = 2) +
  theme(panel.spacing = unit(.5, "cm")) 

```

##### Summary

Overall, all methods produced acceptable IPTW weights across covariate conditions, especially when complexities were not included in the population treatment or outcome model. However, with added complexities and a higher number of covariates, logistic regression, and nn-1 produced extreme weights, which could result in biased treatment effects. These results indicate that most machine learning approaches, particularly the DNN approach and Random Forest, would be good options for generating IPTW weights in high-dimensional covariate conditions. The sensitivity of the IPTW weights to complexities in the treatment and outcome models underscores the importance of considering these complexities when selecting a method for IPTW weighting to ensure accurate treatment effect estimates and minimize the risk of extreme weights.

### Power

Finally, I evaluated the statistical power of the methods. Statistical power refers to the probability of detecting a statistically significant effect at a specific alpha level, typically .05. Low power means that the method has a high rate of committing type II error and may miss detecting real differences in the treatment effect estimation. High power, on the other hand, indicates a low rate of type II error. However, it is important to note that factors such as sample size and the magnitude of the effect can impact statistical power. In this simulation, the sample size was fixed at 10,000, and the treatment effect was relatively small. Statistical power may not be the most relevant benchmark for this simulation and should not be generalized outside my specific assessment of an ATE = 0.3. The following is a brief summary of the results.

@fig-power shows the results of the statistical power evaluation, averaged across the population treatment and outcome models. The standard alpha level of 0.05 was used to assess statistical significance. We observe a consistent pattern across methods, wherein in the low-dimensional condition, all methods can correctly identify a significant treatment effect, while as the number of covariates increases, we see a decrease in power. However, this decrease in power is more pronounced for both logistic regression and nn-1, while the other machine learning methods have a less steep drop.

```{r}
#| label: fig-power
#| fig-cap: "Power"

over_df %>%
  ggplot(aes(x = method, y = Power, fill = as.factor(p))) +
  ylab("Power") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  coord_cartesian(ylim=c(0,1)) +
  theme_Publication() +
  scale_fill_Publication() 
```

\newpage

## Discussion and Conclusion


### Summary of Results

Overall, these findings suggest that in the low-dimensional setting, logistic regression produces the least biased estimate, while as the dimensionality increases, logistic regression produces widely biased estimates compared to all other machine learning approaches, with the exception of nn-1. Of the machine learning approaches, my DNN-2 approach and bagged CART produced the least biased estimates. To put this into perspective, in the 200 covariate condition, logistic regression produced ATE estimates that were nearly four times more biased than my DNN-2 approach.

In summary, my simulation study suggests that in low-dimensional settings with non-complex outcome and treatment models, logistic regression may be the best choice for estimating the average treatment effect (ATE). However, as the number of covariates and complexities increase, the results indicate that my DNN-2 approach is a more robust and suitable method, offering low bias, better covariate balance, and lower mean squared error (MSE). In high-dimensional covariate settings with complexities in the outcome model, the DNN-2 approach and bagging are both strong contenders for estimating the ATE. It's important to consider both the bias and variability of the estimated ATE, as well as complexities in the treatment models, when choosing the optimal estimation method and evaluating covariate balance and selecting a method for inverse propensity score weighting.



In the absence of randomization, educational researchers often turn to quasi-experimental methods to reduce bias in their estimated treatment effects. The prevalence of high-dimensional data may make it challenging to correctly estimate unbiased treatment effects, as traditional parametric approaches are not equipped to model high-dimensional data. However, advancements in machine learning have led to a range of algorithms that are capable of modeling complex high-dimensional data. This simulation study aimed to evaluate the performance of logistic regression against commonly used machine learning techniques in propensity score estimation using propensity score weighting. The study also extends the literature by developing and testing a deep neural network-based approach to estimate propensity scores, and by evaluating the performance of these methods under different covariate conditions ranging from 20 covariates, the standard number used in applied propensity score analysis, to 200 covariates, scenarios that are becoming more common in education research.

Towards this end, I conducted an exhaustive Monte Carlo simulation study covering various dimensionalities and complexities in both the treatment and outcome model, which are representative of high-dimensional administrative data. My proposed DNN-based approach demonstrates superior bias reduction in estimating the ATE in a high-dimensional case not just against the traditional logistic regression but also other commonly used propensity score-based methods in the literature. In addition, my DNN approach leads to better covariate balance and MSE. This extends the literature by providing a novel method for propensity score estimation.

However, my findings suggest that the right tool needs to be chosen based on the context, and the more computationally expensive methods may not always be the ideal choice. When dealing with a low number of covariates, logistic regression produced good balance on covariates and unbiased ATE estimates, and its simplicity in terms of ease of implementation, interpretability, and lower computational cost make it a preferred choice.

On the other hand, when dealing with high-dimensional datasets, machine learning methods, particularly ensemble learning methods like bagged cart and my proposed DNN approach, should be used. These methods lead to less bias and better covariate balance compared to traditional logistic regression, which produces extreme weights and leads to inaccurate ATE estimates in high-dimensional settings.

### Summary of Main Results

The results of the ATE bias evaluation showed that the standard logistic regression approach was not always the best method for estimating unbiased treatment effects. The inclusion of complexities in both the treatment and outcome model led to an increase in bias, with logistic regression being particularly sensitive to the number of covariates. The deep neural network (DNN) based approach developed in this study, as well as bagging and some other machine learning methods, produced lower ATE bias and were more robust to the number of covariates.

In terms of standard error (SE) and mean squared error (MSE), the results were consistent with the findings for ATE bias. The DNN based approach and bagging produced the lowest SE and MSE across all covariate conditions, including in the highest dimensional case with 200 covariates. The logistic regression approach was found to be less robust to the number of covariates, producing the largest SE and MSE in the highest dimensional case. For 95% confidence interval coverage, the results were similar with the logistic regression approach and the neural network based method nn-1 being sensitive to the number of covariates, while the other machine learning methods, including the DNN based approach, were more robust.

The results of the covariate balance evaluation using the ASAM metric showed that all methods using the common 0.2 threshold produced acceptable results, with the exception of CART, Random Forest, and Logistic Regression, which had the largest ASAM values. However, when using the more stringent 0.1 threshold, the bagging method, the DNN-based approaches, and logistic regression produced optimal balance. In the highest covariate case, logistic regression produced extreme imbalance (ASAM = 74.9), while all other machine learning approaches produced good covariate balance with ASAM values below 0.15. The DNN-2 approach produced the best balance with an ASAM value of 0.06.

These results suggest that the use of the ASAM metric is important in assessing covariate balance in simulation studies that use propensity score weighting. The results also show that the machine learning approaches, particularly the bagging method and the DNN-based approaches, perform better than logistic regression in terms of achieving good covariate balance. This highlights the importance of considering alternative methods to logistic regression in educational research studies that utilize high-dimensional data.

### Implications and Contributions of the Study

The results of this study have important implications for both practice and future research. In terms of practice, the findings suggest that machine learning methods should be considered when dealing with high-dimensional data in educational research. The superior performance of machine learning methods in terms of reducing bias and improving covariate balance over traditional logistic regression should be taken into consideration by researchers when choosing a method for propensity score estimation. In particular, the proposed DNN approach provides a novel and effective solution for high-dimensional data, and researchers should consider this method as a viable option.

In terms of future research, the results of this study suggest that there is room for further exploration of machine learning methods in propensity score estimation. While this study focused on a comprehensive evaluation of commonly used methods, there may be other machine learning algorithms that could perform even better. Additionally, this study only examined the performance of these methods under a limited set of conditions, and further research should aim to evaluate their performance under different assumptions, such as non-linear relationships between treatment and covariates. Another area for future research could be the exploration of alternative metrics for evaluating covariate balance, as the ASAM metric is still contested in the literature. Overall, the results of this study highlight the need for continued research in this area to improve our understanding of the best methods for estimating treatment effects in the presence of high-dimensional data.

-   Comparison of the results with previous studies in the field

-   In terms of comparison with previous studies in the field, it is important to note the advancements in machine learning techniques that have been made since previous studies were conducted. While previous studies have shown that traditional methods like logistic regression can perform well in low dimensional settings, they have not been able to address the challenges posed by high dimensional data. Our study extends the literature by showing that machine learning techniques, particularly the proposed DNN approach, can effectively address these challenges and provide better covariate balance and unbiased ATE estimates in high dimensional settings.

    Additionally, our study provides a more comprehensive evaluation of various machine learning techniques by considering different levels of covariate complexity and varying numbers of covariates. This allows us to provide a more nuanced understanding of the strengths and weaknesses of each technique, and the specific contexts in which they may be most useful. Our results suggest that the appropriate method for propensity score estimation will depend on the specifics of the dataset and the research question being addressed, and highlights the importance of considering both the computational cost and the quality of results when selecting a method.

-   Explanation of how the results contribute to the advancement of knowledge in the area of propensity score weighting and treatment effect estimation

-   The results of this study contribute to the advancement of knowledge in the area of propensity score weighting and treatment effect estimation in several important ways. Firstly, the evaluation of logistic regression against commonly used machine learning techniques, as well as the development and testing of a deep neural network-based approach to estimate propensity scores, provides a comprehensive analysis of the performance of these methods. This is particularly relevant in the context of high-dimensional data, which is becoming increasingly common in educational research. Secondly, the examination of the performance of these methods across different covariate conditions, including 20, 100, and 200 covariates, provides valuable insights into the robustness of these methods and the conditions under which they are most effective.

    Additionally, the examination of the implications of covariate balance using the ASAM metric highlights the importance of considering balance in the context of propensity score weighting. The results of this study demonstrate that some methods, such as logistic regression, are more sensitive to covariate balance than others, such as the proposed DNN-based approach. This highlights the importance of considering balance when choosing a method for propensity score estimation, and suggests that the proposed DNN-based approach may be a valuable addition to the existing literature in this area. Overall, the results of this study contribute to the advancement of knowledge by providing a comprehensive evaluation of the performance of different methods for propensity score weighting and treatment effect estimation.

This study highlights the importance of testing various complexities in both the outcome and treatment models in simulation studies focused on propensity score estimation. The results showed that some methods, such as logistic regression and nn-1, are sensitive to the number of covariates and complexities in the models, leading to large standard error and mean squared error values, as well as low coverage rates. In contrast, other methods, such as bagged and dnn approaches, were fairly robust to these variations and produced better results. Thus, it is crucial to consider the complexities in the models and the number of covariates when selecting a method for propensity score estimation in practice.

### Limitations and Future Research Directions

-   The current simulation study is subject to several limitations that may affect the validity of the results. One limitation of Monte Carlo simulations is that they are only as good as the underlying assumptions and models used to generate the data. In this study, the assumptions and models used to generate the data may not accurately reflect real-world educational data. Additionally, the sample size used in the simulation study may not be large enough to fully capture the variability in the data, which could affect the validity of the results.

    Another limitation of the study is that it only considers a limited set of methods for propensity score weighting and treatment effect estimation. Other methods, such as doubly robust estimation or inverse probability weighting, may perform differently in high-dimensional data scenarios and should be considered in future research. Additionally, the study only considers the performance of the methods in terms of bias reduction and covariate balance, but does not consider other important aspects such as implementation time or computational cost.

-   Explanation of how future research can build on the results and address limitations

Another limitation of the study is that we did not test super learners, which is an ensemble machine learning method that combines several models to obtain better performance. This is an important avenue for future research as super learners have been shown to improve the performance of treatment effect estimation in other studies. By incorporating super learners, we can evaluate if this method provides an even more robust solution for propensity score weighting in high-dimensional data compared to the methods tested in this simulation. Additionally, future research can also explore the performance of super learners with different covariate conditions, complexity levels, and sample sizes to gain a more comprehensive understanding of this method.

-   Future research can build on the results of this study by addressing the limitations and improving upon the methodology. One potential avenue for improvement is to incorporate more realistic scenarios and data structures in the simulation study. This can include the use of real-world datasets, or simulations that better reflect the complexities and nuances of real-world data. Additionally, future research can explore alternative methods for evaluating treatment effect estimation, such as cross-validation or other performance metrics that better capture the accuracy and robustness of the methods being evaluated. Furthermore, future research can examine the generalizability of the findings to different types of datasets, or to different application domains. Finally, future research can consider incorporating additional machine learning techniques, or exploring the use of hybrid approaches that combine elements of traditional methods and machine learning algorithms to further improve the accuracy and efficiency of treatment effect estimation

### Conclusions

-   Summary of the main findings and contributions of the study

-   Discussion of the implications of the results and future research directions

-   It is recommended that practitioners carefully consider the limitations and assumptions of the methods they choose for propensity score estimation and carefully evaluate the covariate balance achieved in their study. Researchers should continue to build on the results of this study by exploring new methods for propensity score estimation and testing their performance under various conditions. Additionally, simulation studies should consider a wider range of complexities in the treatment and outcome models and test various metrics for evaluating covariate balance. This will further advance the field of propensity score weighting and treatment effect estimation, leading to more accurate and unbiased estimates of treatment effects.

\newpage

## Tables and Figures

```{r}
#| label: tbl-bias
#| tbl-cap: Bias of ATE estimation

over_tab <- res |> 
  dplyr::select(p, scenarioT, scenarioY, method, Bias, Abs_Per_Bias)  

gt(over_tab) |>
  tab_spanner(
    label = "Condition",
    columns = c(p, scenarioT, scenarioY)
  ) |>
  fmt_number(columns = c(Bias, Abs_Per_Bias), decimals = 3) |>
  cols_align_decimal() |> 
    cols_label(
    scenarioT = md("Outcome"),
    scenarioY = md("Treatment"),
    Abs_Per_Bias = md("Abs. Bias (%)"),
    method = "Method") 
```

\newpage

```{r}
#| label: tbl-var
#| tbl-cap: ATE estimation - SE, MSE, and 95% C.I. coverage


over_tab <- res |> 
  dplyr::select(p, scenarioT, scenarioY, method, ATE_se, MSE, coverage_95)  

gt(over_tab) |>
  tab_spanner(
    label = "Condition",
    columns = c(p, scenarioT, scenarioY)
  ) |>
  fmt_number(columns = c(ATE_se, MSE, coverage_95), decimals = 3) |>
  cols_align_decimal() |> 
    cols_label(
    scenarioT = md("Outcome"),
    scenarioY = md("Treatment"),
    ATE_se = md("SE"),
    method = "Method",
    coverage_95 = md("95% C.I.<br>Coverage"))
```

\newpage

```{r}
#| label: tbl-power
#| tbl-cap: Power

over_tab <- res |> 
  dplyr::select(p, scenarioT, scenarioY, method, Power)  

gt(over_tab) |>
  tab_spanner(
    label = "Condition",
    columns = c(p, scenarioT, scenarioY)
  ) |>
  fmt_number(columns = c(Power), decimals = 3) |>
  cols_align_decimal() |> 
    cols_label(
    scenarioT = md("Outcome"),
    scenarioY = md("Treatment"),
    method = "Method")
```

\newpage

```{r}
#| label: tbl-asam
#| tbl-cap: ASAM

over_tab <- res |> 
  dplyr::select(p, scenarioT, scenarioY, method, ASAM)  

gt(over_tab) |>
  tab_spanner(
    label = "Condition",
    columns = c(p, scenarioT, scenarioY)
  ) |>
  fmt_number(columns = c(ASAM), decimals = 3) |>
  cols_align_decimal() |> 
    cols_label(
    scenarioT = md("Outcome"),
    scenarioY = md("Treatment"),
    method = "Method")
```
