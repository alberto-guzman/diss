# Paper 1: Evaluating modern propensity score estimation methods with high-dimensional data

## Introduction

Traditionally, questions of causality have been addressed through the use of randomized control trials (RCTs), in which individuals are randomly assigned to either a treatment group that receives an intervention or a control group (Murnane & Willett, 2010). Randomization ensures that, on average, both groups are balanced on all observable and unobservable characteristics, eliminating the possibility that an intervention's effect is due to differences in group characteristics (Rosenbaum, 2010; Rosenbaum, 2020). However, in education and broader social science research, ethical, financial, or practical obstacles can prevent a researcher from conducting a randomized experiment. Instead, researchers must rely on nonexperimental or observational studies in which students self-select or are placed in an intervention without randomization.

Observational data complicates causal inference because, in the absence of randomization, an imbalance in the distribution of student covariates between groups could lead us to incorrectly attribute a change in outcome to the intervention or policy, as opposed to differences in group composition. A vast body of methodological literature has centered on developing quasi-experimental methods that attempt to address the internal validity issues associated with observational data by balancing the *observed* characteristics of those who received treatment and those who did not. One such popular approach is propensity score analysis (CITE).

Propensity score analysis is one of the most commonly employed quasi-experimental methods in education research (Fan & Nowell, 2011; Powell et al., 2019; Rosenbaum, 2020; Stuart, 2007; Thoemmes & Kim). Propensity score analysis attempts to balance *observed* characteristics between treated and untreated students, just as randomization to treatment and control conditions creates balance on observable characteristics. The propensity score represents the probability that a student *would have* been exposed to treatment, conditional on observed covariates (Rosenbaum & Rubin, 1983, 1984). Propensity scores can be used in a variety of ways to balance student covariates, including propensity score matching, in which treated and untreated students with similar propensity scores are matched, and propensity score weighting, in which treatment groups are re-weighted so that observed covariates are balanced between both groups.

However, to estimate unbiased treatment effects, the researcher must correctly specify the propensity score model such that:

1.  The propensity score model captures *all* covariates associated with treatment assingnment, omitting no potential confounding variables (Rosenbaum, 2010, 2020).

2.  The propensity score model accurately models the relationship between covariates and treatment selection, meaning that all appropriate interactions and non-linear terms are specified (Rosenbaum, 2010; 2020).

The literature suggests a "kitchen sink" approach for variable selection to guard against the first condition because the penalties are high if a potential confounder is *not* included in the propensity score model, resulting in biased treatment effect estimates (Pirracchio et al., 2015; Shortree & Ertefaio, 2017). For the second condition, the literature suggests iterating through various model specifications, with each iteration adding interactions or non-linear terms until a balance is achieved among the observed characteristics between treated and untreated students (Murnane & Willett, 2010; Rosenbaum).

At first glance, high-dimensional (i.e., big data) data should be an asset to propensity score analysis, since conventional practices stress the importance of including *all* available variables in the propensity score model, given that it increases the likelihood of capturing all relevant confounders. Nevertheless, this is not the case. The performance of the traditional logistic model for estimating propensity scores tends to degrade as more variables are included in the estimation step (Hill et al., 2011). This can manifest itself in a variety of ways when dealing with high-dimensional data, such as:

1.  A large number of covariates may prevent the logistic model from running and produce unsatisfactory propensity scores.

2.  If the model runs successfully, it is likely to overfit the data. The logistic model will begin to model random error in the data rather than the relationship between variables. This may result in propensity scores with little to no variability, making them inappropriate for propensity score analysis.

3.  In a high-dimensional context, it becomes increasingly difficult to iteratively model all of the appropriate interactions and non-linearities with a logistic model (D'Amour et al., 2020; Dorie et al., 2019; Hill et al., 2011).

With the rapid emergence of increasingly sophisticated machine learning algorithms (Daniel, 2019; Gibson & Ifenthaler, 2016; Williamson, 2017), there are increasing possibilities for developing and applying new analytic methods that can flexibly estimate propensity scores. These machine learning algorithms enable an automatic, data-driven method of capturing nonlinearities and non-additivity in the propensity score estimation model, which may result in a more balanced covariate distribution without the need for tedious iteration. To date, however, no research has examined the performance of these machine learning methods when applied to high-dimensional data, as the majority of previous propensity score simulation research has focused on simulated low-dimensional datasets.

Furthermore, recent advances in computer science have generated a wealth of research on a potential method that could be used to estimate the propensity score, deep neural networks (DNN) (Deng & Yu, 2014; Hernández-Blanco et al., 2019; LeCun et al., 2015; Perrotta & Selwyn, 2019). DNNs are algorithms based on the neural architecture of the human brain. They are commonly used in industry to model complex prediction and classification tasks and have already proven to excel in modeling high-dimensional data (LeCun et al., 2015). DNNs are highly flexible and capable of capturing complex interactions and non-linearities, and they enable automatic variable selection (Henandex-Blanco et al., 2010; LeCu et al., 2015; Zou et al., 2019).

DNNs represent an essential tool for researchers in estimating the correct propensity score model, in general, and mainly when dealing with large datasets that include hundreds of covariates on students and when those covariates have complex associations.

Therefore, my first dissertation study aims to assess the performance of the traditional logistic model and other machine learning methods with high-dimensional data, using propensity score weighting. In addition, I overcome the limitations of existing methods by proposing a novel DNN-based approach to the estimation of propensity scores and illustrate a novel new framework for generating multi-type, correlated high-dimensional data replicating the high-dimensional administrative data that is now becoming available to education researchers.

The remainder of this document is structured as follows: First, I review the relevant literature for this study. Then, I describe my proposed data and simulation approach. Finally, I conclude with a discussion of the remaining questions for my analysis.

## Literature Review

In this section, I review the literature on causal inference and propensity score analysis, focusing on machine learning approaches to estimating propensity scores. In addition, I draw on the literature on neural network approaches, specifically deep neural networks, to motivate the development of my DNN-based method for estimating propensity scores.

### Neyman-Rubin-Holland Model for Causal Inference

My approach to causal inference is based on the Neyman-Rubin-Holland model, hereafter referred to as the Rubin model (Holland 1986; Rubin 2006, 1974; Rosenbaum 2002). A core tenet of the Rubin model states that a treatment effect is the difference between two potential outcomes for an individual. For a dichotomous treatment variable ($Z$), let $Z_i = 1$ if the $i$'th student was assigned to the treatment group -- say a college access program - and $Z_i = 0$ if they were not. Let $Y_{ij}$ be a potential outcome for an individual student depending on treatment assignment $Z_i$, such that an individual student has two potential outcomes. $Y_{i1}$ is the potential outcome had a student participated in the college access program and $Y_{i0}$ is the potential outcome had that same student not participated. Therefore, the treatment effect of the college access program for student $i$ would simply be the difference in their potential outcomes:

$$
\tau_i = Y_{i1} - Y_{i0}
$$ {#eq-treat}

However, in the real world, we do not simultaneously observe both potential outcomes for the same student. That is to say, we do not observe student $i$'s potential outcome both under receiving the college access program and not. This is why the fundamental problem of causal inference is a missing data problem (CITE). Although we cannot directly observe these potential outcomes under certain assumptions, we can estimate the average treatment effect (ATE) across our sample, defined as:

$$
ATE= \mathbb{E}(Y_{i1} -Y_{i0})=\mathbb{E}(Y_{i1})-\mathbb{E}(Y_{i0})
$$ {#eq-ate}

Where $\mathbb{E}(Y_{i1})$ is the expected value of all students in the treatment group and $\mathbb{E}(Y_{i0})$ is the expected value of all students in the control group. In the context of a true experiment in which students are *randomly* assigned to treatment conditions, we can estimate the ATE in a straightforward manner since with a large enough sample size, both treated and control students will be balanced on all observable and unobservable characteristics. Stated differently, a randomized experiment ensures that $Y_1$ and $Y_0$ are independent of treatment assignment ($Z$), and therefore, the treatment effect can be regarded as causal.

In addition, we may also be interested in estimating the average treatment effect for those who were *actually* treated. It should be noted that even in an experiment perfect compliance is not always possible. Some students in the treated group may decide not to take up the treatment, while students in the control group may find a way to receive treatment. In this case, we may be interested in the average treatment effect on the treated (ATT) (CITE), which is defined as:

$$
ATT= \mathbb{E}(Y_{i1} -Y_{i0}|T_i=1)=\mathbb{E}(Y_{i1}|Z_i=1)-\mathbb{E}(Y_{i0}|Z_i=1)
$$ {#eq-att}

In practice, the ATT is the most policy-relevant estimand since its the treatment effect on students who took up treatment (CITE).

### Observational Studies

In behavioral and social science research, ethical, cost, or practical barriers can preclude a researcher from conducting a randomized experiment and instead must rely on using non-experimental or observational data (Bai, 20XX). As opposed to RCT's, students in observational studies either self-select or are placed into an intervention without randomization. This complicates causal inference because without randomization, their may be differences in students observable and unobservable characteristics between those who received the treatment and those who did not that may influence a students outcome.

For example, say that in our college-access program example, students were not randomized into treatment or control. Instead, the students self-selected into treatment. In this scenario, the underlying treatment assignment is unknown. It could be that students who self-select into the program are generally more interested in going to college than those who did not sign up. Given this imbalance in college interest, we may incorrectly attribute a increase in college enrollment to the treatment instead of differences in college interest between treated and untreated students. In other words, observational studies *cannot* ensure that a student's outcome is independent of treatment ($Z$).

Various statistical methods have been proposed that focus on estimating unbiased treatment effects in observational studies. These include instrumental variable approaches (CITE), regression discontinuity (CITE), synthetic control (CITE), and propensity score analysis (CITE). For my dissertation, I focus on propensity score analysis which leverage student-level covariates to create a balancing score that can account for *observed* group differences.

### Propensity Scores

Rosenbaum and Rubin initially proposed the concept of a propensity score in 1983 as a means to increase the accuracy of capturing unbiased treatment effects in observational studies (CITE). Propensity score analysis is one of the most popular quasi-experimental methods among social and behavioral researchers, particularly among applied educational researchers (CITE).

Rosenbaum and Rubin define a propensity score ($e$) as the conditional probability of assignment to treatment conditional on observed pre-treatment covariates. Defined as:

$$
e(x) = Pr(Z=1|X)
$$ {#eq-ps}

Propensity scores are typically estimated as predicted probabilities from a logistic regression in which a binary outcome variable indicating whether a unit received treatment is being predicted by observed individual characteristics ($X$). They range from 0 to 1, with values closer to 1 indicating that an individual is more likely to be assigned to treatment. Rosenbaum and Rubin (1983, 1984) demonstrate that if the propensity score is balanced across treatment and control groups, then the distribution of observed covariates between treatment and control is also balanced (CITE). Therefore, theoretically, two students with similar propensity scores should have a similar distribution of observed covariates.

Researchers can condition on the propensity score using a variety of techniques, including matching, which pairs treated students with control students based on the proximity of their propensity score; stratification, which groups students into unique strata based on their propensity score; and weighting, which reweighs students in the sample so that the distribution of the propensity score of the control group is similar to that of the treated group. All these methods have a similar aim of using the propensity score to adjust for the imbalance in the covariate distribution between treated and control units to approximate true randomization conditions. Therefore, by conditioning on the propensity score through matching, stratification, or weighting, we can eliminate bias in our treatment effect and approximate the true causal effect. However, for this to be true, certain assumptions must hold.

#### Assumptions

Several assumptions must be met to estimate an unbiased treatment effect using propensity scores. These include the strong ignorability assumption, the stable unit value assumption, and region of common support.

The strong ignorability assumption states that the potential outcome of an individual is conditionally independent of treatment assignment if we successfully conditioned on all covariates related to treatment, $Y_1, Y_0 \perp Z|X$. In experimental studies, this assumption is satisfied by the act of randomization, since randomization ensures that treatment assignment is independent of an individual's outcome. However, in observational studies, this assumption holds if *all* covariates related to treatment assignment are included in the propensity score model.

The second assumption, the stable unit treatment value assumption (SUTVA), states that the potential outcome of an individual is unaffected by the treatment assignment of any other individual. In other words, the outcome of a student is independent of whether another student is assigned to the treatment or control group.

Finally, the third assumption is the region of common support. This assumption states that there must be sufficient overlap in the estimated propensity score distribution between the treatment and control groups. With sufficient overlap, we can ensure that we can find suitable matches for treated students.

If these assumptions hold, we can estimate causal effects from observational data. Unfortunately, many of these assumptions are untestable. For example, in observational data, the majority of the time, we are blind to the underlying treatment assignment mechanism, and therefore we cannot be sure that *all* covariates related to treatment assignment are included in our propensity score model. In practice, propensity scores should be viewed as a means of reducing some, but not all, of the bias associated between treatment and outcome in observational studies. In practice, we cannot ensure that all confounding variables are included in our propensity score model. Therefore, special attention should be paid to which covariates are selected to be included in the propensity score model.

#### Covariate Selection

A reasonable approach to selecting covariates in propensity score analysis is selecting covariates that are theoretically sound and related to treatment assignment and the outcome. Since in order to estimate unbiased treatment effects correctly using propensity scores, we have to capture *all* covariates related to the selection into treatment, leaving out no potential confounders (Rosenbaum, 2010, 2020). However, in practice, it may be impossible to know which covariates our confounders; therefore, the literature suggests a "kitchen sink" approach for variable selection to guard against the penalties of excluding potential founders (Pirracchio et al., 2015; Shortreed & Ertefaie, 2017). In addition, the propensity score model must capture the correct functional form of the covariates, meaning that all proper interactions and non-linear terms are specified (Rosenbaum, 2010, 2020). If the propensity score model is misspecified, it will lead to biased treatment effect estimates (Rosenbaum et al., 1983; Shadish et al., 2010; Steiner et al., 2011).

To better capture the relationship among covariates, the literature suggests iterating through various model specifications, with each iteration adding interactions or non-linear terms until a balance is achieved amongst the observed characteristics between treated and untreated students (Murnane & Willett, 2010; Rosenbaum). This iterative process becomes increasingly cumbersome as the number of covariates increases (CITE).

### Estimating the Propensity Score

The most critical step in propensity score analysis is estimating the propensity score ($e$), since a misspecified propensity model will yield propensity scores that are not well suited to correctly balanced observed characteristics. Any parametric or non-parametric model can generate propensity scores if it outputs a bounded probability between 0 and 1. Logistic regression is the most common parametric model used for estimating the propensity score (CITE), especially in applied educaitional research.

#### Logistic Regression

Logistic regression is the most common model used to generate propensity scores in the social science literature due to its ease of interpretation and familiarity with many applied researchers (Keller et al., 2013) and its ability to accommodate both continuous and categorical covariates. The logistic model is defined as follows:

$$
ln\left( \frac{Pr(Z=1|X)}{1-Pr(Z=1|X)} \right) = \beta_o + \beta_1X_1 + \cdots + \beta_pX_p
$$

Where $X$ is a vector of observed covariates that are regressed on $Z$, a binary indicator equal to 1 if a student is in the treatment group and 0 otherwise. The output of the logistic regression is predicted probabilities that are continuous and bounded between 0 and 1 (i.e., propensity scores).

Although logistic regression is the most widely used model for estimating propensity scores, it may not be the most valid for inference. Various simulation studies suggest that using logistic regression for modeling propensity scores is inadequate and leads to biased treatment effects when incorrectly specified, given that parametric models require assumptions regarding the functional form and variable distributions (Lee et al., 2010; Pan & Bai, 2015; Setoguchi et al., 2008; Westreich et al., 2010). This is especially true when estimating propensity scores with hundreds of covariates. The logistic model tends to degrade and estimate a propensity score that falls outside the desired 0 to 1 bound in a high-dimensional context. With the ever-increasing amounts of "big" administrative data at the disposal of applied behavioral and social science research, the degradation of logistic regression is worrisome. However, outside social science, research has suggested alternatives to logistic regression in estimating the propensity score and instead use non-parametric machine learning algorithms.

#### Machine Learning Approaches

Machine learning algorithms have recently gained popularity in the causal inference and propensity score literature (CITE Athey) due to these algorithms being highly flexible and able to model complex functional forms iteratively without explicit manipulation from the research. With a simple modification, these algorithms can output bounded probabilities that can be used as propensity scores. Given the success of machine learning methods for prediction tasks, such as XXX (CITE), it is reasonable to think they are worthy candidates for the propensity score estimation problem, which is a prediction problem. Broadly, machine learning algorithms used in propensity score estimation can be classified into tree-based, ensemble methods, and -- the focus of this dissertation -- neural network based approaches.

#### Tree Based Methods

One popular machine learning approach to estimating propensity score is classification and regression trees (CART) (Friedman, Olshen, & Stone in 1984). This algorithm splits data recursively into subsets based on individual covariates to predict the probability of membership assignment of a given individual. To generate propensity scores, the predicted outcome is simply a binary indicator of treatment assignment ($Z$). This algorithm splits covariates by level of importance, with the first split being the covariate that can produce the most distinctive split. For example, if a student above 15 years of age is related to treatment assignment, the first split would split the data into students with $age > 15$ and $age < 15$. The algorithm continues by splitting the data by the next most influential covariate split, creating a tree-like structure (FIGURE). Splitting stops when the data is binned into unique "branches," which minimizes the prediction error in which an additional split would not improve the prediction of treatment assignment. The final "branches" of the model represent the individual grouping of students with similar propensity scores. The output of CART is the predicted probabilities which are then used as propensity scores. Like logistic regression, the CART algorithm can handle continuous and categorical covariates. However, unlike logistic regression, CART is insensitive to outliers and can model interactions and non-linearities automatically (CITE).

However, a well-documented issue with CART is that the single tree may likely overfit the data. Therefore, a modified version of CART was developed that "prunes" back branches that do not lead to a reduction in prediction error, referred to as XXX. This leads to trees that do not overfit the data. Although pruning can alleviate some of the changes of overfitting the data, a significant drawback of CART is that it relies on a single tree which may be weak in predicting the propensity score.

#### Ensemble Methods

Ensemble methods are a family of algorithms that generate multiple trees predicting treatment assignment. Exploiting differences among each tree ensemble method can significantly improve prediction, given that many weak trees together can generate better prediction than a single CART tree. A popular ensemble method is bootstrapped aggregated CART, referred to as Bagged CART (CITE). Bagged CART involves fitting multiple CARTs (i.e., trees) on bootstrapped data samples. By "growing" trees on bootstrapped samples, we can reduce the chances of overfitting our data. Each tree will generate a probability that an individual will be assigned to treatment, with a final probability of membership based on aggregating the probability of assignment across all trees. Finally, a more robust ensemble method is random forest (CITE). Compared to CART and Bagging, random forest algorithm grows trees by randomly selecting covariates and individuals to grow individual trees. With each new tree, the algorithm "learns" the best combination of covariates to generate a final probabilistic prediction based on the "forest" it has generated. These random forest techniques have shown significant promise in predicting propensity scores in the presence of complex data associations (CITE). However, with the recent advances in computing power, non-tree-based methods, such as neural networks, have increased in popularity.

### Artificial Neural Networks

#### Artificial Neuron

The building block for artificial neural networks were initially developed by psychologist Frank Rosenblatt (1958) with their idea of a perceptron or artificial neuron. An artificial neuron is a simplified mathematical model of neurons in our brain. Neurons are biological switches that take input signals from other neurons that either cause the neuron to fire or not. Neurons can be thought of as simple processing units. In our brain, these individual neurons are connected into vast networks of billions of neurons, where the outputs of one neuron become inputs of another, allowing for the transmission of complex information.

An artificial neuron mimics the function of a biological neuron. The artificial neuron takes in a set of inputs of $p$ variables, $X=(X_1, X_2, \cdots, X_i)$ that have corresponding weights $w=(w_1, w_2, \cdots ,w_i)$ that represent the different input strength of each variable. Each variable is then multiplied by its corresponding weight. These weighted variables are then summed together, and a bias term is added ($\beta_0$), to create a linear transformation of the input variables. The weights and bias terms are analogous to the slope and intercept of a linear regression. The linear transformation is then passed to an activation function $g(z)$. The activation function takes in the linear transformation and performs a computation that generates an output ($\hat{y}$). This activation function is a linear function, such as the sigmoid function, which outputs values between 0 and 1.

We can express this artificial neuron generally as:

$$
\hat{y} = g(z) = g(\beta_0 + \sum_{i=1}^{p}X_iw_i)
$$

This simple neuron can be used to solve a binary classification problem. In our case, we are interested in classifying each student in our sample as belonging to the treatment or control condition conditional on observed covariates. If we set the activation function $g(z)$ to a sigmoid function, we are guaranteed an output value between 0 and 1. We can consider it as a predicted probability of assignment to treatment (i.e., propensity score). For the neuron to make accurate predictions for each student, it must "learn" the correct weight and bias parameter specifications. This is analogous to finding the right slope and intercept that produce a line of best fit for bivariate data.

This learning process works by taking each observation in our sample and feeding it into the neuron. In which the first pass has initialized weights and bias set to a random number. The outcome variable is set to the binary indicator of treatment assignment, so we obtain a predicted probability of treatment assignment for each student. Our initial pass of the data will likely generate wrong predictions since the weights and bias terms are initialized to random numbers. In other words, our prediction of treatment assignment ($\hat{y}$) will be far away from the actual treatment assignment ($y$), with an associated loss.

The goal of the neuron -- and most machine learning algorithms - is to correctly learn the values of the weights and bias parameters that minimize a loss function, for example, the mean squared error (MSE). For each subsequent data pass, the neuron will update the bias values and weights iteratively to minimize this loss function. This updating is commonly done using a process known as backpropagation. After a user-defined number of iterations, the neuron will be optimized with bias and weight terms that best predict assignment to treatment. For simple classification problems that have linear covariate associations a single neuron may be appropriate, however, as the complexities of the associations between covariates increase more neurons are needed in order to learn complex functional forms.

#### Single-Layer Neural Network

Following the development of the artificial neuron, work began to model more complex processing units composed of interconnected artificial neurons organized in layers, culminating with the creation of the single-layer feed-forward neural network (NN) (Figure X). NN are refereed to as single-layer because of the single middle layer of interconnection artificial neurons. These NN can learn very complex data representations, by generating various non-linear representation of the data through multiple artificial neurons. These non-linear outputs are passed into a final layer which pools the outputs of the individual neurons into a non-linear function, $f(x)$ that generation a prediction ($\hat{y}$). By having multiple artificial neurons calculating predictions on various non-linear combination of the input variables these network are able to learn complex non-linear and non-additive association between covariates.

The NN is comprised of a series of artificial neurons organized into three layers (Schalkoff, 1997). The first layer is the input layer of $p$ variables, $X_p=(X_1, X_2, \cdots, X_p)$, our observed covariates. Those input variables are fully connected to the second layer - a hidden layer - made up of $K$ hidden neurons, $A_k = (A_1, A_2, \cdots, A_k)$, which are identical to the artificial neurons mentioned previously, except that the activation function is swapped out to a non-linear function. Each hidden neuron calculates its own prediction based on its unique non-linear combination of variable inputs, with have its own associated weight and bias parameters. The last layer is the output layer, which pools the non-linear outputs of the hidden neurons and passes them as inputs to a non-linear output function, $f(x)$ which generates a final prediction ($\hat{y}$). The interconnected neurons allow NN to learn complex association between covariates and treatment assignment than any single artificial neuron.

We can express a NN generally as[^02-propensity_over-1]:

[^02-propensity_over-1]: This notation was adapted from James, Witten, Hastie and Tibshirani, 2021

$$
\hat{y} = f(x) = B_0 + \sum_{k=1}^{K}B_kh_k(X) = B_0 + \sum_{k=1}^{K}B_kg(w_{k0}+\sum_{j=1}^{p}w_{kj}X_j)
$$

Where each hidden unit $A_k, k =1,\cdots,K$ is created from a weighted linear combination of input variables, $X_1,X_2, \cdots,X_p$ that are applied to an activation function $g(z)$ resulting in an "activation" for each hidden neuron, $A_k$:

$$
A_k = h_k(X) = g(w_{k0}+\sum_{j=1}^{p}w_{kj}X_j)
$$

Where $B_k$ and $w_{kj}$ are the bias and corresponding weights for each $A_k$ hidden neuron. We can regard these activations are the individual predictions of each hidden neuron. Once these predictions are pooled together and passed through the output function ($f(x)$) has its own bias term, $B_0$, and outputs a final prediction ($\hat{y}$).

This learning process of the NN works in similar way to a single neuron but instead of learning only one set of weights and bias terms it learns multiple, simultaneously. In a similar fashion, the ultimate goal of the NN is to learn the correct weights and bias parameters such that the loss function is minimized. In contrast to artifical neuron, NN can learn complex non-linear relationships in a computationally effecient manner. Relatively few studies have looked at the overall performance of NN in estimating propensity scores. XX find that the NN approach is able...

More modern approaches use NN that have more than one hidden layer, with varying number of nodes. This NN architecutre is reffered to as deep neural networks (DNN) or multilayer neural networks.

#### Deep Neural Networks

DNN distinguishign feature is multiple hidden layers that allow it to capture even more complex relationships amongst covariates. These multiple hidden layers is what makes this neural network architecture "deep." Figure X shows a diagram of a DNN with two hidden layers ($L_1,L2$), which we use as superscript to distinc which layer we are reffering to. The first hidden layer functions exactly as the middle layer of a NN (eq...). Such that:

$$
A_k^{(1)}=h_k^{(1)}(X) = g(w_{k0}^{(1)}+\sum_{j=1}^{p}w_{kj}^{(1)}X_j)
$$Where each hidden unit in the first layer $A_k^{(1)}, k =1,\cdots,K_1$ is created from a weighted linear combination of input variables, $X_1,X_2, \cdots,X_p$ that are applied to a non-linear activation function $g(z)$ resulting in an output for each hidden neuron in the first layer, $A_k^{(1)}$.

However, in this DNN example we have a second hidden layer, $L_2$. Which takes as inputs the output $A_k^{(1)}$ of the first hidden layer and computer new activations. We calculate the activation of the second layer as follows: ,

$$
A_l^{(2)}=h_k^{(2)}(X) = g(w_{l0}^{(2)}+\sum_{k=1}^{K_1}w_{lk}^{(2)}A_k^{(1)})
$$

Where each hidden unit in the second layer $A_l^{(2)}, l =1,\cdots,K_2$ is a function from the output of the weighted non-linear combinations of $A_k^{(1)}$. This process could continue with increasing number of hidden layers. As data moves from layer to layer, the DNN is able to aproximate any smooth function (CITE). The final layer of the DNN pools outputs of the second layer and feeds it into an output function which generates a prediction.

DNN's can uncover complex relationships in high-dimensional data with better precision than traditional statistical methods and machine learning methods, which were novel just a decade ago (Hernández-Blanco et al., 2019; LeCun et al., 2015; Schmidhuber, 2014; Zou et al., 2019). A DNNs ability to identify the intricate relationships withing large-volume, high-dimensional data makes these algorithms applicable to causal methods, like propensity score analysis.

DNN's have gained significant attention in the industry and academic research, making considerable advances in image recognition and natural language processing (Krittanawong et al., 2019; LeCun et al., 2015; Zou et al., 2019). Recent investments by Google in software development now makes it easy to develop DNN models using user-friendly programming languages (Doleck et al., 2020; Pang et al., 2019). Though to date, DNN has not been evaluated in propensity score estimation.

talka bout them and propensity score and lack of that...generaly talk about other propensity score simluations with neural networks (maybe incorporate that in the invidiual sections)?

## Proposed Method

### Simulation Design

My proposed simulation is informed by earlier propensity score estimation simulations conducted by Setoguchi et al. (2008), Lee et al. (2009), and Cannas and Arpino (2019). The starting point for the development of my simulation code-base began with the publicly available simulation code provided by Cannas and Arpino (2019).

I use a Monte Carlo simulation to compare the performance of various propensity score estimation methods against the traditional logistic regression and my proposed DNN-based approach in a high-dimensional context. I assess the performance by how well each method captures a simulated ATE and balances the observed covariates. Specifically, I access the performance of popular machine learning techniques used in the propensity score estimation literature, including classification and regression tree (CART), bagged CART, random forest, and single-layer neural network. In this section, I describe the simulation design for this study, including the data generation process and the scenarios that I consider.

#### Covariates

I took a programmatic approach to generating three covariate levels that varied in dimensionality ($p = 20, 100, 200$) . The covariates were generated from random pulls from three distributions; a normal distribution with a mean of 0 and SD of 1, a uniform distribution with a range from 0 to 1, and a Bernoulli distribution with a probability of success set to 0.5. For each covariate level half of the covariates were generated from the normal distribution, a quarter from the uniform distribution and the remaining quarter from the Bernoulli distribution. The majority of the propensity score simulation literature generate uncorrelated covariates (CITE), however, this is not representative of the correlated data typically found in administrative datasets. Therefore, I induce a correlation structure among the covariates by using the inverse CDF (cumulative distribution function) method (CITE).

Covariates were constructed in such a way that half of the covariates affected both the treatment and the outcome (i.e., confounders), a quarter affected only treatment, and the remaining quarter affected the outcome.

#### Sample Size

I use a constant sample size of 300,000 for all simulation conditions. This sample size is in line with average high-dimensional datasets found in educaiton (CITE). Note most of the current propensity score simulation studies generate much lower samples sizes. Given I am trying to mimic real world applicaiton of high-dimensianl administrative data this sample size seems appropriate.

#### Population Treatment Assignment Models

I generated the population treatment assignment models using standard logistic regression. These population treatment assignment models can also be refereed to as population propensity score models since they generate the "true" probability of being assignment to treatment conditional on observed covariates. The following logistic regression model represented the general structure of these models.

$$
Pr(Z = 1) = (1 + exp(-(\beta_0 + \sum_{i=1}^{p} \beta_iX_i)))^{-1}
$$ {#eq-popps}

Where $X_i$ represents $p$ covariates with their associated coefficients ($B_i$) . The output, $Pr(Z = 1)$ is the probability of assignment to the treatment condition (i.e. propensity score). The exact values of the coefficients will be based on previous coefficient values for propensity score simulation studies, mainly Keller, et al. 2015. I generated a binary treatment assignment variable by comparing an individuals true propensity score to a random draw from a uniform distribution bounded between 0 and 1. If an individuals propensity score was higher than the random draw from the uniform distribution the individual was assigned to treatment, such that $Z = 1$, otherwise $Z = 0$ . Using this approach the probability of assignment to treatment should be approximately 0.5. Which is the probability of assignment to a simple randomized experiment with a binary treatment.

For this simulation, I generated three types of population treatment assignment models (A, B, C, and D) in which I added increasing levels of complexity to the model in the form of interactions (i.e., non-additivity) and higher order terms (i.e. non-linearity).

**Scenario "A"** includes only main effects which assumes a linear relationship between covariates and treatment assignment. I consider this our base model which is expressed using the following formula:

$$
PS^{A}(X) = (1 + exp(-(\beta_0 + \sum_{i=1}^{p} \beta_iX_i)))^{-1}
$$ {#eq-psA}

$X_i$ represents $p$ covariates with their associated coefficients $\beta_i$. The output, $Pr(Z = 1)$ is the probability of assignment to the treatment condition (i.e. propensity score).

**Scenario "B"** describes a non-linear model in which I include quadratic terms on a subset of covariates:

$$
PS^{B} = PS^{A}(X) + \beta_iX_i^2
$$ {#eq-psB}

**Scenario "C"** describes a non-additive model in which I include interactions on a subset of covariates:

$$
PS^{C} = PS^{A}(X) + \beta_iX_iX_{i+1}
$$ {#eq-psC}

**Scenario "D"** describes the most complex population treatment assignment model and includes both non-linearities and non-additivities:

$$
PS^{D} = PS^{A}(X) + \beta_iX_i^2 + \beta_iX_iX_{i+1}
$$ {#eq-psD}

#### Population Outcome Models

In addition, I generated a series of population outcome models in which I also varied the level of complexities between covariates and a potential outcome by including interactions and higher order terms. Adding these complexities in the outcome model is relatively novel in the propensity score literature, although it is much more representative of the complex associations found in educational research (CITE).

Continuous potential outcomes were generated through regression models. The general structure of the population outcome models is defines as follows:

$$
Y(Z) = \alpha^Z + \sum_{i=1}^{p} \pi_iX_i; Z = 0,1
$$

Where $X_i$ represents $p$ covariates with their associated coefficients ($\pi_i$) . The output, $Y$ is the continuous outcome. When $Z=1$, the outcome is the potential outcome for treatment and when $Z = 0$ the potential outcome for the control. $\alpha^Z$ represented the coefficient and intercept for $\alpha^0$ and $\alpha^1$ which are set such that the population treatment effect is 0.3. Which is in line with the typical small treatment effects we observe in education research in which the average effect size is equal to .28 SD (CITE).

Similar to the population treatment assignment models, I generated three types of population outcome models, indexed by lower-case letters (a, b, c, and d) with varying levels of complexity.

**Scenario "a"** - the base model - includes only main effects which assumes a linear relationship between covariates and potential outcome. I consider this our base model which is expressed using the following formula:

$$
Y^{a}(Z) = \alpha_0^{a,Z} + \sum_{i=1}^{p} \alpha_i^{a,Z}X_i = \alpha_0^{a,Z} + g^a(X); Z = 0,1
$$

Where $X_i$ represents $p$ covariates with their associated coefficients ($\alpha_i^{\alpha,Z}$) depending on treatment assignment, $Z$. The output, $Y^{a}(Z)$ is set to $Y^{a}(1)$ for the outcome model related to treatment assignment and $Y^{a}(0)$ for the control condition.

**Scenario "b"** describes a non-linear model in which I include quadratic terms on a subset of covariates:

$$
Y^{b}(Z) =  \alpha_0^{a,Z} + g^a(X) + \alpha_iX_i^2; Z = 0,1
$$

**Scenario "c"** describes a non-additive model in which I include interactions on a subset of covariates:

$$
Y^{c}(Z) =  \alpha_0^{a,Z} + g^a(X) + \alpha_iX_iX_{i+1}; Z = 0,1
$$

**Scenario "d"** describes the complex model which includes both non-linearities and non-additivities:

$$
Y^{d}(Z) =  \alpha_0^{a,Z} + g^a(X) + \alpha_iX_i^2 + \alpha_iX_iX_{i+1}; Z = 0,1
$$

#### Propensity Score Estimation Methods

Specifically, I access the performance of popular machine learning techniques used in the propensity score estimation literature, including classification and regression tree (CART), bagged CART, random forest, and single-layer neural network. In this section, I describe the simulation design for this study, including the data generation process and the scenarios that I consider.

-   **Logistic regression**: standard logistic regression with only main effects

-   **Classification and Regression Trees (CART)**: using the R *rpart* package (Therneau & Atkinson, 2018) with default parameters and recursive partitioning.

-   **Bagged CART**: boostrapped aggregated trees with 100 bootstrap samples using the R *ipred* package (Peters, Hothorn, & Lausen, 2002).

-   **Random Forest**: parallel tree generation on subsamples based on randomly selected covariates using the R *randomForest* package with default parameters (Liaw & Wiener, 2002).

-   **Single-Layer Neural Network (NN)**: a simple three layer NN with an input layer, one hidden layer, and an output layer. The input layer was equal to $p$ covariates and the hidden layer was made up of hidden neurons equal to 1/3 of the input covariates, as is standard practice in neural network applications and default parameters. Using the R *nnet* package (Ripley & Venables, 2016).

-   **Deep Neural Network (DNN-2)**: a four layer DNN with an input layer, thow hidden layers, and an output layer. The input layer was equal to $p$ covariates and the hidden layers were made up of hidden neurons equal to 1/3 of the input covariates with default parameters. Using the R *keras* package (Chollet et al., 2017) which interfaces with Python.

```{=html}
<!-- -->
```
-   **Deep Neural Network (DNN-3)**: a five layer DNN with an input layer, three hidden layers, and an output layer. The input layer was equal to $p$ covariates and the hidden layers were made up of hidden neurons equal to 1/3 of the input covariates with default parameters. Using the R *keras* package (Chollet et al., 2017) which interfaces with Python.

#### Simulation Scenarios

In total I test three covariates levels (p = 20, 100, 200), with four population treatment asisngment models (A,B,C,D) and four population outcome models (a,b,c,d). I cross all these conditions for a total of 48 simulated conditions. Given the high-dimensional data being generated in my simulation I use the University of Pittsburgh Center for Research Computing cluster to run all my simulations in high-memory nodes. Each scenario was simulated 1,000 to make it a true Monte Carlo simulation. Simulated data and data analysis was conducted on Python 3 on a Mac platform.

|     | A (base) | B (non-linearity | C (non-additivity) | D (complex) |
|:---:|:--------:|:----------------:|:------------------:|:-----------:|
|  a  |    Aa    |        Ba        |         Ca         |     Da      |
|  b  |    Ab    |        Bb        |         Cb         |     Db      |
|  c  |    Ac    |        Bc        |         Cc         |     Dc      |
|  d  |    Ad    |        Bd        |         Cd         |     Dd      |

#### Estimating Treatment Effects - Propensity Score Weighting (PW)

In order to recapture the treatment effect I used a propensity score weighting approach, specifaly inverse probability of treatment weighting (IPTW) (CITE). Where covariate balance is achieved between treated and untreated units by reweighting observations based on their propensity scores. Such that each treatd obervation is equal to 1/e(x) and untreated units receive a weight of 1/(1-ei(x), where ei(x) is the estimated propensity socre for unit i this weighting scheme estimates the ATE. However, in observational studies the more policy relevent estimand is the ATT. Therefore for this simulation I give a weighth of 1 to each treated unit and a control unit gets assigned a weight of e(x)/(1-e(x), so that untreated units with similar propensity score to that of treated units are upweighted and untreated units with disimilar proensity scores to treated units are downweighted (CITE). The ATT was calculated using alinear model where the outcome variable was (Y) and being predicted by a binary treatment assigmetn varible (Z), with ATT weights included.

#### Performance Metrics

To gauge performance of the aforementioned methods I used the following metrics:

-   **Average Standardized Absolute Mean Distance (ASAM)**: is a measure of of covariate balance. After calculating the ATT, the absolute value of the standardized difference of means between the treated and untreated group was calculated for each covariate and averaged across all covariates. Lower ASAM values indicate better covariate balance.

-   **Absolute Relative Bias**: the absolute difference between the estimated treatment effect and the population treatment effect. Lower values indicate less bias in recapturing the population treatment effect of 0.3.

-   **Mean Squared Error (MSE)**: measures the average squared difference between the estimated treatment effect and the population treatment effect. MSE is a good representation of both bias and variance and a common performane metric in machine learning literature.

-   **Weights**: a known issue with PSW is that extreme weights can result in biased treatment effect. Thefore I examin the distribution of the estimated wweights for the untreated units

## Looking Ahead

As educational research moves towards the age of Big Data, researchers will have access to tremendous amounts of covariates. With such a large number of covariates at our disposal, researchers can leverage causal methods like propensity score estimation. The traditional approach has been to use logistic regression, but as computational power has increased more sophisticated Deep Learning methods have been developed that are well situated to solve classification problems such as propensity score estimation. Results showed that DNN and Logistic regression performed equally well across conditions and sample sizes. In the final iteration of the paper, we will focus on creating a Monte Carlo simulation study to analyze these effects. We will further consider varying the linear relationship the covariates had with the outcome to more complex non-linear patterns, perhaps in that context DNN model would outperform Logistic models.

## Tables and Figures
