```{r}
#| include: false
packages <- c(
  "tidyverse",
  "gt",
  "ggthemes",
  "gtsummary",
  "simhelpers"
)

lapply(packages, library, character.only = TRUE)

source("~/Projects/inProgress/dissertation/ggplot_theme_pub.R")

df1 <- readRDS("~/Projects/inProgress/2018_propensity_neuralnet_paper/data/sim_results_n10000_r1000_P_e.rds")

df2 <- readRDS("~/Projects/inProgress/2018_propensity_neuralnet_paper/data/sim_results_n10000_r1000_NP_e.rds")

res <- bind_rows(df1, df2)



res_all <- readRDS("~/Projects/inProgress/2018_propensity_neuralnet_paper/data/res_all.rds")


over_df <-
  res %>%
  group_by(p, method) %>%
  summarise(
    Bias = mean(Bias),
    Rel_Bias = mean(Rel_Bias),
    Per_Rel_Bias = mean(Per_Rel_Bias),
    ATE_se = mean(ATE_se),
    MSE = mean(MSE),
    Power = mean(Power),
    PS_Weights = mean(mean_ps_weights),
    ASAM = mean(ASAM),
    coverage_95 = mean(ci_95),
    .groups = "rowwise"
  ) %>%
  ungroup()


res_sum_df <-
  res %>%
  group_by(p, method, scenarioT, scenarioY) %>%
  summarise(
    Bias = mean(Bias),
    Rel_Bias = mean(Rel_Bias),
    Per_Rel_Bias = mean(Per_Rel_Bias),
    ATE_se = mean(ATE_se),
    MSE = mean(MSE),
    Power = mean(Power),
    PS_Weights = mean(mean_ps_weights),
    ASAM = mean(ASAM),
    coverage_95 = mean(ci_95),
    .groups = "rowwise"
  ) %>%
  ungroup()

```

```{r}


df <- res_all |> 
  mutate(true_ATE = 0.3) |> 
  group_by(method, p) |> 
  do(calc_absolute(., estimates = ATE, true_param = true_ATE))  

df |> 
  ggplot(aes(x = method, y = bias, fill = as.factor(p))) +
  ylab("Bias") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  theme_Publication() +
  scale_fill_Publication() +
  geom_pointrange(aes(ymin=bias-bias_mcse, ymax=bias+bias_mcse, y = bias))




df |> 
  ggplot(aes(x = method, y = bias, colour = as.factor(p))) +
  geom_hline(yintercept = 0, linetype = 'solid', size = .2) +
  geom_segment(aes(yend = 0, xend = method), color = 'grey') +
  geom_point() +
  geom_point(aes(y = bias-bias_mcse), shape = 95, size = 3) +
  geom_point(aes(y = bias+bias_mcse), shape = 95, size = 3) +
  theme_Publication() +
  facet_wrap(~as.factor(p))


```

# Paper 1: Evaluating modern propensity score estimation methods with high-dimensional data

## Introduction

Traditionally, questions of causality have been addressed through the use of randomized control trials (RCT), in which individuals are randomly assigned to either a treatment group that receives an intervention or a control group [@murnane2010methods; @Rosenbaum.2010]. Randomization ensures that, on average, both groups are balanced on all observable and unobservable characteristics, eliminating the possibility that an intervention's effect is due to differences in group characteristics [@Rosenbaum.2010]. However, in education and broader social science research, ethical, financial, or practical obstacles can prevent a researcher from conducting a randomized experiment. Instead, researchers must rely on non-experimental or observational studies in which students self-select or are placed in an intervention without randomization.

Observational studies complicate causal inference because, in the absence of randomization, an imbalance in the distribution of student characteristics (i.e., covariates) between treated and untreated students could lead us to incorrectly attribute a change in the outcome to the intervention or policy, as opposed to differences in group composition [@Rosenbaum.2010]. A vast body of methodological literature has centered on developing quasi-experimental methods that attempt to address the internal validity issues associated with observational data by balancing the *observed* characteristics of those who received treatment and those who did not. One such popular approach is propensity score analysis [@Stuart.2010; @Lee.2010].

Propensity score analysis is a widely used quasi-experimental method in education research [@Fan.2011; @Powell.2020; @Rosenbaum.2010; @Stuart.2010; @ho2007matching; @Lee.2010]. It aims to balance observed characteristics between treated and untreated students, similar to how randomization balances observable characteristics between treatment and control groups. The propensity score represents the probability that a student would have received treatment based on their observed covariates [@Rosenbaum.1981; @Rosenbaum.1984]. There are several ways to use propensity scores to balance student covariates, including *matching*, in which treated and untreated students with similar propensity scores are paired to form matched sets;\* weighting*, in which treatment groups are re-weighted so that observed covariates are balanced between groups, and* stratification\*, in which the propensity score is used to group students into unique strata with similar scores, to balance observed covariates within each stratum [@Pan.2018qcl].

However, certain strict assumptions must be met to estimate unbiased treatment effects correctly using propensity score analysis. For example, the propensity score model must:

(1) capture all the covariates related to the selection into treatment (Paul R. Rosenbaum 2010).

(2) correctly model the association between the student's characteristics and treatment selection, meaning that all proper interactions and non-linear terms should be specified (Paul R. Rosenbaum 2010).

To have the best chance of meeting the first condition, the literature suggests a "kitchen sink" approach for variable selection because the penalties are high if a potential confounder is not included in the propensity score model [@Pirracchio.2015; @Shortreed.2017lu8; @Stuart.2010]. To account for unmeasured confounders, sensitivity analysis can be used to understand how excluding such confounders could introduce bias into the treatment effect estimation (CITE). To meet the second condition, the literature suggests iterating through various model specifications, with each iteration adding interactions or non-linear terms until a balance is achieved among the observed characteristics between treated and untreated students [@Stuart.2010]. If these assumptions are not met, it could lead to biased treatment effect estimates (CITE).

At first glance, high-dimensional data (i.e., big data) should be an asset to propensity score analysis, as conventional practices stress the importance of including all available variables in the propensity score model to increase the chances of capturing all relevant confounders. Nevertheless, this is not the case. The performance of the traditional logistic model for estimating propensity scores tends to decrease as more variables are included in the estimation step [@Hill.2011soh].

When modeling high-dimensional data, logistic models may overfit the data or experience issues with perfect separation, resulting in fitted probabilities that are either 0 or 1 [@Hill.2011soh]. This can lead to propensity scores with little variability. Additionally, as the number of covariates available to estimate the propensity score increases, it becomes increasingly challenging to iteratively model all appropriate interactions and non-linearities using a logistic model [@d2020underspecification; @Dorie.2019; @Hill.2011soh].

With the rapid advancement of sophisticated machine learning algorithms, there are growing opportunities to develop and apply new methods for estimating propensity scores in high-dimensional data [@daniel2019big; @Hill.2011soh; @hernandez2019systematic; @williamson2017big]. These machine learning algorithms offer an automatic, data-driven way to capture non-linearities and non-additivity in the propensity score estimation model, potentially leading to a more balanced covariate distribution without the need for manual iteration. Most previous propensity score simulation research has focused on low-dimensional datasets [@Setoguchi.2008; @Stuart.2010; @Lee.2010]. To date, no research has evaluated the performance of these machine learning methods when applied to high-dimensional data.

Additionally, recent advances in computing and software have generated a novel machine learning algorithm that could be used to estimate the propensity score, deep neural networks (DNNs) [@LeCun.2015; @hernandez2019systematic; @LeCun.2015]. DNNs are artificial intelligence algorithms modeled after the architecture of the human brain [@LeCun.2015; @Pang.2019; @Farrell.2021]. They are widely used across various industries for complex prediction and classification tasks and have been shown to be effective in modeling complex high-dimensional data. The flexibility of DNNs enables them to capture complex interactions and non-linearities and perform automatic variable selection [@hernandez2019systematic; @LeCun.2015; @schmidhuber2015deep; @Pang.2019]. However, there has been limited application of DNNs to causal inference, particularly in the field of social science research.

In this study, I assess the performance of the traditional logistic model and other machine learning methods with high-dimensional data using propensity score weighting. In addition, I overcome the limitations of existing methods by proposing a novel DNN-based approach to the estimation of propensity scores and illustrating a novel framework for generating multi-type, correlated high-dimensional data replicating the high-dimensional administrative data that is becoming available to education researchers.

The remainder of this document is structured as follows: First, I will review the relevant literature for this study. Then, I will describe my simulation approach and present my results. Finally, I will conclude with a discussion section that focuses on the contributions of my study.

\newpage

## Literature Review

In this section, I will review the literature on causal inference and propensity score analysis, focusing specifically on machine learning approaches for estimating propensity scores. Additionally, I will draw from the literature on neural network approaches, including deep neural networks, to motivate the development of my DNN-based method for estimating propensity scores.

### Neyman-Holland-Rubin Model for Causal Inference

The Neyman-Holland-Rubin model, also known as the Rubin model, forms the basis of my approach to causal inference [@rubin1976inference; @rubin2005causal; @holland1986statistics; @neyman1923application]. A core tenet of the Rubin model states that a treatment effect is the difference between two potential outcomes for an individual [@rubin2005causal]. Consider a dichotomous treatment variable ($Z$), where $Z_i = 1$ represents the $i$'th student being in the treatment group -- say a college access program - and $Z_i = 0$ represents the student not being in the treatment group. Let $Y_{iz}$ be a potential outcome for student $i$ depending on treatment assignment $Z$, such that an individual student has two potential outcomes. $Y_{i1}$ is the potential outcome had student $i$ participated in the college access program and $Y_{i0}$ is the potential outcome had that same student not participated. Therefore, the treatment effect of the program for student $i$ would simply be the difference in their two potential outcomes:

$$
\tau_i = Y_{i1} - Y_{i0}
$$ {#eq-treat}

However, in the real world, we do not simultaneously observe both potential outcomes for the same student. That is to say, we can only observe student $i$'s potential outcome in the college access program if they participated in the program. This is why the fundamental problem of causal inference is a missing data problem [@holland1986statistics]. Although we cannot directly observe these potential outcomes under certain assumptions, we can estimate the average treatment effect (ATE) across our sample, defined as:

$$
ATE= \mathbb{E}(Y_{i1} -Y_{i0})=\mathbb{E}(Y_{i1})-\mathbb{E}(Y_{i0})
$$ {#eq-ate}

Where $\mathbb{E}(Y_{i1})$ is the expected value of all students in the treatment group and $\mathbb{E}(Y_{i0})$ is the expected value of all students in the control group. In the context of a true experiment in which students are *randomly* assigned to treatment conditions, we can estimate the ATE in a straightforward manner since with a large enough sample size, both treated and control students will be balanced on all observable and unobservable characteristics. Stated differently, a randomized experiment ensures that $Y_1$ and $Y_0$ are independent of treatment assignment ($Z$), and therefore, the treatment effect can be regarded as causal [@Rosenbaum.2010].

In addition, we may also be interested in estimating the average treatment effect among those who actually received the treatment. In this case, we may be interested in the average treatment effect on the treated (ATT) [@Rosenbaum.2010], which is defined as:

$$
ATT= \mathbb{E}(Y_{i1} -Y_{i0}|Z_i=1)=\mathbb{E}(Y_{i1}|Z_i=1)-\mathbb{E}(Y_{i0}|Z_i=1)
$$ {#eq-att}

### Observational Studies

In behavioral and social science research, conducting a randomized experiment may not always be possible due to ethical, financial, or practical limitations [@Bai.2011; @Pan.2018qcl]. This leads researchers to rely on non-experimental or observational data, where students either self-select or are assigned to an intervention without randomization. This creates a challenge for causal inference, as an imbalance in the distribution of student characteristics (i.e., covariates) between the treated and untreated students could lead to incorrect attributions of changes in the outcome to the intervention rather than differences in group composition [@Austin.2011].

For example, consider our earlier example of the college-access program. If students were not randomized into treatment or control groups but instead self-selected into the program, the underlying treatment assignment would be unknown. It could be that students who self-select into the program are generally more interested in going to college than those who did not sign up. Given this imbalance in college interest, we may incorrectly attribute an increase in college enrollment to the treatment instead of differences in college interest between treated and untreated students. In other words, without randomization, observational studies cannot ensure that a student's outcome is independent of treatment assignment ($Z$) [@Rosenbaum.2010].

In order to overcome the challenges posed by observational data, various statistical methods have been developed to estimate unbiased treatment effects. These methods include instrumental variable approaches, regression discontinuity, synthetic control, and propensity score analysis [@Austin.2011; @Bai.2011; @Stuart.2010; @Rosenbaum.2010]. For the purpose of this dissertation, I focus on propensity score analysis, a quasi-experimental method that attempts to balance observed group differences through the use of a balancing score that is derived from student-level covariates.

\newpage

### Propensity Scores

The concept of propensity scores was first introduced by Rosenbaum and Rubin as a means to enhance the accuracy of capturing unbiased treatment effects in observational studies [@Rosenbaum.1984; @Rosenbaum.1981]. Propensity score analysis has become one of the most widely used quasi-experimental methods among social and behavioral researchers, particularly in the field of applied educational research [@Lee.2010; @ho2007matching; @Stuart.2010].

Rosenbaum and Rubin define a propensity score, $e(x)$ as the probability of assignment to treatment conditional on observed pre-treatment covariates. Defined as:

$$
e(x) = Pr(Z=1|\boldsymbol{X})
$$ {#eq-ps}

Propensity scores are typically estimated as predicted probabilities from a logistic regression model, where the binary outcome variable indicates whether a student received treatment, and is being predicted based on individual student characteristics ($\boldsymbol{X}$). The estimated propensity scores range from 0 to 1, with values closer to 1 indicating that an individual is more likely to be assigned to treatment. According to Rosenbaum and Rubin, if the propensity scores are balanced between the treatment and control groups, the distribution of observed covariates between the two groups will also be balanced [@Rosenbaum.1981; @Rosenbaum.1984; @Rosenbaum.2022]. This means that, in theory, two students with similar propensity scores should have a similar distribution of observed covariates.

Researchers can adjust for the imbalance in the joint covariate distribution between the treatment and control groups by conditioning on the propensity score using various techniques, including matching, stratification, and weighting. Matching involves pairing treated students with control students based on the proximity of their propensity scores, while stratification involves grouping students into unique strata based on their propensity scores [@ho2007matching; @Rosenbaum.2010]. Weighting involves reweighing students in the sample so that the distribution of propensity scores in the control group is similar to that of the treated group [@Lee.2010]. The goal of these methods is to approximate true randomization conditions by using the propensity score to eliminate bias in the treatment effect due to observed confounding and approximate the true causal effect [@Pan.2018qcl]. However, for this to be achieved, certain assumptions must hold.

#### Assumptions

Several assumptions must be met in order to estimate an unbiased treatment effect using propensity scores. These include the stable unit value assumption (SUTVA), the strong ignorability assumption, and the region of common support [@rubin2005causal]. It is important to note that, while necessary, these assumptions are not sufficient for unbiased causal inference using propensity scores. Furthermore, it can be challenging to assess the validity of these assumptions in practice, which can impact the validity of the estimated treatment effects [@Stuart.2010].

The first assumption, the stable unit treatment value assumption (SUTVA), states that there is only one version of the treatment condition and that the potential outcome of an individual is not influenced by the treatment assignment of any other individual [@Rosenbaum.2022]. This means that the outcome of a student is independent of whether another student is assigned to the treatment or control group.

The strong ignorability assumption states that the potential outcome of an individual is conditionally independent of treatment assignment if we successfully conditioned on all covariates related to treatment, $Y_1, Y_0 \perp Z|\boldsymbol{X}$. This assumption is satisfied in experimental studies through randomization, which ensures that treatment assignment is independent of an individual's outcome [@rubin2005causal; @Rosenbaum.2022]. However, this assumption holds true in observational studies only if *all* covariates related to treatment assignment are included in the propensity score model [@Rosenbaum.1981; @Rosenbaum.1984]. In practice, it is unlikely that all such covariates are included, and sensitivity analysis is, therefore, necessary to assess the influence of unmeasured confounding \[CITE\].

Finally, the region of common support assumption states that there must be sufficient overlap in the estimated propensity score distribution between the treatment and control groups [@Rosenbaum.2022]. This is crucial to ensure that suitable matches can be found for treated individuals. The adequacy of the common support can be assessed by plotting the distribution of the propensity scores in both the treatment and control groups and evaluating if there is sufficient overlap between the two distributions.

If these assumptions are satisfied, it is possible to estimate causal effects from observational data. However, many of these assumptions are untestable [@Stuart.2010; @ho2007matching; @Lee.2010]. For instance, in observational data, the underlying mechanism of treatment assignment is usually unknown, making it difficult to determine if all covariates related to treatment assignment have been included in the propensity score model [@Stuart.2010]. As a result, propensity score analysis should be viewed as a method to reduce, but not eliminate, the bias between treatment and outcome in observational studies. It is crucial to carefully consider which covariates are included in the propensity score model and to conduct sensitivity analysis to assess the impact of unmeasured confounding (CITE).

#### Covariate Selection

The selection of covariates in propensity score analysis should be based on those that are theoretically sound, related to treatment assignment, and related to the outcome, also known as confounders [@Stuart.2010; @Bai.2011; @Pan.2018qcl]. However, there is a debate in the literature as to which covariates should be included \[Austin, 2011\]. Some simulation studies have found that including confounders or covariates that only affect the outcome leads to more precise estimates of treatment effects [@Austin.2007]. On the other hand, other research has shown that including only covariates that affect treatment assignment leads to increased variance in the estimated treatment effect with no reduction in bias [@Brookhard.2006].

In practice, determining which covariates are confounders or related to just the treatment or the outcome can be challenging. To reduce the risk of excluding potential confounders, the literature suggests using a "kitchen sink" approach to variable selection, where all available covariates are included in the estimation of the propensity scores [@Pirracchio.2015; @Shortreed.2017lu8]. Additionally, it is important to ensure that the propensity score model captures the correct functional form of the covariates, including all relevant interactions and non-linear terms [@Rosenbaum.2010; @Pan.2018qcl]. If the propensity score model is misspecified, it will result in biased treatment effect estimates [@Rosenbaum.1984; @Rosenbaum.1981; @Guo.2009].

To balance the observed characteristics between treated and untreated students, iterating through various model specifications that include interactions and non-linear terms among the covariates is recommended [@murnane2010methods; @Rosenbaum.2010]. However, this iterative process can become challenging and time-consuming as the number of available covariates increases, especially when conducting propensity score analysis with high-dimensional administrative data.

\newpage

### Estimating Propensity Scores

Estimating the propensity score ($e(x)$) is a crucial step in propensity score analysis, as a misspecified propensity score model can result in propensity scores with little variability or extreme scores that lead to biased treatment effect estimates \[CITE\]. Propensity scores can be estimated using either parametric or non-parametric models, as long as the model outputs a probability that is bounded between 0 and 1. Logistic regression is a commonly used parametric model for estimating propensity scores, particularly in applied educational research [@Stuart.2010; @ho2007matching; @Lee.2010; @Collier.2021lp; @Keller.2015q8].

#### Logistic Regression

Logistic regression is the most common parametric model used to estimate propensity scores in the social science literature due to its ease of interpretation and familiarity with many applied researchers and its ability to accommodate both continuous and categorical covariates[@Keller.2015q8]. The logistic model is defined as follows:

$$
logit(Z = 1|\boldsymbol{X})=log\left( \frac{Pr(Z=1|\boldsymbol{X})}{1-Pr(Z=1|\boldsymbol{X})} \right) = \beta_o + \beta_1X_1 + \cdots + \beta_pX_p
$$ {#eq-log}

where $Z$, is a binary indicator equal to 1 if a student is in the treatment group and 0 otherwise, regressed on $\boldsymbol{X}$ a vector of observed covariates. The output of the logistic regression is predicted probabilities that are continuous and bounded between 0 and 1 (i.e., propensity scores).

Although logistic regression is the most commonly used method for estimating propensity scores, it may not always be the best option for inference. Several simulation studies have found that using logistic regression for propensity score modeling can result in biased treatment effects when the model is misspecified, especially in complex data where non-linear and non-additive terms are not included in the propensity score model [@Lee.2010; @Pan.2018qcl; @Setoguchi.2008; @Westreich.2010]. This is because parametric models, such as logistic regression, require assumptions about the functional form and distribution of covariates (CITE). This problem can be particularly pronounced when estimating propensity scores with many covariates [@Hill.2011soh]. In high-dimensional settings, the logistic model may produce propensity scores that are outside the desired range of 0 to 1 [@Hill.2011soh; @Collier.2021lp; @Keller.2015q8]. However, non-parametric machine learning algorithms have been proposed as alternatives for estimating the propensity score [@Stuart.2010; @ho2007matching; @Lee.2010; @Setoguchi.2008; @Watkins.2013; @Westreich.2010; @Cannas.2019].

#### Machine Learning Approaches

Machine learning algorithms have recently gained popularity in the causal inference and propensity score literature due to these algorithms being highly flexible and able to model complex functional forms iteratively without explicit manipulation from the research [@pearl2019seven; @grimmer2015we; @athey2015machine; @cui2020causal]. With a simple modification, these algorithms can output bounded probabilities that can be used as propensity scores. Given the success of machine learning algorithms for prediction tasks, it is reasonable to think they are worthy candidates for the propensity score estimation problem, which is a prediction problem. Broadly, machine learning algorithms used in propensity score estimation can be classified into tree-based, ensemble methods, and -- the focus of this dissertation -- neural network-based approaches.

#### Tree Based Methods

One popular machine learning approach to estimating propensity score is classification and regression trees (CART) [@li1984classification]. This algorithm recursively divides data into subsets based on individual covariates to predict the probability of membership assignment of a given individual. To generate propensity scores, the outcome is set to a binary indicator of treatment assignment ($Z$). This algorithm splits covariates by level of importance, with the first split being the covariate that can produce the most distinctive split. For example, if a student's age is related to treatment assignment, the first split would divide the data into students with $age > 15$ and $age < 15$. The algorithm continues by splitting the data by the next most influential covariate, creating a tree-like structure (see Figure @fig-decision_tree). Splitting stops when the data are binned into unique "branches," which minimizes the prediction error such that an additional split would not improve the prediction of treatment assignment. The final "branches" of the model represent the individual groups of students with similar propensity scores [@Stuart.2010]. The output of CART is the predicted probabilities, which are then used as propensity scores. Like logistic regression, the CART algorithm can handle both continuous and categorical covariates. However, unlike logistic regression, CART is insensitive to outliers and can automatically model interactions and higher-order terms [@Lee.2010]. Through a simulation study, Lee et al. (2009) found that CART outperformed logistic regression in terms of reducing bias and balancing covariates when the data had complex associations.

[![Simple decision tree predicting students GPA based on gender and homework grade](figures/decision_tree.jpg){#fig-decision_tree fig-align="center" width="436"}](https://www.apa.org/science/about/psa/2018/04/classification-regression-trees)

However, a well-known issue with CART is its tendency to overfit the data [@Stuart.2010; @ho2007matching]. To address this issue, a modified version of CART was developed that prunes back branches that do not contribute to reducing prediction error, referred to as pruned-CART [@gelfand1989iterative]. Although pruning can alleviate some of the changes in overfitting, a significant drawback of CART is its reliance on a single tree, which may be weak in predicting the propensity score.

#### Ensemble Methods

Ensemble methods are a family of algorithms that generate multiple trees to predict treatment assignment [@Keller.2019; @Stuart.2010; @ho2007matching; @Lee.2010]. By combining the predictions from multiple trees, ensemble methods can improve the accuracy of predictions since many weak trees together can create a better prediction than any single tree [@Lee.2010]. A popular ensemble method is bootstrapped aggregated CART, referred to as Bagged CART, which fits multiple CARTs on bootstrapped data samples [@Lee.2010]. Growing trees on bootstrapped samples reduce the chances of overfitting the data. Each tree will generate a probability that an individual will be assigned to treatment. The final probability of membership is based on aggregating the probabilities of assignment across all trees (i.e., forest). A more robust ensemble method is random forest [@biau2016random]. Compared to CART and Bagging, the random forest algorithm grows trees by randomly selecting covariates and individuals to grow individual trees. With each new tree, the algorithm "learns" the best combination of covariates to generate a final probabilistic prediction based on the "forest" it has created. These random forest techniques have shown significant promise in predicting propensity scores in complex data associations [@Cannas.2019; @Lee.2010]. However, with the recent advances in computing power, non-tree-based methods, such as neural networks, have increased in popularity [@Collier.2021lp; @Setoguchi.2008; @Keller.2015q8].

\newpage

### Artificial Neural Network Architectures

#### Artificial Neuron

The building block for artificial neural networks was initially developed by psychologist Frank Rosenblatt (1958) with their idea of a perceptron or artificial neuron, see @fig-perceptron. An artificial neuron is a simplified mathematical model of neurons in our brain [@james2013introduction]. Neurons are biological switches that take input signals from other neurons that cause the neuron to fire. Neurons can be thought of as simple processing units. In our brain, these individual neurons are connected into vast networks of billions of neurons, where the outputs of one neuron become inputs of another, allowing for the transmission of complex information [@schmidhuber2015deep; @LeCun.2015].

![Structure of a perceptron - artificial neuron](figures/perceptron.png){#fig-perceptron fig-align="center" width="437"}

The artificial neuron receives a set of inputs $X_i = (X_1, X_2, \dots, X_p)$ with corresponding weights $w_i = (w_1, w_2, \dots ,w_p)$ that represent the strength of each input variable. The inputs are multiplied by their corresponding weights and then summed together with a bias term $\beta_0$ to create a linear transformation of the inputs. The weight and bias terms are similar to the slope and intercept in linear regression. The linear transformation is then passed through an activation function $g(z)$ to generate an output ($\hat{y}$) that is either a 1 or 0, or a value between 0 and 1 if the activation function is set to be linear, such as the sigmoid function.

The artificial neuron in equation ({#eq-nn}) can be used to solve a binary classification problem. In our case, we are interested in classifying each student in our sample as belonging to the treatment or control condition based on observed covariates. If the activation function $g(z)$ is set to a sigmoid function, it will guarantee an output value between 0 and 1, which is the desired propensity score. For the neuron to make accurate predictions for each student, it must "learn" the correct weight and bias parameter specifications, similar to finding the correct slope and intercept in bivariate data.

$$
\hat{y} = g(z) = g(\beta_0 + \sum_{i=1}^{p}X_iw_i)
$$ {#eq-nn}

This learning process starts by feeding each observation in the sample into the neuron. The first pass has initialized weights and bias set to random numbers. The outcome variable is set to the binary indicator of treatment assignment, so a predicted probability of treatment assignment is calculated for each student. However, the initial pass of the data is likely to generate incorrect predictions as the weights and bias terms are initialized to random numbers. In other words, the prediction of treatment assignment ($\hat{y}$) will be far from the actual treatment assignment ($y$), leading to a high associated loss.

It's important to note that this learning process is an optimization problem that can be solved through various optimization algorithms such as gradient descent or stochastic gradient descent [@Goodfellow.2016]. The choice of optimization algorithm will impact the speed and accuracy of the learning process.

The goal of the artificial neuron and many machine learning algorithms is to correctly learn the values of the weights and bias parameters that minimize a loss function, such as the mean squared error (MSE) [@james2013introduction; @Pang.2019]. The neuron will update the bias and weight parameters iteratively for each subsequent data pass to minimize the loss function. This update is typically done using a process known as backpropagation [@schmidhuber2015deep]. After a user-defined number of iterations, the neuron will be optimized with bias and weight terms that best predict treatment assignment. A single neuron may be appropriate for simple classification problems with linear covariate associations, but as the complexities of covariate associations increase, more neurons may be needed to learn complex functional forms.

#### Single-Layer Neural Network

Following the development of the artificial neuron, researchers began to model more complex processing units composed of interconnected artificial neurons organized in layers. This work culminated in the creation of the single-layer feed-forward neural network (NN), as shown in Figure [@fig-nn; @Schmidhuber.2015; @Collier.2022; @Keller.2015q8]. NNs are referred to as single-layer because they have a single middle layer of interconnected artificial neurons. These NNs can learn very complex data representations by generating various non-linear representations of the data through multiple artificial neurons [@james2013introduction]. These non-linear outputs are passed into a final layer that pools the outputs of the individual neurons into a non-linear function, $f(x)$, generating a prediction ($\hat{y}$). By having multiple artificial neurons calculating predictions on various non-linear combinations of the input variables, these networks can learn complex non-linear and non-additive associations between covariates.

![Structure of single-layer neural network](figures/nn.png){#fig-nn fig-align="center" width="493"}

\newpage

The single-layer neural network (NN) comprises a series of artificial neurons organized into three layers [@james2013introduction]. The first layer is the input layer of $p$ variables, $X_i=(X_1, X_2, \dots, X_p)$, our observed covariates. These input variables are fully connected to the second layer - a hidden layer - which is made up of $K$ hidden neurons, $A = (A_1, A_2, \dots, A_k)$. The hidden neurons are identical to the artificial neurons mentioned previously, but the activation function is swapped out for a non-linear function. Each hidden neuron calculates its own prediction based on its unique non-linear combination of variable inputs and associated weight and bias parameters. The last layer is the output layer, which pools the non-linear outputs of the hidden neurons and passes them as inputs to a non-linear output function, $f(x)$, which generates the final prediction ($\hat{y}$). The interconnected neurons in NN allow it to learn more complex associations between covariates and treatment assignment than a single artificial neuron [@james2013introduction; @Schmidhuber.2015; @hernandez2019systematic].

We can express a NN generally as [^02-propensity_over-1]:

[^02-propensity_over-1]: This notation was adapted from James, Witten, Hastie and Tibshirani, 2021

$$
\hat{y} = f(x) = B_0 + \sum_{k=1}^{K}B_kh_k(X) = B_0 + \sum_{k=1}^{K}B_kg(w_{k0}+\sum_{i=1}^{p}w_{ki}X_i)
$$ {#eq-nn_gen}

Where each hidden unit $A_k, k =1,\dots,K$ is created from a weighted linear combination of input variables, $X_1,X_2, \dots,X_p$ that are applied to an activation function $g(z)$ resulting in an "activation" for each hidden neuron, $A_k$:

$$
A_k = h_k(X) = g(w_{k0}+\sum_{i=1}^{p}w_{ki}X_i)
$$ {#eq-nn_activation}

$w_{k0}$ and $w_{ki}$ are the bias and corresponding weights for each $A_k$ hidden neuron. We can regard these activations as individual predictions of each hidden neuron. These predictions are then pooled and passed to the output function ($f(x)$), which has its bias term, $B_0$, and produces a final prediction ($\hat{y}$).

The learning process of a NN works similarly to that of a single neuron, but instead of learning only one set of weights and bias terms, it learns multiple sets simultaneously. The ultimate goal of the NN is to minimize the loss function by learning the correct weights and bias parameters. Unlike a single artificial neuron, NN can efficiently learn complex non-linear relationships between covariates and treatment assignment [@james2013introduction]. A handful of studies have evaluated the performance of NN in estimating propensity scores in a low-dimensional setting [@Setoguchi.2008; @Cannas.2019; @Westreich.2010; @Keller.2015q8; @Collier.2021lp]. These studies suggest that NN is a suitable method for estimating the propensity score and generally outperforms logistic regression in reducing bias and balancing covariates [@Setoguchi.2008; @Cannas.2019; @Westreich.2010; @Keller.2015q8; @Collier.2021lp].

The development of NN has led to the creation of more complex NN architectures, such as deep neural networks (DNN) or multilayer neural networks [@LeCun.2015]. These networks consist of multiple hidden layers and varying numbers of hidden neurons.

#### Deep Neural Networks

The DNN architecture allows for the capturing of even more complex relationships among covariates compared to a single-layer NN [@Pang.2019; @hernandez2019systematic]. This is due to the multiple hidden layers in the network, making it "deep." The diagram in @fig-nn shows a DNN with two hidden layers ($L_1, L_2$).

![Structure of deep neural network with two hidden layers](figures/dnn.png){#fig-dnn fig-align="center"}

The first hidden layer functions in the same way as the middle layer in a NN (as described in Equation @eq-nn_activation). The first layer in a DNN can be represented as:

$$
A_k^{(1)}=h_k^{(1)}(X) = g(w_{k0}^{(1)}+\sum_{i=1}^{p}w_{ki}^{(1)}X_i)
$$ {#eq-dnn-1}

Here, each hidden unit in the first layer, $A_k^{(1)}, k =1,\dots,K$ is created from a weighted linear combination of the input variables, $X_1,X_2, \dots,X_p$. The weighted linear combination is then applied to a non-linear activation function $g(z)$, resulting in an output for each hidden neuron in the first layer, $A_k^{(1)}$.

The second hidden layer $L_2$ takes as inputs the outputs of $A_k^{(1)}$ of the first hidden layer and computes a new non-linear transformation, $A_k^{(2)}$. The new activations of the second layer are calculated as follows:

$$
A_l^{(2)}=h_k^{(2)}(X) = g(w_{l0}^{(2)}+\sum_{k=1}^{K_1}w_{lk}^{(2)}A_k^{(1)})
$$ {#eq-dnn-2}

Where each hidden unit in the second layer $A_l^{(2)}, l =1,\dots,K$ is a function of the output of the weighted non-linear combinations of $A_k^{(1)}$, this process can be extended to additional hidden layers. As data move from layer to layer, DNN can approximate increasingly complex functional forms [@LeCun.2015; @schmidhuber2015deep; @james2013introduction]. The choice of the number of hidden layers in a DNN should be informed by the complexity of the problem and computational resources available [@LeCun.2015]. While more hidden layers can allow the network to capture increasingly complex functional forms, they also come with a higher computational cost [@LeCun.2015]. In general, three hidden layers are considered sufficient for capturing almost any functional form in problems outside of image and speech recognition [@james2013introduction].

#### Deep Neural in Causal Inference

The popularity of using DNN in causal inference has increased in recent years due to its ability to uncover complex relationships in high-dimensional data with improved precision compared to traditional statistical and machine learning methods [@hernandez2019systematic; @LeCun.2015; @Schmidhuber.2015]. This capability to capture intricate relationships in large-volume data makes DNN a suitable algorithm for applications in propensity score analysis and other causal inference methods. DNN has garnered substantial attention in both industry and academic research, making substantial advancements in image recognition and natural language processing [@LeCun.2015; @james2013introduction]. The recent investment by Google in software development has made it easier to develop DNN models using high-level API's such as Keras [@Pang.2019].

To date, DNN have received limited attention in the broader literature on causal inference, with most research focused on medical or genomic applications [@Kale.2015; @Iglesias.2021; @LeCun.2015]. DNN has not yet been widely adopted in social science, likely due to a limited understanding of how to implement these algorithms [@Farrell.2021]. As high-dimensional datasets become more prevalent in social science research, it is expected that the use of DNN will become more widespread and become a promising area of investigation in the future. Despite its potential, there has been limited research that incorporates DNN into propensity score analysis.

Only two medical peer-reviewed studies have applied DNN for propensity score estimation [@Whata.2022; @Weberpals.2021]. @Weberpals.2021 used data from around 130,000 cancer patients to construct a simulated dataset with 31 baseline covariates, where the treatment was a fictional cancer drug, and the outcome was a binary survival indicator. They employed a specific DNN architecture known as an "autoencoder" with three hidden layers and found that DNN reduced confounding bias in treatment effect compared to the traditional logistic model with manual variable selection. On the other hand, @Whata.2022 compared the performance of logistic regression to DNN with four hidden layers through a simulation (with 15 covariates) and a real-world application. In their simulation, they varied the level of complexity in the population treatment model but not the outcome model. The results showed that DNN outperformed logistic regression regarding covariate balance, classification accuracy, and absolute relative bias.

Both studies demonstrate the growing interest in using DNN for causal inference and suggest that DNN may be a valid approach for estimating the propensity score. My dissertation builds upon these studies by comparing DNN to traditional logistic regression and established machine learning methods in the propensity score simulation literature. Additionally, my study will test the complexity of both the outcome and treatment models, making it the first to examine the use of DNN for propensity score estimation outside of medical research and with high-dimensional data that resembles high-dimensional administrative datasets encountered in educational research.

\newpage

## Method

The focus of my first dissertation study is a statistical simulation aimed at evaluating the performance of several modern machine learning techniques, including classification and regression tree (CART), bagged CART, random forest, and a single-layer neural network. These techniques will be compared to the traditional logistic regression and my proposed deep neural network (DNN) based approach in estimating the propensity score for the recovery of the average treatment effect (ATE) using propensity score weighting. I will specifically examine the impact of different levels of data dimensionality (i.e., low vs. high-dimensional context) and misspecifications in both the population treatment and outcome models on the estimate of the ATE and covariate balance.

My study will use a Monte Carlo simulation design informed by previous propensity score estimation studies conducted by Setoguchi et al. (2008), Lee et al. (2009), and Cannas and Arpino (2019). My simulation code will be built upon Cannas and Arpino's publicly available code from their 2019 publication and will follow best practices in statistical simulation as outlined by Morris et al. (2018). In the following sections, I will detail the simulation design for this study, including the data generation process, estimands of interest, specific method specifications, and performance measures. Appendix X includes CODE.

### Data-Generation Mechanisms

The data generation mechanism for my simulation was informed by the evaluation I conducted as part of my second dissertation study, which involved a large-scale evaluation of an AI chatbot intervention using high-dimensional administrative data. I based the coefficient values of the covariates and error variances on that data. By basing my simulation on real-world data, I aim to address the limitations of statistical simulations that may not generalize well to actual datasets (CITE).

#### Covariates

I took a programmatic approach to generate correlated covariates, $(X_1,X_2,X_3,\dots,X_p)$. Specifically, I simulated three covariate conditions with varying numbers of covariates generated, with $p = 20, 100, 200$. The 20 covariate condition was considered representative of the average number of covariates used in typical propensity score analyses in educational research (Ansong, Wu, & Chowa, 2015; Cho et al., 2012; Monlezun et al., 2015; Morgan et al., 2015; Sullivan & Field, 2013). The remaining two conditions, with $p = 100$ and $p = 200$, were chosen as they represent high-dimensional datasets commonly encountered in education research (CITE HILL).

Much of the literature on propensity score simulation generates uncorrelated covariates [@Setoguchi.2008; @Stuart.2010; @ho2007matching; @Lee.2010]. However, this is not representative of the correlated data typically found in administrative data. To address this, all covariates were generated from random draws from a multivariate normal distribution with $\mu=0$ and $\sigma=1$, with a specified correlation matrix with correlations ranging from -0.3 to 0.3, using the mvrnorm function from the MASS package in R (CITE). This approach was based on the correlations found in the Common App data.

In order to preserve the correlation structure, I used the inverse cumulative distribution function (CDF) approach to convert the initial random pull of covariates from the multivariate normal distribution to three distributions: a normal distribution with a mean of 0 and standard deviation of 1, a uniform distribution with a range of 0 to 1, and a Bernoulli distribution with a probability of success set at 0.5. This approach aims to maintain the same correlation structure, although some correlations may be slightly suppressed, but should be equal on average across the simulation. The covariates were constructed such that half affected both treatment and outcome (i.e., confounders), a quarter affected treatment only, and the remaining quarter affected the outcome only.

#### Sample Size

I used a fixed sample size of 10,000 for all conditions. This sample size is larger than what is typically used in many propensity score simulation studies (n = 500; 2,000) [@Setoguchi.2008; @Stuart.2010; @ho2007matching; @Lee.2010]. The choice of this sample size is motivated by my goal to mimic real-world high-dimensional administrative data.

#### Population Treatment Assignment Models

I generated the population treatment assignment models using standard logistic regression. These models can also be referred to as population propensity score models, as they generate the true probability of being assigned to treatment. The following logistic regression model represents the general structure of these models.

$$
Pr(Z = 1) = (1 + exp(-(\beta_0 + \sum_{i=1}^{p} \beta_i X_i)))^{-1}
$$ {#eq-popps}

$X_i$ represents $p$ covariates and their associated coefficients ($\beta_i$). The output, $Pr(Z = 1)$ is the probability of assignment to the treatment condition (i.e., the propensity score, $e(x)$). The intercept, $\beta_0$, was set to 0.25, and the remaining coefficients, $\beta_i$, were randomly assigned values between -0.4 and 0.4.

I generated two types of population treatment assignment models, Base(T) and Complex(T). To add complexity to these models, I included interactions (non-additivity) and higher-order terms (non-linearity) for the confounders and covariates related to treatment.

**Base(T)** - includes only main effects, which assumes a linear relationship between covariates and treatment assignment. I consider this the base model, which is expressed using the following formula:

$$
PS^{Base} = (1 + exp(-(\beta_0 + \sum_{i=1}^{p} \beta_iX_i)))^{-1}
$$ {#eq-psA}

$X$ represents $p$ covariates and their associated coefficients $\beta$. The output is the probability of assignment to the treatment condition (i.e., propensity score).

**Complex(T)** - is the most complex population treatment assignment model in my simulation. To construct it, I incorporated quadratic terms for half of the covariates that are both confounders and related to treatment, as well as interactions with another half of the same group of covariates. The aim of this design is to account for complex relationships between covariates and treatment assignment.

$$
PS^{Complex} = PS^{Base} + \beta_iX_i^2 + \beta_iX_iX_{i+1}
$$ {#eq-psD}

Using the propensity scores generated from these population treatment models I generated a binary treatment assignment variable by comparing an individual's true propensity score to a random draw from a uniform distribution between 0 and 1. (CITE) If the propensity score of an individual was higher than the random draw of the uniform distribution, the individual was assigned to treatment such that $Z = 1$, otherwise $Z = 0$. This approach should approximate a probability of assignment to treatment of 0.5, which is the probability of assignment to a randomized experiment with binary treatment.

#### Population Outcome Models

In addition to the population treatment assignment models, I also generated two population outcome models. To vary the level of complexity between the covariates and the outcome, I included interactions and higher-order terms on covariates related to outcome and overall confounders. This approach of adding complexities to the outcome model is relatively novel in the propensity score literature, but it is more representative of the complex associations found in datasets in educational research.

The continuous outcome, $Y$, was generated through regression models. The values of $\alpha$ were based on the Common App evaluation, such that the intercept $\alpha_0$ equaled -0.18 and the remaining $\alpha$ coefficients received a random value between -0.2 and 0.3. $X_i$ represents $p$ covariates and their associated coefficients ($\alpha_i$). The error term $\epsilon$ was set to have a mean of 0 and variance of 0.17. The binary treatment variable $Z$ is multiplied by $\gamma$, which represents the population treatment effect and was set to 0.3. This treatment effect is consistent with the small treatment effects typically observed in education research, where the average effect size is equal to 0.28 SD @richardson2011eta.

The general structure of the population outcome models is defined as follows:

$$
Y = \alpha_0 + \gamma Z + \sum_{i=1}^{p} \alpha_iX_i + \epsilon
$$ {#eq-outcome}

**Base(Y)** - the base model - includes only main effects, which assumes a linear relationship between covariates and outcomes. I consider this the base model, which is expressed using the following formula:

$$
Y^{Base} = \alpha_0 + \gamma Z + \sum_{i=1}^{p} \alpha_iX_i + \epsilon
$$ {#eq-outcome_a}

**Complex(Y)** - describes the complex model which includes both non-linearities and non-additivities which were generated in an identical way to how they were generated in the population treatment outcome models:

$$
Y^{Complex} =  Y^{Base} + \alpha_iX_i^2 + \alpha_iX_iX_{i+1}
$$ {#eq-outcome_d}

#### Propensity Score Estimation Methods

Using the simulated data, I estimate propensity scores using standard logistic regression and popular machine learning techniques commonly used in the propensity score estimation literature. This includes classification and regression tree (CART), bagged CART, random forest, and a single-layer neural network. In all cases, the propensity scores are estimated using all $p$ covariates. This scenario is unique in that all confounders are included in the propensity score estimation, which is not typically the case in practice. However, given my interest in evaluating the ability of these methods to recover the true unbiased treatment effect, this approach is well-suited. Future research should examine the performance of these methods when a subset of confounders is omitted from the propensity score estimation model.

Here I present the specification of each method used to estimate propensity scores:

-   **Logistic regression**: standard logistic regression with only main effects, using the R *glm* command.

-   **Classification and Regression Trees (CART)**: using the R *rpart* package (Therneau & Atkinson, 2018) with default parameters and recursive partitioning.

-   **Bagged CART**: bootstrapped aggregated trees using the R *ipred* package (Peters, Hothorn, & Lausen, 2002). I changed the default parameter to use 100 boostrap replicated to align with Lee et. al XXX simulation.

-   **Random Forest**: parallel tree generation on subsamples based on randomly selected covariates using the R *randomForest* package with default parameters (Liaw & Wiener, 2002).

-   **Single-Layer Neural Network (NN)**: I used a simple three-layer NN that consisted of an input layer, one hidden layer, and an output layer. The input layer consisted of $p$ covariates, and the hidden layer was made up of hidden neurons equal to 1/3 of the input covariates, which is a common practice in neural network applications (CITE). I used a ReLU activation function in the hidden layer for computational efficiency and a sigmoid activation function in the output layer to obtain probabilities bounded between 0 and 1. The NN was trained and fit using the R keras package (Chollet et al., 2017), which interfaces with Python.

-   **Deep Neural Network (DNN-2)**: I used a four-layer deep neural network with an input layer, **two** hidden layers, and an output layer. The input layer consisted of $p$ covariates and each hidden layer was made up of hidden neurons equal to 1/3 of the input covariates. Similar to the NN, I used the ReLU activation function in the hidden layers, while the output layer consisted of a single neuron with a sigmoid activation function, which provides bounded probabilities between 0 and 1. I trained and fit the DNN-2 using the R keras package (Chollet et al., 2017).

-   **Deep Neural Network (DNN-3)**: The last neural architecture I tested was a five-layer deep neural network with an input layer, **three** hidden layers, and an output layer. The output layer was a single neuron with a sigmoid activation function to produce bounded probabilities between 0 and 1. The DNN-3 was also trained and fit using the R keras package (Chollet et al., 2017), which interfaces with Python.

#### Training Methodology for Neural Network Approaches (NN-1, DNN-2, DNN-3)

A major challenge in using neural network and machine learning algorithms is the need to specify values for hyperparameters. Hyperparameters are parameter values that are not learned from the data, but are set prior to training the model and have a significant impact on the model's performance. Finding the optimal values for these hyperparameters, also known as tuning, are typically found through cross-validation, which involves testing a range of values for the targeted parameters to determine the optimal parameters for the neural network. Choosing incorrect hyperparameters can result in widely varying outcomes.

Hyperparameter specification in neural networks is critical to their performance. The hyperparameters include the number of hidden layers, the number of hidden neurons, the learning rate, and weight decay. The learning rate determines the speed at which the weights are updated during training, while weight decay regulates the weights to prevent overfitting. For this simulation, the number of hidden neurons, activation functions, and loss function were selected based on established norms in the field.

To optimize the performance of the model, various techniques for hyperparameter tuning have been developed, such as k-fold cross-validation and grid search. However, in this simulation, the ADAM optimizer was utilized. This optimizer eliminates the need for hand-tuning the learning rate and is a widely used algorithm that combines the benefits of other algorithms, adapting the learning rates of each parameter based on historical gradient information, making it more efficient. The ADAM optimizer uses multiple training runs of the data to determine the optimal values of the weights and bias terms in the neural network, making it a computationally efficient process without requiring researcher intervention.

The neural network approaches I tested (NN-1, DNN-2, DNN-3) were trained using a standard 80/20 split, with 80% of the sample used for training and 20% for validation. The ADAM optimizer was employed to automatically find the optimal learning rate that minimizes the binary cross-entropy loss function, which is a standard in binary classification problems and measures the difference between predicted and actual probabilities. Early stopping was used to stop the training process when the loss function values did not change after 5 training runs, and L2 regularization with a value of 0.1 was applied to penalize large weights. This minimized the changes that the neural networks overfit the data. 

ReLU activation was used in the hidden layers for computational efficiency, and a sigmoid activation function was used in the output layer to generate probabilities between 0 and 1. The training was conducted over 100 epochs with a batch size of 64. The choice of hyperparameters was consistent across all three methods to ensure a fair comparison of their performance. The use of the ADAM optimizer, early stopping, and L2 regularization all contributed to the efficient training of the neural networks and the optimization of their performance. The hyperparameter values for each of the nn condition, dnn-2, and dnn-3 methods are presented in Table X.

### Estimating Treatment Effects - Propensity Score Weighting

To estimate the treatment effect, I used a propensity score weighting approach, specifically the inverse probability of treatment weighting (IPTW) [@Lee.2010]. This method involves reweighting the observations based on their estimated propensity score, so as to achieve covariate balance between the treated and untreated units. The weight for each treated observation was equal to $1/e_i(x)$, and for untreated units, the weight was $1/(1-e_i(x)$. The estimated propensity score, $e_i(x)$, was determined for each unit $i$. This weighting scheme was used to estimate the average treatment effect (ATE).

I used the R *survey* package to calculate the Average Treatment Effect (ATE). The outcome variable ($Y$) was predicted using the binary treatment assignment variable, with the ATE weights included. The ATE point estimate, as well as the robust standard error of the ATE, were saved.

#### Estimating Standard Error of the ATE

The proper way to estimate standard errors when using IPTW weights calculated from propensity scores and targeting the Average Treatment Effect (ATE) remains a topic of debate in the literature. While some researchers support the use of simple methods, such as the standard deviation of the weighted outcome (@Rosenbaum.1983), others argue that more advanced methods, such as bootstrapping (Reifeis and Hudges; 2020; Chan, Yam, and Zhang 2016), are necessary to account for the bias introduced by the weighting process.

Studies that have used propensity score weighting to estimate treatment effects have shown that the choice of standard error estimator can have a significant impact on the results. For example, [@Stuart.2010] found that different standard error estimators can lead to vastly different conclusions about the presence of treatment effects, while [@Cattaneo.2015] found that using a bootstrap estimator led to more accurate confidence intervals compared to using the standard deviation of the weighted outcome.

Given these debates, it is important to choose an appropriate standard error estimator based on the specific goals of the analysis and the trade-off between computational feasibility and statistical accuracy. To account for the uncertainty in the estimated propensity score, I used the "robust" standard errors calculated from the R survey package, as recommended by (Robins, Hernn, and Brumback 2000; Hmlinen, 2012).

### Performance Metrics

To gauge the performance of each propensity score estimation method, I used the following metrics, which are widely used to access model performance in the propensity score simulation literature [@Setoguchi.2008; @Cannas.2019; @Stuart.2010; @Lee.2010; @McCaffrey.2004]:

-   **Bias**: The difference between the estimated treatment effect and the true population treatment effect of 0.3. Bias values show how far off the estimated ATE is from the true population treatment effect and allow us to determine if we overestimate or underestimate the true ATE.

-   **Relative Bias (Bias)**: the difference between the estimated treatment effect and the population treatment effect, divided by the true population treatment effect of 0.3. A low relative bias value indicates that the estimated treatment effect is close to the true population treatment effect, while a high relative bias value indicates that the estimated treatment effect deviates significantly from the true ATE.

-   **ATE Standard Error**: is simply the standard error associated with the estimated ATE.

-   **Mean Squared Error (MSE)**: the average of the squared differences between the estimated treatment effect and the population treatment effect. MSE is a good representation of both bias and variance and a common performance metric in the machine learning literature [@Athey.2015; @Mullainathan.2017].

-   **95% Confidence Interval Coverage**: indicator of wether the population treatment effect of 0.3 is found within the estimate 95% confidence interval.

-   **Average Standardized Absolute Mean Distance (ASAM)**: is a measure of of covariate balance. After calculating the ATE, the absolute value of the standardized difference of means between the treated and untreated group was calculated for each covariate and averaged across all covariates. Lower ASAM values indicate better covariate balance.

-   **Power**: indicator of whether the p-value associated with the estimated ATE is lower than an alpha level of 0.05. This metric helps determine if the estimated ATE is statistically significant and provides evidence of a treatment effect.

-   **Weights**: a known issue with PSW is that extreme weights can result in biased treatment effects. Therefore I examine the distribution of the estimated weights.

### Simulation Scenarios

In total, I tested three different covariate conditions (p = 20, 100, 200) with two treatment models (Base(T) and Complex(T)) and two outcome models (Base(Y) and Complex(Y)). I  evaluated seven propensity score estimation methods including logit, CART, BAG, forest, NN-1, DNN-2, and DNN-3. The sample size was fixed at 10,000. By fully crossing all these conditions, I tested a total of 84 scenarios, each of which was simulated 1,000 times.

### Software

The simulations were performed on the University of Pittsburgh Center for Research Computing cluster, utilizing high-memory CPU and GPU nodes to handle the complexity and high-dimensional data generated. Both R (version 4.1.0) and Python (version 3.7) were used, along with popular deep learning libraries Keras and Tensorflow.
\newpage

## Results

This section presents the results of my Monte Carlo simulation study, which compares the performance of my DNN-based approach, traditional logistic regression, and other commonly used machine learning-based approaches, including CART, bagged CART, and random forest, in estimating the average treatment effect (ATE) through propensity score weighting. The simulation aims to evaluate how accurately these methods estimate the population ATE (ATE = 0.3) under different levels of covariate conditions (p = 20, 100, 200) and when complexities are introduced into the treatment and outcome models.

The results are organized into three main parts. First, I present the results on the bias in the ATE estimation. Next, I evaluate the variability of the ATE estimation by examining the standard error (SE), mean squared error (MSE), and 95% confidence interval coverage. Then, I assess covariate balance through the average standardized absolute mean difference (ASMD). Finally, I discuss the mean IPTW weights and statistical power.

### Bias of ATE Estimation

#### Bias

I start by summarizing the bias in the average treatment effect (ATE) across the population treatment and outcome model conditions to understand the average performance of the methods under different conditions. Figure @fig-bias displays the results for the bias of the ATE by method and covariate conditions (p = 20, 100, 200). The "true" ATE is set to 0.3, so a bias of 0 means that the method has accurately estimated the true ATE without any bias. If the bar is above 0, the method overestimates the true ATE, and if it is below 0, it underestimates the ATE.

```{r}
#| label: fig-bias
#| fig-cap: "Bias of the ATE"

over_df %>%
  ggplot(aes(x = method, y = Bias, fill = as.factor(p))) +
  ylab("Bias") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  coord_cartesian(ylim=c(-.05,.05)) +
  theme_Publication() +
  scale_fill_Publication() +
  geom_hline(yintercept = -.05, color = "white", size = 2)
```

Across methods and covariate conditions, there is much variability in terms of bias. In the low-dimensional condition (p = 20), all methods accurately estimate the true ATE with relatively little bias, except for bagged CART, which substantially underestimates the true ATE. As the number of covariates increases to 100, the magnitude of bias also increases, with logistic regression overestimating the ATE, while all other machine learning approaches estimate the ATE with less bias.

The results become more pronounced in the highest covariate condition (p = 200) compared to the low-dimensional condition (p = 20). Logistic regression severely overestimates the true ATE, while the machine learning methods estimate the true ATE with significantly less bias. A notable exception is nn-1, which in the high covariate scenario severely underestimates the ATE, albeit in the opposite direction, with a larger magnitude than logistic regression.

#### Absolute Percent Bias

In addition to the raw bias, I also evaluated the absolute percent bias, which is displayed in Figure @fig-abs-bias. An absolute percent bias of 0 indicates that there is no bias in the estimated ATE. It's worth noting that with absolute percent bias, we are disregarding the direction of the bias for easier visual representation.

```{r}
#| label: fig-abs-bias
#| fig-cap: "Absolute bias (%) of the ATE"

over_df %>%
  ggplot(aes(x = method, y = Rel_Bias, fill = as.factor(p))) +
  ylab("Relative Bias") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  coord_cartesian(ylim=c(0,10)) +
  theme_Publication() +
  scale_fill_Publication() +
  geom_hline(yintercept = 10, color = "white", size = 2)

over_df %>%
  ggplot(aes(x = method, y = Per_Rel_Bias, fill = as.factor(p))) +
  ylab("Relative Bias") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  theme_Publication() +
  scale_fill_Publication() 
```

In the low-dimensional condition (p = 20), nearly all methods accurately estimate the ATE with little bias. Logistic regression has the least amount of absolute bias, while the DNN and NN approaches are relatively close, and all other approaches produce less than 2% absolute bias. The exception is bagged CART, which has a large amount of bias (absolute bias = 11.3%). As covariates increase from 20 to 100 and 200, logistic regression becomes substantially more biased than all other methods. All machine learning approaches perform well in the 100 covariate condition, with all producing bias below 2.5%. In this scenario, the lowest bias estimates are produced by my DNN approach, specifically DNN-2 and nn-1 (absolute bias = 0.43% and 0.21%, respectively). In the highest dimensional covariate condition (p = 200), all methods have an increase in bias. However, the machine learning approaches perform exceptionally well compared to logistic regression. Bagged CART produces the least biased estimate (absolute bias = 1.02%), followed by my DNN-2 approach (absolute bias = 1.33%), although the difference in magnitude is relatively small. An interesting finding is that the nn-1 approach produces the most biased results, even more so than logistic regression (23.5%, and 5.18%, respectively).

##### Complexities in the Population Outcome and Treatment Models

Next, I evaluate the absolute bias performance of each method by complexities introduced into the treatment and outcome model. Recall, that the data generating models *Base T* and *Base Y* include only main effects in the treatment and outcome model, while *Complex T* and *Complex Y* include non-addivity and non-linearities in the form quadratic and interaction terms among the covariates.

@fig-abs-bias-cond presents a faceted figure where the upper-left facet represents the condition where the population treatment and outcome models only include main effects (i.e., Base T & Base Y). The lower-right facet represents the condition where the population treatment and outcome models include quadratic and interaction terms (i.e., Complex T & Complex Y). Deviations from the diagonal facets indicate conditions where only one of the models includes quadratic and interaction terms, while the other only includes main effects.

```{r}
#| label: fig-abs-bias-cond
#| fig-cap: "Absolute bias (%) of the ATE by data generation mechanisms"
#| fig-width: 7.5
#| fig-height: 5

res_sum_df %>%
  ggplot(aes(x = method, y = Abs_Per_Bias, fill = as.factor(p))) +
  ylab("Absolute Bias (%)") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  facet_grid(scenarioT ~ scenarioY, scales = "fixed") +
  coord_cartesian(ylim=c(0,10)) +
  theme_Publication() +
  scale_fill_Publication() +
  theme(panel.spacing = unit(.5, "cm")) +
  geom_hline(yintercept = 10, color = "white", size = 2)
```

###### Base - Population Outcome and Treatment Models

When complexities are not included in the treatment or outcome models, nearly all methods perform well, regardless of the covariate conditions, exhibiting an absolute bias of less than 2.5%. The only exception is bagged CART, which produced significant bias with 20 covariates (absolute bias = 18.3%). Logistic regression produces the least biased estimates (absolute bias = 0.01%) when both the population treatment and outcome models include only main effects, which is expected as when estimating the propensity score using the various methods, only main effects are included in the model. Thus, in the Base Y and Base T condition, the population treatment model is equal to the propensity score estimation model. As the number of covariates increases, all methods perform exceptionally well, with my DNN-3 approach producing the least biased estimate (absolute bias = 0.11%).

###### Complex - Population Outcome and Treatment Models

In contrast to the models where only main effects are included in the population treatment and outcome models, the lower-right facet of @fig-abs-bias-cond displays the results when both the population treatment and outcome models include quadratic and interaction terms. When complexities are included in the population models, there is much more variability across methods and covariate conditions. All methods perform well when only 20 covariates are included, with absolute bias below 1.5%, except for bagged CART.

However, as the number of covariates increases, there is a significant increase in absolute bias. With 100 covariates, all methods perform well, with absolute bias below 1.5%, with bagged CART and nn-1 producing the least biased estimates (absolute bias = 0.08% and 0.13%, respectively). The differences become more pronounced in the high covariate condition (p = 200). Specifically, logistic and nn-1 produce the largest biased ATE estimates (absolute bias = 16.6% and 87.9%, respectively). The least biased estimates are produced by my DNN-2 approach (absolute bias = 2.67%), which is far lower than the absolute bias produced by all other machine learning approaches.

###### Deviations - Population Outcome and Treatment Models

In @fig-abs-bias-cond, deviations from the treatment and outcome model are indicated by points off the diagonal in the facets. The lower-left facet displays the results when the treatment model is complex, and the outcome model only has main effects, while the upper-right facet displays the results when the outcome model is complex, but the treatment model only includes main effects. In general, bias performance is sensitive to complexities in the outcome model or when complexities exist in both the outcome and treatment models, specifically in the highest covariate condition.

##### Summary

My findings suggest variability in the bias of the estimated ATE across methods and covariate conditions. In low-dimensional condition (p=20), most methods accurately estimate the true ATE with little bias. As the number of covariates increases, the magnitude of bias also increases, with logistic regression overestimating the ATE, while all other machine learning approaches produce less biased estimates. In the highest covariate condition (p=200), logistic regression severely overestimates the true ATE, while the machine learning methods estimate the true ATE with significantly less bias. Notably, my DNN-2 approach is a robust method for estimating the ATE in the high-dimensional covariate setting, especially when complexities are present in the population treatment and outcome model.

### ATE Estimation: SE, MSE, and 95% CI Coverage

Although bias in the average treatment effect (ATE) is an important metric to evaluate the performance of the methods, it is equally important to consider the sampling variability of the estimated ATE. The optimal method should have low bias in the estimated ATE and exhibit minimal variability through tight standard errors (SE) and mean squared errors (MSE) and high 95% confidence interval coverage. These properties enhance the precision of the estimated ATE. In this section, I summarize the findings of the estimated ATE in terms of standard error (SE), mean squared error (MSE), and 95% confidence interval coverage rate.

@fig-ate-var is a multi-panel figure that compares the performance of each method in terms of SE and MSE of the ATE and 95% confidence interval coverage rate across population models.

```{r}
#| label: fig-ate-var
#| fig-cap: Variability in ATE
#| fig-subcap: 
#|   - "Standard Error (SE)"
#|   - "Mean Squared Error (MSE)"
#|   - "95% CI Coverage"
#| layout-ncol: 2

over_df %>%
  ggplot(aes(x = method, y = ATE_se, fill = as.factor(p))) +
  ylab("Standard Error (SE)") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  coord_cartesian(ylim=c(0,1)) +
  theme_Publication() +
  scale_fill_Publication() +
  geom_hline(yintercept = 1, color = "white", size = 2)

over_df %>%
  ggplot(aes(x = method, y = MSE, fill = as.factor(p))) +
  ylab("Mean Squared Error (MSE)") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  coord_cartesian(ylim=c(0,4)) +
  theme_Publication() +
  scale_fill_Publication() +
  geom_hline(yintercept = 4, color = "white", size = 2)


over_df %>%
  ggplot(aes(x = method, y = coverage_95, fill = as.factor(p))) +
  ylab("95% C.I. Coverage") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  coord_cartesian(ylim=c(0,1)) +
  theme_Publication() +
  scale_fill_Publication() +
  geom_hline(yintercept = .95, linetype = 2, color = "black", size = .5) 
```

#### Standard Error (SE) and Mean Squared Error

The first panel (@fig-ate-var-1) displays the results for standard error of the estimated ATE by covariate condition. All methods produce comparable SE in the low dimensional condition (p = 20). As the number of covariates increases, logistic regression and nn-1 produce very large standard errors, while the other machine learnig approaches smaller SE, with cart and forest having the smallest standard errors (SE = 0.17 and 0.16, respectively).

In addition to the standard error (SE) of the estimated ATE, I also evaluated the mean squared error (MSE) of the ATE (@fig-ate-var-2). MSE measures the average squared difference between the estimated and true values, where a lower MSE indicates a better fit between the two. MSE provides a summary statistic that incorporates both bias and variance of the ATE. All methods perform well in the 20 covariate condition, producing small SE values. However, in the highest covariate condition (p = 200), logistic regression and nn-1 produce large MSE values, while all other machine learning approaches perform well with MSE values below 2. Out of all the machine learning approaches, my DNN-2 approach performs the best, with the lowest MSE value (MSE = 0.87), which is 35 times lower than the MSE for logistic regression (MSE = 29.4).

##### Complexities in the Population Outcome and Treatment Models

As with the previous ATE bias results, next, I evaluate the SE and MSE performance of each method by complexities introduced into the treatment and outcome model.

@fig-ate-var-cond further delves into the performance of each method by evaluating SE, MSE, and 95% CI coverage in the presence of complexities in the treatment and outcome models. The figure is divided into three panels, each representing the SE, MSE, or 95% CI coverage rate. Within each panel, the upper-left facet represents the condition where only main effects are included in the population treatment and outcome models (Base T & Base Y), while the lower-right facet represents the condition where both the population treatment and outcome models include quadratic and interaction terms (Complex T & Complex Y). The deviations from the diagonal facets indicate conditions where only one of the models includes quadratic and interaction terms while the other only includes main effects.

```{r}
#| label: fig-ate-var-cond
#| fig-cap: Variability in ATE by data generating mechanisms
#| fig-subcap: 
#|   - "Standard Error (SE)"
#|   - "Mean Squared Error (MSE)"
#|   - "95% CI Coverage"
#| layout-ncol: 2
#| fig-width: 7.5
#| fig-height: 5

res_sum_df %>%
  ggplot(aes(x = method, y = ATE_se, fill = as.factor(p))) +
  ylab("ATE SE") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  facet_grid(scenarioT ~ scenarioY, scales = "fixed") +
  coord_cartesian(ylim=c(0,1)) +
  theme_Publication() +
  scale_fill_Publication() +
  theme(panel.spacing = unit(.5, "cm")) +
  geom_hline(yintercept = 1, color = "white", size = 2)

res_sum_df %>%
  ggplot(aes(x = method, y = MSE, fill = as.factor(p))) +
  ylab("MSE") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  facet_grid(scenarioT ~ scenarioY, scales = "fixed") +
  coord_cartesian(ylim=c(0,4)) +
  theme_Publication() +
  scale_fill_Publication() +
  theme(panel.spacing = unit(.5, "cm")) +
  geom_hline(yintercept = 4, color = "white", size = 2)

res_sum_df %>%
  ggplot(aes(x = method, y = coverage_95, fill = as.factor(p))) +
  ylab("95% CI Coverage") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  facet_grid(scenarioT ~ scenarioY, scales = "fixed") +
  coord_cartesian(ylim=c(0,1)) +
  theme_Publication() +
  scale_fill_Publication() +
  theme(panel.spacing = unit(.5, "cm")) 
```

###### Base - Population Outcome and Treatment Models

The upper-left panels in @fig-ate-var-cond-1 and @fig-ate-var-cond-2 show the results for the SE and MSE of the ATE, when only main effects are included in the population treatment and outcome models. When only main effects are included all methods, regarless of covariate condition, perform well producing low SE and MSE values.

###### Complex - Population Outcome and Treatment Models

However, when both the treatment and outcome models include quadratic and interaction terms (lower-right panels in @fig-ate-var-cond-1 and @fig-ate-var-cond-2), a drastic increase in SE and MSE values is observed. Focusing on the MSE results, as the number of covariates increases to 100, logistic regression, CART, random forest, and nn-1 exhibit high SE and MSE values. Logistic regression has the highest MSE value (MSE = 1.05). In the highest dimensional case with 200 covariates, logistic regression leads to exceptionally high SE and MSE values (SE = 3.24; MSE = 115.1) compared to all other machine learning approaches, except for nn-1. This contrasts logistic regression's low SE and MSE values in the base condition (SE = 0.013; MSE = 0.00008). On the other hand, DNN-2 and DNN-3 have significantly lower MSE values than all other methods, with MSE values below 3, which is nearly 40 times lower than the MSE value for logistic regression in the high dimensional condition.

###### Deviations - Population Outcome and Treatment Models

As with the ATE bias evaluation results, both SE and MSE appear sensitive to complexities in the outcome model, more so than when complexities are only included in the treatment model. When complexities are only included in the outcome model, bagging, DNN-2, DNN-3, and logistic regression are robust to covariate conditions, with MSE values below 1. In contrast, nn-1 has the highest MSE, followed by bagged cart and random forest.

#### 95% Confidence Interval Coverage

Finally, in the third panel (@fig-ate-var-3), I present the results for the 95% confidence interval (CI) coverage rate of the ATE, indicated by the dashed line which represents the desired coverage rate. The 95% CI coverage rate assesses how well each method captures the true ATE of 0.3 within the estimated 95% confidence interval. A good estimator should have high coverage, meaning that the true population ATE is reliably captured within the interval. The machine learning approaches appear relatively insensitive to the different covariate conditions. However, both logistic regression and nn-1 seem more sensitive, showing a decrease in coverage rate as the number of covariates increases. Overall, all methods exhibit adequate coverage, except for bagged CART in the 20 covariate condition, and CART and random forest which exhibit lower coverage rates across all covariate conditions.

##### Complexities in the Population Outcome and Treatment Models

The third panel of @fig-ate-var-cond shows the 95% CI coverage rate results by model complexities. Overall, coverage rates appear particularly sensitive to complexities in the population treatment model, with all methods displaying lower coverage rates than when only main effects are included in the population treatment model. In addition, we observe a fairly consistent pattern that the neural network-based approach (nn-1, dnn-2, dnn-3) and logistic regression produce lower coverage rates as covariates increase.

##### Summary

All methods produced ATE with low SE and MSE values in the low-dimensional condition. However, as the number of covariates increased, the DNN approaches (dnn-2 and dnn-3) outperformed traditional logistic regression and other methods, producing the lowest MSE values. These findings suggest that the DNN approaches are reliable and consistent for ATE estimation in high-dimensional contexts, while also emphasizing the sensitivity of the estimates to complexities in the population outcome and treatment models. Bagging also performed well, with sound 95% confidence interval coverage and low MSE, making it another robust method in high-dimensional contexts. Logistic regression was effective in low-dimensional contexts, but its effectiveness decreased with an increase in dimensionality. The results emphasize the importance of considering both the bias and variability of estimated ATE when choosing the optimal estimation method.

### Covariate Balance

Up to this point, I have considered performance metrics that are not directly observable in practice. For example, a researcher cannot assess the degree of bias introduced by each propensity score estimation method within their observational dataset. A commonly used and directly observable metric for deciding which method to use is covariate balance. Covariate balance is typically assessed using the average standardized absolute mean difference (ASAM) between treated and untreated units. Maintaining covariate balance reduces the risk of bias in the estimated treatment effect and leads to more accurate results. However, the use of average standardized absolute mean difference as the metric for assessing balance is contested, with some researchers suggesting that other moments besides the mean should be considered. In my simulation, I focused exclusively on assessing balance through ASAM. For simplicity, I took the average ASAM for all covariates in the propensity score estimation model, but in practice, each covariate should be assessed separately. A mean ASAM value of 0 indicates balance in the mean between treated and untreated units across all covariates. I consider two commonly used thresholds for deciding whether adequate covariate balance has been achieved: the more typical 0.2 threshold proposed by (CITE), and the more stringent threshold of 0.1.

The results of the ASAM evaluation averaged across all population outcome models can be seen in Figure @fig-asam, where the thicker dashed line indicates an ASAM value of 0.2, and the thinner dashed line indicates an ASAM value of 0.1. Overall, all methods achieved adequate covariate balance in the low-dimensional condition with ASAM values below 0.2. However, using the more stringent cutoff of 0.1, both CART and random forest failed to achieve covariate balance. The lowest ASAM value achieved in the 20 covariate condition was by logistic regression with an ASAM value of XXX. As the covariate dimensionality increased, logistic regression produced extreme imbalance (ASAM = 74.9), while all other machine learning approaches maintained good covariate balance with ASAM values at or below the most stringent cutoff of 0.1. The lowest levels of covariate balance were achieved by bagged CART and my DNN approaches, with the DNN-2 approach producing the best balance with an ASAM value of 0.06.

```{r}
#| label: fig-asam
#| fig-cap: "Average Standardized Absolute Mean Difference (ASAM)"

over_df %>%
  ggplot(aes(x = method, y = ASAM, fill = as.factor(p))) +
  ylab("ASAM") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  coord_cartesian(ylim=c(0,.5)) +
  theme_Publication() +
  scale_fill_Publication() +
  geom_hline(yintercept = .5, color = "white", size = 2) +
  geom_hline(yintercept = .1, linetype = 2, color = "black", size = .25) +
  geom_hline(yintercept = .2, linetype = 2, color = "black", size = .5) 
```

###### Base - Population Outcome and Treatment Models

When the outcome and treatment models included only main effects, all methods produced good covariate balance using the 0.2 thresholds across all covariate conditions (as shown in the upper-left facet of @fig-asam-cond). With the more stringent 0.1 thresholds, bagging, the neural network-based approaches, and logistic regression produced optimal balance. In the base condition, the machine learning approaches appear insensitive to the number of covariates, while logistic regression and nn-1 seem sensitive to dimensionality.

```{r}
#| label: fig-asam-cond
#| fig-cap: "Average Standardized Absolute Mean Difference (ASAM) by data generating mechanisms"
#| fig-width: 7.5
#| fig-height: 5

res_sum_df %>%
  ggplot(aes(x = method, y = ASAM, fill = as.factor(p))) +
  ylab("ASAM") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  facet_grid(scenarioT ~ scenarioY, scales = "fixed") +
  coord_cartesian(ylim=c(0,.5)) +
  theme(panel.spacing = unit(.5, "cm")) +
  theme_Publication() +
  scale_fill_Publication() +
  geom_hline(yintercept = .5, color = "white", size = 2) +
  geom_hline(yintercept = .1, linetype = 2, color = "black", size = .25) +
  geom_hline(yintercept = .2, linetype = 2, color = "black", size = .5) 
```

###### Complex - Population Outcome and Treatment Models

When complexities are introduced into the treatment and outcome models, a slight increase in imbalance is observed, but nothing significant. In the low-covariate condition, all methods perform well with ASAM values below 0.2. With the more stringent threshold, CART and random forest fail to achieve covariate balance. In line with the overall ASAM results, logistic regression and nn-1 produced extreme imbalances among covariates in the higher covariate condition. For example, with 200 covariates, logistic regression produced extreme imbalances (ASAM = 74.94). In comparison, my DNN-2 approach achieved adequate covariate balance (ASAM = 0.061), and using the most stringent threshold, the DNN-2 and DNN-3 approaches were the only methods to achieve covariate balance.

#### Deviations from the Treatment and Outcome Models

Covariate balance is relatively robust to different specifications of covariate conditions and complexities in the outcome and treatment models. However, when dealing with higher covariates, specifically 100 and 200, logistic regression and nn-1 are sensitive to complexities in the treatment models, producing more imbalance in this context.

##### Summary

Most methods produced acceptable balance using a 0.2 threshold across all covariate levels, except for CART, Random Forest, and Logistic Regression. With the more stringent 0.1 thresholds, bagging, DNN-2, and DNN-3 performed better. In the highest covariate case, logistic regression produced extreme imbalance, while DNN-2 had the best balance with an ASAM value of 0.06. When considering complexities in both the outcome and treatment models, DNN-2 led to the best covariate balance among all methods, especially in high-dimensional cases, while logistic and nn-1 were sensitive to complexities in the treatment models. These results suggest that DNN-2 will lead to the best covariate balance in high-dimensional cases, especially when complexities are present.

### Weights Assessment

I estimated the average treatment effect (ATE) using inverse propensity score weighting (IPTW). To ensure unbiased treatment effects, it is important to avoid extreme IPTW weights, as large weights can result in bias (CITE). To evaluate this, I calculated the mean IPTW weight for each method, with a considerable departure from 1, indicating the presence of extreme weights.

@fig-weights shows the mean IPTW weights across treated and untreated units. All machine learning approaches resulted in acceptable weights below 2.5. All methods performed adequately well in the low-dimensional case with mean weights below 2. However, in the high-dimensional cases, logistic regression and nn-1 produced extreme weights. In the 200 covariate case, logistic regression produced extreme weights (IPTW = 225,868,800), as did nn-1 (IPTW = 130,406).

```{r}
#| label: fig-weights
#| fig-cap: "Mean IPTW weights"

over_df %>%
  ggplot(aes(x = method, y = PS_Weights, fill = as.factor(p))) +
  ylab("IPTW Weights") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  coord_cartesian(ylim=c(0,5)) +
  theme_Publication() +
  scale_fill_Publication() +
  geom_hline(yintercept = 5, color = "white", size = 2) 

```

##### Complexities in the Population Outcome and Treatment Models

When assesing the IPTW weight by the various complexities in the population treatment and outcome model, we find that all methods regardless of condition produce adequate weights below 2 (@fig-weights-cond). The mean values of the IPTW were consistent across all covariate conditions, indicating that the balance between the treatment and control groups was not affected by the presence of covariates. The only deviation from this result is logistic and nn-1, where they produce extreme weights whever complexities are introduced into the population treatment model, we don't see this affect when the base treatment model only includes main effects.

```{r}
#| label: fig-weights-cond
#| fig-cap: "IPTW Weights by data generating mechanisms"
#| fig-width: 7.5
#| fig-height: 5

res_sum_df %>%
  ggplot(aes(x = method, y = PS_Weights, fill = as.factor(p))) +
  ylab("IPTW Weights") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  facet_grid(scenarioT ~ scenarioY, scales = "fixed") +
  coord_cartesian(ylim=c(0,5)) +
  theme_Publication() +
  scale_fill_Publication() +
  geom_hline(yintercept = 5, color = "white", size = 2) +
  theme(panel.spacing = unit(.5, "cm")) 

```

##### Summary

Overall, all methods produced acceptable IPTW weights across covariate conditions, especially when complexities were not included in the population treatment or outcome model. However, with added complexities and a higher number of covariates, logistic regression, and nn-1 produced extreme weights, which could result in biased treatment effects. These results indicate that most machine learning approaches, particularly the DNN approach and Random Forest, would be good options for generating IPTW weights in high-dimensional covariate conditions. The sensitivity of the IPTW weights to complexities in the treatment and outcome models underscores the importance of considering these complexities when selecting a method for IPTW weighting to ensure accurate treatment effect estimates and minimize the risk of extreme weights.

### Power

Finally, I evaluated the statistical power of the methods. Statistical power refers to the probability of detecting a statistically significant effect at a specific alpha level, typically .05. Low power means that the method has a high rate of committing type II error and may miss detecting real differences in the treatment effect estimation. High power, on the other hand, indicates a low rate of type II error. However, it is important to note that factors such as sample size and the magnitude of the effect can impact statistical power. In this simulation, the sample size was fixed at 10,000, and the treatment effect was relatively small. Statistical power may not be the most relevant benchmark for this simulation and should not be generalized outside my specific assessment of an ATE = 0.3. The following is a brief summary of the results.

@fig-power shows the results of the statistical power evaluation, averaged across the population treatment and outcome models. The standard alpha level of 0.05 was used to assess statistical significance. We observe a consistent pattern across methods, wherein in the low-dimensional condition, all methods can correctly identify a significant treatment effect, while as the number of covariates increases, we see a decrease in power. However, this decrease in power is more pronounced for both logistic regression and nn-1, while the other machine learning methods have a less steep drop.

```{r}
#| label: fig-power
#| fig-cap: "Power"

over_df %>%
  ggplot(aes(x = method, y = Power, fill = as.factor(p))) +
  ylab("Power") +
  geom_bar(position = "dodge", stat = "identity") +
  labs(fill='Covariates') +
  coord_cartesian(ylim=c(0,1)) +
  theme_Publication() +
  scale_fill_Publication() 
```

\newpage

## Discussion and Conclusion

Quasi-experimental methods are commonly used in educational research to address the challenge of estimating treatment effects in the absence of randomization. One such method is propensity score weighting, which uses a propensity score model to balance observed covariates between treatment and control groups. However, estimating the propensity score model accurately can be challenging, particularly in high-dimensional data settings.

In this study, I evaluated the performance of various methods for estimating propensity scores, including logistic regression, tree-based machine learning approaches (CART, bagged CART, random forest), and neural network-based approaches (neural network, deep neural networks). The study also included a novel deep neural network-based approach for estimating propensity scores, which builds upon the literature on propensity score simulation (CITE).

The study evaluated the performance of the methods under different covariate conditions, ranging from 20 covariates to 200 covariates, and with varying levels of complexity in both the population treatment model and the population outcome model. The results showed that the choice of method for propensity score estimation should be based on the context and dimensionality of the data. For low-dimensional datasets, logistic regression produced good balance on covariates and unbiased ATE estimates, regardless of complexities in the population treatment or outcome model. Its simplicity in terms of ease of implementation, interpretability, and lower computational cost make it a preferred choice in these settings.

However, for high-dimensional datasets, machine learning methods, such as bagged CART and the proposed DNN approach, were found to be the optimal choice. These methods resulted in less bias and better covariate balance compared to traditional logistic regression, which produced extreme weights and led to inaccurate ATE estimates in high-dimensional settings. The DNN-based approach demonstrated superior bias reduction in estimating the ATE in a high-dimensional scenario, outperforming not only traditional logistic regression but also all other machine learning methods tested, including bagged CART.

These findings build upon previous simulation studies (CITE) that have demonstrated the superiority of machine learning approaches over traditional logistic regression in estimating propensity scores. However, this study extends the previous work by demonstrating the superiority of the DNN-based approach in high-dimensional settings, outperforming other machine learning methods, including bagged CART. The results of this study provide a valuable contribution to the field of causal inference and have implications for the design and analysis of observational studies in educational research.

In conclusion, the results suggest that machine learning methods, such as the proposed DNN approach, should be considered when dealing with high-dimensional data in educational research. These methods offer a flexible, data-driven approach for capturing complex interactions and non-linearities in the model, which may result in a more balanced covariate distribution and accurate estimates of treatment effects.

## Using other studies

In recent years, machine learning-based approaches have gained popularity in propensity score analysis, and several studies have compared the performance of different machine learning-based methods with traditional logistic regression. Our simulation study contributes to this growing literature by comparing the performance of traditional logistic regression, as well as other commonly used machine learning-based methods such as CART, bagged CART, random forest, and our novel deep neural network (DNN) approach.

The results of our study are in line with previous findings by Setoguchi et al. and Lee, who have shown that machine learning-based methods, such as CART and random forests, provide improved performance compared to logistic regression in terms of bias reduction and effect estimation. Our study extends these findings by including a comparison of DNN-based approaches with other machine learning-based methods and logistic regression. Our results suggest that DNN-2, in particular, is a robust method for estimating the average treatment effect (ATE) in high-dimensional covariate conditions, especially when complexities are present in the population treatment and outcome model. This finding supports the notion that deep neural networks can be useful in propensity score analysis and adds to the growing body of literature in this field.

Moreover, our study highlights the importance of considering both the bias and variability of the estimated ATE when choosing the optimal estimation method. Our results suggest that traditional logistic regression is effective in low-dimensional contexts, but its effectiveness decreases with an increase in dimensionality. In contrast, our DNN-2 approach is reliable and consistent for ATE estimation in high-dimensional contexts, producing the lowest mean squared error values. Our results also indicate that bagging is another robust method in high-dimensional contexts.

Additionally, our study sheds light on the issue of covariate balance in propensity score analysis. Our results suggest that all methods produced acceptable balance using a 0.2 threshold across all covariate levels, except for CART, random forest, and logistic regression. With the more stringent 0.1 threshold, bagging, DNN-2, and DNN-3 performed better. In the highest covariate case, logistic regression produced extreme imbalance, while DNN-2 had the best balance with an average standardized absolute mean difference (ASAM) value of 0.06. Our results suggest that DNN-2 will lead to the best covariate balance in high-dimensional cases, especially when complexities are present. These findings are in line with the findings of Keller, who noted that neural networks are promising for propensity score estimation because they algorithmically deal with non-linearities in the selection surface, making iterative respecification unnecessary.

In conclusion, our simulation study provides valuable insights into the performance of various machine learning-based approaches and logistic regression in propensity score analysis. Our results suggest that DNN-2 is a robust method for estimating the ATE in high-dimensional covariate conditions and highlights the importance of considering both the bias and variability of the estimated ATE when choosing the optimal estimation method. Our findings contribute to the growing body of literature in this field and provide valuable guidance for researchers and practitioners who are interested in using machine learning-based approaches in propensity score analysis.

# i like this one

In this study, we compared the performance of a deep neural network (DNN) approach, traditional logistic regression, and other machine learning-based approaches, including CART, bagged CART, and random forest, in estimating the average treatment effect (ATE) through propensity score weighting. Our study aimed to evaluate the accuracy of these methods in estimating the population ATE under different levels of covariate conditions and when complexities are introduced into the treatment and outcome models.

Our results showed that the magnitude of bias in the estimated ATE increases as the number of covariates increases. In the highest covariate condition (p=200), logistic regression severely overestimated the true ATE, while the machine learning methods, including the DNN-2 approach, produced the least biased estimates. Our findings also showed that the DNN approaches (DNN-2 and DNN-3) were reliable and consistent in estimating the ATE, producing the lowest mean squared error (MSE) values compared to other methods.

Our results are in line with other studies that have also found that traditional logistic regression may not be suitable for estimating ATE in high-dimensional covariate conditions (Collier et al., 2018; Hill et al., 2018). For example, a comparative simulation study by Collier et al. (2018) also found that traditional logistic regression performed poorly in high-dimensional covariate conditions, while neural networks produced more accurate ATE estimates. Additionally, Hill et al. (2018) found that traditional propensity score strategies may not be feasible in a high-dimensional setting, making machine learning approaches, such as DNNs, a promising alternative.

Furthermore, our results showed that the DNN-2 approach was a robust method for estimating the ATE in high-dimensional covariate conditions, especially when complexities were present in the population treatment and outcome models. This finding is also in line with a real-world data study by Weberpals et al. (2021), which found that deep learning-based propensity scores were effective in controlling confounding in comparative effectiveness research.

In terms of covariate balance, our results showed that DNN-2 had the best balance among all methods, especially in high-dimensional cases, while logistic regression produced extreme imbalance. This is consistent with the findings of Collier et al. (2018), who found that neural networks were more effective in producing balanced covariates compared to traditional methods.

In conclusion, our simulation study provides further evidence that traditional logistic regression may not be suitable for estimating ATE in high-dimensional covariate conditions, and that machine learning-based approaches, particularly DNNs, are a promising alternative. Our findings are in line with other studies and highlight the importance of considering both the bias and variability of the estimated ATE when choosing the optimal estimation method. Our results also demonstrate the robustness of the DNN-2 approach in estimating ATE in high-dimensional covariate conditions, particularly when complexities are present in the population treatment and outcome models.

#heres one with more literature The results of our Monte Carlo simulation study add to the growing body of literature exploring the use of artificial neural networks (ANNs) in propensity score analysis. Our study is particularly notable because it compares the performance of our deep neural network (DNN) approach with traditional logistic regression and other commonly used machine learning-based approaches, including CART, bagged CART, and random forest, in estimating the average treatment effect (ATE) through propensity score weighting. The study evaluates the accuracy of these methods in estimating the population ATE under different levels of covariate conditions (p = 20, 100, 200) and when complexities are introduced into the treatment and outcome models.

Our findings suggest variability in the bias of the estimated ATE across methods and covariate conditions. In low-dimensional condition (p=20), most methods accurately estimate the true ATE with little bias. As the number of covariates increases, the magnitude of bias also increases, with logistic regression overestimating the ATE, while all other machine learning approaches produce less biased estimates. In the highest covariate condition (p=200), logistic regression severely overestimates the true ATE, while the machine learning methods estimate the true ATE with significantly less bias. Notably, our DNN-2 approach is a robust method for estimating the ATE in the high-dimensional covariate setting, especially when complexities are present in the population treatment and outcome model.

Our results align with the findings of previous studies that have explored the use of ANNs in propensity score analysis. For example, in their comparative simulation study, Collier, Leite, and Zhang (2017) found that ANNs outperformed traditional logistic regression in estimating the ATE, especially in high-dimensional contexts. Similarly, Hill, Weiss, and Zhai (2020) found that ANNs were more effective than traditional logistic regression in controlling for confounding in high-dimensional settings. These findings suggest that ANNs may be a promising alternative to traditional logistic regression in propensity score analysis.

Our results also support the findings of Weberpals et al. (2021), who conducted a large-scale real-world data study and found that ANNs were effective in controlling for confounding in comparative effectiveness research. The authors found that ANNs were more accurate than traditional logistic regression in estimating the propensity scores, especially in high-dimensional contexts.

In conclusion, our study contributes to the growing literature on the use of ANNs in propensity score analysis by providing further evidence that ANNs may be a promising alternative to traditional logistic regression. Our findings suggest that ANNs are particularly effective in controlling for confounding in high-dimensional contexts and when complexities are present in the population treatment and outcome models. However, as Keller (2017) noted, the implementation of ANNs in propensity score analysis is not without challenges, including computational expenses and the potential for overfitting. Further research is needed to explore the potential of ANNs in propensity score analysis and to address these challenges.

The field of propensity score analysis has seen a growth in the application of machine learning techniques in recent years, with studies exploring the performance of various algorithms, including traditional logistic regression, CART, bagged CART, random forest, and neural networks. In this context, our simulation study aimed to compare the performance of our DNN-based approach, traditional logistic regression, and other commonly used machine learning-based approaches in estimating the average treatment effect (ATE) through propensity score weighting. The results of our study contribute to the existing literature by providing insights into the biases, variability, and covariate balance of the ATE estimates produced by these methods in high-dimensional covariate conditions.

Our results suggest that the DNN-based approaches, particularly DNN-2, are robust methods for estimating the ATE in high-dimensional contexts, with lower bias, lower variability, and better covariate balance than traditional logistic regression and other methods. This is in line with the findings of Collier et al. (2017) and Collier et al. (2018), who also noted the superiority of neural networks in high-dimensional settings. Our findings also support the results of Weberpals et al. (2021), who found that deep learning-based propensity scores outperformed traditional methods in a real-world study.

Our results also highlight the importance of considering the complexities in both the treatment and outcome models, as methods that performed well in low-dimensional settings may produce biased estimates in high-dimensional, complex settings. This is in agreement with the findings of Hill et al. (2017), who also emphasized the challenges of propensity score strategies in high-dimensional settings and the need for alternative methods that can handle these complexities.

Our study also highlights the importance of considering both the bias and variability of the estimated ATE when choosing the optimal estimation method. Our results suggest that all methods produced acceptable IPTW weights across covariate conditions, especially when complexities were not present in the population treatment or outcome model. However, with added complexities and a higher number of covariates, traditional logistic regression and nn-1 produced extreme weights, which could result in biased treatment effects. This emphasizes the need for choosing a method that is robust to complexities and high-dimensional contexts.

Overall, our study adds to the existing literature by demonstrating the robustness of DNN-based approaches in high-dimensional, complex settings and emphasizing the importance of considering both the bias and variability of the estimated ATE when choosing the optimal estimation method. Further research is needed to explore the performance of DNN-based approaches in different settings and to compare their performance with other methods, such as those proposed by Keller (????) and Lee (????). Nevertheless, our study provides valuable insights into the performance of DNN-based approaches in propensity score analysis and highlights their potential as a useful tool for confounding control in comparative effectiveness research.

One of the novel aspects of my simulation study is the use of the Adam optimization algorithm for fitting the DNN model. Unlike the other studies on neural network-based propensity score estimation, such as Collier, Leite and Zhang (2017) and Hill, Weiss, and Zhai (2017), I opted to use the Adam optimization algorithm instead of cross-validation to select the optimal model. The use of the Adam optimizer reduces the computational burden associated with cross-validation, which is especially relevant in high-dimensional covariate settings where the process of cross-validation can become computationally intensive. Moreover, the Adam optimization algorithm has been shown to perform well in deep learning models, particularly in complex and high-dimensional settings (Kingma and Ba, 2014). The use of the Adam optimizer in my study adds to the growing body of literature on neural network-based propensity score estimation and highlights the potential benefits of this approach in terms of reducing the computational burden while still achieving high accuracy.

In conclusion, this study is the first to compare various machine learning methods, including DNNs, in a high-dimensional context for propensity score analysis. The results show that DNN-2 outperforms traditional logistic regression and other machine learning methods in terms of ATE estimation, variability, covariate balance, and IPTW weighting, especially in high-dimensional cases. Furthermore, the use of the Adam optimizer in fitting the DNN negates the need for cross-validation, making the model more efficient. Given the increasing availability of rich high-dimensional administrative datasets in educational and social science research, these findings have important implications for researchers seeking accurate and reliable methods for propensity score analysis. The results of this study highlight the importance of considering machine learning methods, particularly DNNs, for propensity score analysis in high-dimensional contexts and set a foundation for further research in this area.

\newpage

## Tables and Figures

```{r}
#| label: tbl-bias
#| tbl-cap: Bias of ATE estimation

over_tab <- res |> 
  dplyr::select(p, scenarioT, scenarioY, method, Bias, Abs_Per_Bias)  

gt(over_tab) |>
  tab_spanner(
    label = "Condition",
    columns = c(p, scenarioT, scenarioY)
  ) |>
  fmt_number(columns = c(Bias, Abs_Per_Bias), decimals = 3) |>
  cols_align_decimal() |> 
    cols_label(
    scenarioT = md("Outcome"),
    scenarioY = md("Treatment"),
    Abs_Per_Bias = md("Abs. Bias (%)"),
    method = "Method") 
```

\newpage

```{r}
#| label: tbl-var
#| tbl-cap: ATE estimation - SE, MSE, and 95% C.I. coverage


over_tab <- res |> 
  dplyr::select(p, scenarioT, scenarioY, method, ATE_se, MSE, coverage_95)  

gt(over_tab) |>
  tab_spanner(
    label = "Condition",
    columns = c(p, scenarioT, scenarioY)
  ) |>
  fmt_number(columns = c(ATE_se, MSE, coverage_95), decimals = 3) |>
  cols_align_decimal() |> 
    cols_label(
    scenarioT = md("Outcome"),
    scenarioY = md("Treatment"),
    ATE_se = md("SE"),
    method = "Method",
    coverage_95 = md("95% C.I.<br>Coverage"))
```

\newpage

```{r}
#| label: tbl-power
#| tbl-cap: Power

over_tab <- res |> 
  dplyr::select(p, scenarioT, scenarioY, method, Power)  

gt(over_tab) |>
  tab_spanner(
    label = "Condition",
    columns = c(p, scenarioT, scenarioY)
  ) |>
  fmt_number(columns = c(Power), decimals = 3) |>
  cols_align_decimal() |> 
    cols_label(
    scenarioT = md("Outcome"),
    scenarioY = md("Treatment"),
    method = "Method")
```

\newpage

```{r}
#| label: tbl-asam
#| tbl-cap: ASAM

over_tab <- res |> 
  dplyr::select(p, scenarioT, scenarioY, method, ASAM)  

gt(over_tab) |>
  tab_spanner(
    label = "Condition",
    columns = c(p, scenarioT, scenarioY)
  ) |>
  fmt_number(columns = c(ASAM), decimals = 3) |>
  cols_align_decimal() |> 
    cols_label(
    scenarioT = md("Outcome"),
    scenarioY = md("Treatment"),
    method = "Method")
```
