# Simulation Code

## Data Generating Function

```{r}
#| echo: true
#| eval: false

#############
## WHAT DOES THIS FUNCTION DO?
# The Generate function generates simulated data based on specified conditions. 
# The input is a tibble containing information about the sample size, number of covariates, and conditions for the population treatment and outcome models. 
# The function first generates correlated normal variables and transforms them into normal, Bernoulli, and uniform variables. It then selects a subset of the covariates for use in the population treatment and outcome models. 
# The population treatment and outcome models are generated based on the specified conditions 
# and the selected covariates, and the treatment status is also generated. 
# The function returns a simulated data tibble that includes the original covariates, treatment status, and generated outcome.
#############

Generate <- function(condition, fixed_objects = NULL) {
  # Makes the tibble of sim crossed conditions accessible to the function, from SimDesign package
  Attach(condition)

  # Generate a mean vector of 0s
  mean <- numeric(p)

  # Generate a correlation matrix with correlations between -.3 to .3
  cor <- matrix(runif(p^2, min = -.3, max = .3), nrow = p)
  diag(cor) <- 1

  # Smooth the correlation matrix to ensure it is positive definite
  cor <- psych::cor.smooth(cor)

  # Generate correlated normal variables
  vars <- mvrnorm(n, mean, cor)

  # Calculate the number of normal, Bernoulli, and uniform variables to generate
  num_norm_vars <- floor(p / 2)
  num_bern_vars <- floor(p / 4)
  num_uniform_vars <- p - num_norm_vars - num_bern_vars

  # Convert all variables to uniform variables between 0 and 1
  vars_unif <- pnorm(vars)

  # Convert the first num_norm_vars variables to normal variable with mean = 0 and sd = 1
  vars_normal <- qnorm(vars_unif[, 1:num_norm_vars])

  # Convert the next num_bern_vars variables to Bernoulli variables with probability of success = 0.5
  vars_bern <- qbern(vars_unif[, (num_norm_vars + 1):(num_norm_vars + num_bern_vars)], 0.5)

  # The remainder are left as uniform variables
  vars_uniform <- vars_unif[, (num_norm_vars + num_bern_vars + 1):p]

  # Combine the transformed variables
  vars_transformed <- cbind(vars_normal, vars_uniform, vars_bern)

  # Give the columns of vars names v1,v2,etc.
  colnames(vars_transformed) <- sprintf("v%d", 1:p)

  # Generate variable names and store in the master_covar list
  master_covar <- dimnames(vars_transformed)[[2]]

  # Create p objects with names v1, v2, etc. in working environment
  for (i in 1:p) {
    assign(colnames(vars_transformed)[i], vars_transformed[, i])
  }

  # Sample half of the covariates and save to covar_confound
  covar_confound <- sample(master_covar, size = length(master_covar) / 2)

  # Sample a quarter of the covariates and save to covar_rel_outcome
  covar_rel_outcome <- sample(setdiff(master_covar, covar_confound), size = length(master_covar) / 4)

  # Save the remaining covariates to covar_rel_treatment
  covar_rel_treatment <- setdiff(master_covar, union(covar_confound, covar_rel_outcome))

  # Combine covar_confound and covar_rel_outcome, these are the covariates that will be used for the population outcome models
  covar_for_treatment <- union(covar_confound, covar_rel_treatment)

  # Combine covar_confound and covar_rel_outcome, these are the covariates that will be used for the population outcome models
  covar_for_outcome <- union(covar_confound, covar_rel_outcome)

  #########################################
  #########################################
  # Population treatment models
  #########################################
  #########################################

  # Generate b coefficients for population treatment models
  # Initialize b0 to 0.25
  b0 <- 0.25

  # Create an empty list to store the b coefficients
  beta <- vector("list", length(master_covar))

  # Loop through all variables in the master covariate list
  for (i in seq_len(length(master_covar))) {
    # Generate a random number between -0.4 and 0.4
    x <- runif(1, min = -0.4, max = 0.4)

    # Assign the value to a variable named b1, b2, etc.
    assign(paste0("b", i), x)

    # Store the variable names in the beta list
    b <- paste0("b", i)
    beta[[i]] <- b
  }

  # Extract the coefficient from the covariate name
  b <- sub(".*v", "", covar_for_treatment)

  # Create a new variable called element with the format "b * covar_for_treatment"
  element <- paste0("b", b, " * ", covar_for_treatment)

  #########################################
  # Population treatment model - Generate base model
  #########################################
  if (scenarioT == "base_T") {
    # Concatenate the variables from covar_for_treatment into a single string
    equation <- paste0("(1 + exp(-(b0 + ", paste(element, collapse = " + "), ")))^-1")

    # Evaluate the equation and store the result in trueps
    trueps <- eval(parse(text = equation))
  } else

  #########################################
  # Population treatment model - Complex model
  #########################################
  if (scenarioT == "complex_T") {
    # Sample half of the variables from covar_for_treatment
    sample_vars <- sample(covar_for_treatment, length(covar_for_treatment) / 2)

    # Create a list to store the terms
    terms <- list()

    # Iterate over the sampled variables and create the quadratic terms
    for (var in sample_vars) {
      b <- sub(".*v", "", var)
      quad_term <- paste0("b", b, " * ", var, "^2")
      terms[[var]] <- quad_term
    }

    # Sample half of the variables again from covar_for_treatment
    sample_vars2 <- sample(covar_for_treatment, length(covar_for_treatment) / 2)

    # Create a list of all possible interactions between the variables
    interactions <- combn(sample_vars2, 2, paste0, collapse = "*")

    # Iterate over the interactions and create the interaction terms
    for (inter in interactions) {
      b <- sub(".*v", "", inter)
      inter_term <- paste0("b", b, " * ", inter)
      terms[[inter]] <- inter_term
    }

    # Concatenate all of the terms together and store the result in a new variable called equation
    equation <- paste0("(1 + exp(-(b0 + ", paste(c(unlist(terms), element), collapse = " + "), ")))^-1")

    # Evaluate the equation
    trueps <- eval(parse(text = equation))
  }

  #########################################
  # ~~ binary treatment T
  #########################################

  unif1 <- runif(n, 0, 1)
  T <- ifelse(unif1 < trueps, 1, 0)

  #########################################
  #########################################
  # Population outcome models
  #########################################
  #########################################

  # Generate a coefficients for population outcome models
  # Initialize a0 to -0.18
  a0 <- -0.18

  # Set ATE to 0.3
  g <- 0.3

  # Generate error terms for population outcome models
  e <- rnorm(n, mean = 0, sd = sqrt(0.17))

  alpha <- vector("list", length(master_covar))

  for (i in 1:length(master_covar)) {
    # Generate a random number between -0.2 and 0.3
    x <- runif(1, min = -0.2, max = 0.3)
    # Assign the value to a1, a2, a3, etc.
    assign(paste0("a", i), x)
    a <- paste0("a", i)
    alpha[[i + 1]] <- a
  }

  # Extract the coefficient from the covariate name
  a <- sub(".*v", "", covar_for_outcome)

  # Create a new variable called element with the format "a * covar_for_outcome"
  element <- paste0("a", a, " * ", covar_for_outcome)

  #########################################
  # Population outcome model - Generate base model
  #########################################
  if (scenarioY == "base_Y") {
    equation <- paste0("a0 + g * T", " + ", paste(element, collapse = " + "), " + e")
    Y <- eval(parse(text = equation))
  } else

  #########################################
  # Population outcome model - Complex model
  #########################################
  if (scenarioY == "complex_Y") {
    # Sample half of the variables from covar_for_outcome
    sample_vars <- sample(covar_for_outcome, length(covar_for_outcome) / 2)

    # Create a list to store the terms
    terms <- list()

    # Iterate over the sampled variables and create the quadratic terms
    for (var in sample_vars) {
      a <- sub(".*v", "", var)
      quad_term <- paste0("a", a, " * ", var, "^2")
      terms[[var]] <- quad_term
    }

    # Sample half of the variables again from covar_for_outcome
    sample_vars2 <- sample(covar_for_outcome, length(covar_for_outcome) / 2)

    # Create a list of all possible interactions between the variables
    interactions <- combn(sample_vars2, 2, paste0, collapse = "*")

    # Iterate over the interactions and create the interaction terms
    for (inter in interactions) {
      a <- sub(".*v", "", inter)
      inter_term <- paste0("a", a, " * ", inter)
      terms[[inter]] <- inter_term
    }

    equation <- paste0("a0 + g * T + ", paste(c(unlist(terms), element), collapse = " + "), " + e")
    Y <- eval(parse(text = equation))
  }

  #########################################
  # Form simulated data tibble
  #########################################

  v_list <- mget(paste0("v", 1:length(master_covar)))
  dat <- as_tibble(v_list)
  dat$T <- T
  dat$Y <- Y
  dat$trueps <- trueps
  dat
}

```

## Analyse Function

```{r}
#| echo: true
#| eval: false

#############
## WHAT DOES THIS FUNCTION DO?
# The Analyse function is used to estimate the average treatment effect (ATE) and related metrics for a given condition. 
# The function uses one of several methods, specified by the method argument, to estimate the propensity score.
# The methods used to estimate the propensity score are 
# logistic regression (logit), classification and regression trees (cart), bagging (bag), random forest (forest), 
# and three neural network models (nn-1, dnn-2, and dnn-3). Once the propensity score is estimated, 
# the function uses survey-weighted regression to estimate the ATE, standard error of the ATE, p-value, and 95% confidence interval of the ATE. 
# The function also calculates the absolute standardized average mean (ASAM) for each covariate in the data.
#############

# function to estimate the ATE and other metrics
Analyse <- function(condition, dat, fixed_objects = NULL) {
  Attach(condition)

  # if the method is logit, then estimate the ATE using logistic regression
  if (method == "logit") {
    # estimate the propensity score using logistic regression
    mod <- glm(T ~ . - Y - trueps, data = dat, family = binomial(link = "logit"))
    # predict on the entire dataframe to generate ps
    ps <- predict(mod, newdata = dat, type = "response")
    # if the method is cart, then estimate the ATE using classification and regression trees
  } else if (method == "cart") {
    # estimate the propensity score using classification and regression trees
    mod <- rpart(T ~ . - Y - trueps, method = "class", data = dat)
    # predict on the entire dataframe to generate ps
    ps <- predict(mod, newdata = dat, type = "prob")[, 2]
    # if the method is bag, then estimate the ATE using bagging
  } else if (method == "bag") {
    # estimate the propensity score using bagging
    mod <- bagging(T ~ . - Y - trueps, data = dat, nbagg = 100)
    # save the propensity score to a vector
    ps <- predict(mod, newdata = dat, type = "prob")
    # if the method is forest, then estimate the ATE using random forest
  } else if (method == "forest") {
    # estimate the propensity score using random forest
    mod <- randomForest(factor(T) ~ . - Y - trueps, data = dat)
    # save the propensity score to a vector
    ps <- predict(mod, newdata = dat, type = "prob")[, 2]
  } else if (method == "nn-1") {
    # Preprocess data
    # Split the data into training and validation sets (80/20)
    split <- sample(2, nrow(dat), replace = TRUE, prob = c(0.8, 0.2)) # random split of data
    train_data <- dat[split == 1, ]
    validation_data <- dat[split == 2, ]
    x_train <- as.matrix(train_data[, grep("^v", names(train_data))]) # select columns that start with "v" for input features
    y_train <- as.matrix(train_data[, "T"]) # select column for treatment assignment
    x_validation <- as.matrix(validation_data[, grep("^v", names(validation_data))]) # select columns that start with "v" for input features
    y_validation <- as.matrix(validation_data[, "T"]) # select column for treatment assignment

    # Define model
    p <- ncol(x_train) # number of input features
    input_layer <- layer_input(shape = c(p)) # input layer
    hidden_layer <- layer_dense(units = p, activation = "relu")(input_layer)
    output_layer <- layer_dense(units = 1, activation = "sigmoid")(hidden_layer)
    model <- keras_model(inputs = input_layer, outputs = output_layer)

    # Compile model
    model %>% compile(
      optimizer = "adam",
      loss = "binary_crossentropy",
      metrics = c("accuracy")
    )

    # Define callbacks
    early_stopping <- callback_early_stopping(monitor = "val_loss", min_delta = 0.001, patience = 5)

    # Fit model
    history <- model %>% fit(
      x_train,
      y_train,
      epochs = 100,
      batch_size = 64,
      validation_data = list(x_validation, y_validation),
      callbacks = list(early_stopping),
      verbose = 0
    )

    # Preprocess data
    x <- as.matrix(dat[, grep("^v", names(dat))]) # select columns that start with "v" for input features

    # Predict propensity scores on entire dataset
    ps <- model %>% predict(x)
    ps <- ps[, 1]
  } else if (method == "dnn-2") {
    # Preprocess data
    # Split the data into training and validation sets (80/20)
    split <- sample(2, nrow(dat), replace = TRUE, prob = c(0.8, 0.2)) # random split of data
    train_data <- dat[split == 1, ]
    validation_data <- dat[split == 2, ]
    x_train <- as.matrix(train_data[, grep("^v", names(train_data))]) # select columns that start with "v" for input features
    y_train <- as.matrix(train_data[, "T"]) # select column for treatment assignment
    x_validation <- as.matrix(validation_data[, grep("^v", names(validation_data))]) # select columns that start with "v" for input features
    y_validation <- as.matrix(validation_data[, "T"]) # select column for treatment assignment

    # Define model
    p <- ncol(x_train) # number of input features
    input_layer <- layer_input(shape = c(p)) # input layer
    hidden_layer1 <- layer_dense(units = ceiling(2 * p / 3), activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01))(input_layer) # first hidden layer
    hidden_layer2 <- layer_dense(units = ceiling(2 * p / 3), activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01))(hidden_layer1) # second hidden layer
    output_layer <- layer_dense(units = 1, activation = "sigmoid", kernel_regularizer = regularizer_l2(l = 0.01))(hidden_layer2) # output layer
    model <- keras_model(inputs = input_layer, outputs = output_layer)

    # Compile model
    model %>% compile(
      optimizer = "adam",
      loss = "binary_crossentropy",
      metrics = c("accuracy")
    )

    # Define callbacks
    early_stopping <- callback_early_stopping(monitor = "val_loss", min_delta = 0.001, patience = 5)

    # Fit model
    history <- model %>% fit(
      x_train,
      y_train,
      epochs = 100,
      batch_size = 64,
      validation_data = list(x_validation, y_validation),
      callbacks = list(early_stopping),
      verbose = 0
    )

    # Preprocess data
    x <- as.matrix(dat[, grep("^v", names(dat))]) # select columns that start with "v" for input features

    # Predict propensity scores on entire dataset
    ps <- model %>% predict(x)
    ps <- ps[, 1]
  } else if (method == "dnn-3") {
    # Preprocess data
    # Split the data into training and validation sets (80/20)
    split <- sample(2, nrow(dat), replace = TRUE, prob = c(0.8, 0.2)) # random split of data
    train_data <- dat[split == 1, ]
    validation_data <- dat[split == 2, ]
    x_train <- as.matrix(train_data[, grep("^v", names(train_data))]) # select columns that start with "v" for input features
    y_train <- as.matrix(train_data[, "T"]) # select column for treatment assignment
    x_validation <- as.matrix(validation_data[, grep("^v", names(validation_data))]) # select columns that start with "v" for input features
    y_validation <- as.matrix(validation_data[, "T"]) # select column for treatment assignment

    # Define model
    p <- ncol(x_train) # number of input features
    input_layer <- layer_input(shape = c(p)) # input layer
    hidden_layer1 <- layer_dense(units = ceiling(2 * p / 3), activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01))(input_layer) # first hidden layer
    hidden_layer2 <- layer_dense(units = ceiling(2 * p / 3), activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01))(hidden_layer1) # second hidden layer
    hidden_layer3 <- layer_dense(units = ceiling(2 * p / 3), activation = "relu", kernel_regularizer = regularizer_l2(l = 0.01))(hidden_layer2) # third hidden layer
    output_layer <- layer_dense(units = 1, activation = "sigmoid", kernel_regularizer = regularizer_l2(l = 0.01))(hidden_layer3) # output layer
    model <- keras_model(inputs = input_layer, outputs = output_layer)


    # Compile model
    model %>% compile(
      optimizer = "adam",
      loss = "binary_crossentropy",
      metrics = c("accuracy")
    )

    # Define callbacks
    early_stopping <- callback_early_stopping(monitor = "val_loss", min_delta = 0.001, patience = 5)

    # Fit model
    history <- model %>% fit(
      x_train,
      y_train,
      epochs = 100,
      batch_size = 64,
      validation_data = list(x_validation, y_validation),
      callbacks = list(early_stopping),
      verbose = 0
    )

    # Preprocess data
    x <- as.matrix(dat[, grep("^v", names(dat))]) # select columns that start with "v" for input features

    # Predict propensity scores on entire dataset
    ps <- model %>% predict(x)
    ps <- ps[, 1]
  }

  ##############################
  ### calculate metrics
  ##############################

  dat <- dat %>%
    mutate(
      ps_pred = ps,
      ps_weights = case_when(T == 1 ~ 1 / ps, T == 0 ~ 1 / (1 - ps))
    )

  true_ATE <- 0.3

  # calculate standardized initial bias prior to weighting
  Std_In_Bias <- ((mean(dat$Y[dat$T == 1]) - mean(dat$Y[dat$T == 0])) - true_ATE) / sd(dat$Y[dat$T == 1])
  Prob_Treat <- mean(dat$T)

  # estimate the true_ATE with the weights
  d.w <- svydesign(~0, weights = dat$ps_weights, data = dat)
  fit <- svyglm(Y ~ T, design = d.w)

  # save the true_ATE and se_true_ATE
  ATE <- unname(coef(fit)["T"])
  vcov_matrix <- vcov(fit)
  ATE_se <- unname(sqrt(vcov_matrix["T", "T"]))

  # extract the p-value of T
  p_val <- summary(fit)$coefficients["T", "Pr(>|t|)"]

  # calculate the 95% coverage
  conf_interval <- confint(fit, level = 0.95)["T", ]
  lower_bound <- conf_interval[1]
  upper_bound <- conf_interval[2]

  ci_95 <- ifelse(lower_bound < true_ATE && true_ATE < upper_bound, 1, 0)

  # calculate the mean of weights
  mean_ps_weights <- mean(dat$ps_weights)

  ###############
  # calculate the ASAM for covariates
  ###############

  # subset the data into the treatment and comparison groups
  treatment_group <- dat[dat$T == 1, ]
  comparison_group <- dat[dat$T == 0, ]

  # get the names of the variables that start with "v"
  var_names <- names(dat)[grep("^v", names(dat))]

  # initialize the ASAM_list vector
  ASAM_list <- rep(NA, length(var_names))

  # loop through each covariate
  for (i in 1:length(var_names)) {
    # get the covariate name
    covariate <- var_names[i]

    # extract the covariate data from the treatment and comparison groups
    treatment_data <- treatment_group[[covariate]]
    comparison_data <- comparison_group[[covariate]]

    # extract the weights from the treatment and comparison groups
    treatment_weights <- treatment_group$ps_weights
    comparison_weights <- comparison_group$ps_weights

    # calculate the means of the treatment and comparison groups
    treatment_mean <- weighted.mean(treatment_data, treatment_weights)
    comparison_mean <- weighted.mean(comparison_data, comparison_weights)

    # calculate the variances of the treatment group
    treatment_var <- wtd.var(treatment_data, treatment_weights)

    # calculate the standard deviations of the treatment groups
    treatment_sd <- sqrt(treatment_var)

    # calculate the standardized difference of means
    sd_diff <- (treatment_mean - comparison_mean) / treatment_sd

    # take the absolute value of the standardized difference of means
    abs_sd_diff <- abs(sd_diff)

    # save the absolute standardized difference of means in the ASAM_list vector
    ASAM_list[i] <- abs_sd_diff
  }


  # calculate the mean of the absolute standardized differences of means
  ASAM <- mean(ASAM_list)

  ret <- c(
    Std_In_Bias = Std_In_Bias,
    Prob_Treat = Prob_Treat,
    ATE = ATE,
    ATE_se = ATE_se,
    mean_ps_weights = mean_ps_weights,
    ASAM = ASAM,
    p_val = p_val,
    ci_95 = ci_95
  )
  ret
}

```

## Summarize Function

```{r}
#| echo: true
#| eval: false

# Summarise function
Summarise <- function(condition, results, fixed_objects = NULL) {
  Std_In_Bias <- mean(results$Std_In_Bias)
  Prob_Treat <- mean(results$Prob_Treat)
  Bias <- bias(results$ATE, parameter = 0.3, type = "bias")
  Abs_Per_Bias <- bias(results$ATE, parameter = 0.3, type = "bias", abs = T, percent = T)
  Abs_Per_Rel_Bias <- bias(results$ATE, parameter = 0.3, type = "relative", abs = T, percent = T)
  ATE_se <- mean(results$ATE_se)
  MSE <- RMSE(results$ATE, parameter = 0.3, MSE = T)
  Power <- EDR(results$p_val, alpha = 0.05)
  coverage_95 <- mean(results$ci_95)
  mean_ps_weights <- mean(results$mean_ps_weights)
  ASAM <- mean(results$ASAM)
  # Create a vector of the results
  ret <- c(
    Std_In_Bias = Std_In_Bias,
    Prob_Treat = Prob_Treat,
    Bias = Bias,
    Abs_Per_Bias = Abs_Per_Bias,
    Abs_Per_Rel_Bias = Abs_Per_Rel_Bias,
    ATE_se = ATE_se,
    MSE = MSE,
    Power = Power,
    coverage_95 = coverage_95,
    mean_ps_weights = mean_ps_weights,
    ASAM = ASAM
  )
  # Return the vector
  ret
}

```

## Simulation Driver 1

```{r}
#| echo: true
#| eval: false

######################################################################
# Load libraries and source functions
######################################################################

packages <- c(
  "here",
  "tidyverse",
  "MASS",
  "Rlab",
  "Matrix",
  "psych",
  "Rlab",
  "rpart",
  "ipred",
  "randomForest",
  "nnet",
  "survey",
  "Hmisc",
  "future",
  "furrr",
  "SimDesign",
  "keras",
  "tensorflow",
  "reticulate"
)

lapply(packages, library, character.only = TRUE)

# sets working directory to root of R project
here()

######### source functions
source(here("code", "01_data_gen_fun.R"))
source(here("code", "02_analyse_fun.R"))
source(here("code", "03_summarize_fun.R"))

######################################################################
# Generate sim design dataframe
######################################################################

# fully-crossed simulation experiment
Design <- createDesign(
  n = c(10000),
  p = c(20, 100, 200),
  scenarioT = c("base_T", "complex_T"),
  scenarioY = c("base_Y", "complex_Y"),
  method = c("logit", "cart", "bag", "forest")
)

######################################################################
# Run Simulation
######################################################################

# use_virtualenv("/ihome/xqin/alg223/.virtualenvs/r-reticulate")
# use_condaenv("r-reticulate")

res <- runSimulation(
  design = Design,
  replications = 1000,
  generate = Generate,
  analyse = Analyse,
  summarise = Summarise,
  parallel = T,
  filename = "sim_results_n10000_r1000_P_e.rds",
  save_results = T
)

```

## Simulation Driver 2

```{r}
#| echo: true
#| eval: false

######################################################################
# Load libraries and source functions
######################################################################

packages <- c(
  "here",
  "tidyverse",
  "MASS",
  "Rlab",
  "Matrix",
  "psych",
  "Rlab",
  "rpart",
  "ipred",
  "randomForest",
  "nnet",
  "survey",
  "Hmisc",
  "future",
  "furrr",
  "SimDesign",
  "keras",
  "tensorflow",
  "reticulate"
)

lapply(packages, library, character.only = TRUE)

# sets working directory to root of R project
here()

######### source functions
source(here("code", "01_data_gen_fun.R"))
source(here("code", "02_analyse_fun.R"))
source(here("code", "03_summarize_fun.R"))

######################################################################
# Generate sim design dataframe
######################################################################

# fully-crossed simulation experiment
Design <- createDesign(
  n = c(10000),
  p = c(20, 100, 200),
  scenarioT = c("base_T", "complex_T"),
  scenarioY = c("base_Y", "complex_Y"),
  method = c("nn-1", "dnn-2", "dnn-3")
)

######################################################################
# Run Simulation
######################################################################

use_virtualenv("/ihome/xqin/alg223/.virtualenvs/r-reticulate")
# use_condaenv("r-reticulate")

res <- runSimulation(
  design = Design,
  replications = 1000,
  generate = Generate,
  analyse = Analyse,
  summarise = Summarise,
  parallel = F,
  filename = "sim_results_n10000_r1000_NP_e_batch32_L2_1.rds",
  save_results = T)

```
